{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7240b8a3-dd38-489c-abbc-7cfeac3b0d1e",
   "metadata": {},
   "source": [
    "# <b>機械学習の基礎-3</b>\n",
    "アンサンブルや特徴量エンジニアリングについてメモしておく。  \n",
    "\n",
    "参考：  \n",
    "https://datawokagaku.com/ensemble/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7a9ac-e946-4fb2-9667-14dec904f213",
   "metadata": {
    "tags": []
   },
   "source": [
    "## アンサンブル\n",
    "複数の機械学習モデルを組み合わせて予測する手法。  \n",
    "一般的に単一のモデルよりも精度が高く、実務での予測はアンサンブルが主流。  \n",
    "アンサンブルは主にバギング、ブースティング、スタッキングの3種類がある。\n",
    "\n",
    "組み合わせるモデルはそれぞれの相関が低い（似ていない）モデルが望ましく、できるだけ多種多様なモデルを用いる。  \n",
    "※弱学習器を使うのが良いようだが、KaggleではGBDTとニューラルネットの組み合わせを行っている例もあるので、  \n",
    "　必ずというわけではなさそう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d8a73-852a-421c-8791-d3c757a43946",
   "metadata": {},
   "source": [
    "### バギング(bagging: bootstrap aggregating)\n",
    "下記の流れで予測するのがバギング。\n",
    "1. 学習データを母集団として、重複を許してランダムに標本抽出する。    \n",
    "    （重複を許して： 同じレコードが何回出てきてもよい。）\n",
    "2. 抽出した学習データでモデルに学習させる。\n",
    "3. 学習させたいモデルの数だけ1.～2.を繰り返す（並列にモデルを複数個作るイメージ）。\n",
    "4. 全学習済みモデルの予測結果の多数決や確率の平均をとり、それをバギングによる予測結果とする。  \n",
    "\n",
    "学習データの一部しか使わずに学習させるため、high bias low variance気味になるので、  \n",
    "モデルには高varianceになりやすい決定木を用いることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd29af-68ef-495c-8f08-336ae07af285",
   "metadata": {},
   "source": [
    "### ランダムフォレスト(Random Forest)\n",
    "バギングにおいて学習モデルに決定木を採用し、さらに特徴量を選択して学習させるアンサンブル手法。  \n",
    "それぞれの学習モデルで一部の学習データと特徴量を使わないことで、少し異なる決定木を複数作る。  \n",
    "（相関が低いモデルを作ることが狙い。アンサンブルでは多種多様なモデルを用いることが望ましいため。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048ec3a-6057-4e68-89a2-02eb365051d8",
   "metadata": {},
   "source": [
    "### ブースティング(boosting)\n",
    "バギングと異なり、直列に弱学習器を順次作っていき、  \n",
    "それらの予測値ごとに重みをつけて足し合わせたものを予測値とする手法。  \n",
    "下記の流れのように、「残差を目的変数として学習し、その予測値で残差を埋める」というプロセスを繰り返して、  \n",
    "それぞれのモデルでの残差の予測値を重みをつけて足し合わせた結果を最終的な予測値とする。  \n",
    "\n",
    "＜ブースティングの大まかな流れ（アルゴリズムによって異なる）＞  \n",
    "ブースティングでの最終的な予測結果を$\\hat{f}(x)$、  \n",
    "残差を$r$（$i$番目のデータの残差は$r_i$）とおく。  \n",
    "また、最初のイテレーションでは$\\hat{f}(x)=0$とする。  \n",
    "このとき、予測値が常に0なので残差は目的変数に等しい（$r_i=y_i$）。\n",
    "\n",
    "以下の手順をN回繰り返す。(N = 1,2,...,N)\n",
    "1. 弱学習器$\\hat{f_t}$を学習データ$(X,r)$で学習\n",
    "2. $r_i$を $r_i - \\eta\\hat{f_b}(x_i)$ で更新。すなわち残差を予測値で埋める。$\\eta$は学習率。\n",
    "\n",
    "最終的な予測値は下記のように残差の予測値を学習率をつけて足し合わせたもの。  \n",
    "$$\\hat{f}(x) = \\sum_{b=1}^N \\eta\\hat{f_b}(x_i)$$\n",
    "\n",
    "<br></br>\n",
    "うまくいけばbiasとvarianceの両方を下げることができ、バギングよりも精度が高くなることが期待できる。  \n",
    "弱学習器には決定木を採用することが多い。  \n",
    "2023年3月時点で、このブースティングをベースとした勾配ブースティング決定木(GBDT)とニューラルネットが機械学習の主流。  \n",
    "(画像認識、自然言語処理等ではニューラルネット、テーブルデータではGBDTの方が使われるぽい。)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b663cd-8011-4f27-976b-da1b97488cfa",
   "metadata": {},
   "source": [
    "### AdaBoost(Adaptive Boosting)\n",
    "基本的なブースティングのアルゴリズムの一つ。  \n",
    "損失が大きい（上手く予測できなかった）データに重みをつけて次の学習で重点的に学習できるようにする手法。  \n",
    "データに重みをつけるというのは、通常は各データの損失を表す関数ごとに重み$\\omega_i$をつけるということ。  \n",
    "これを全データ分足し合わせたものが弱学習器の損失関数になり、これが最小になるように学習する。  \n",
    "$$L(x,y)=\\frac{1}{m}\\sum_{i=1}^m l(x_i,y_i)$$ \n",
    "この$l(x_i,y_i)$に対して損失の大きさに応じた重み$\\omega_i$がつけられる。  \n",
    "最初のイテレーションでは一律$\\omega_i=\\frac{1}{m}$がかけられ、  \n",
    "次のイテレーションから各データの損失の大きさに応じて$\\omega_i$は更新されていく。  \n",
    "また、弱学習器自体の重み$\\alpha$も損失に応じて計算され、  \n",
    "AdaBoostでの最終的な予測値はこの重みと各弱学習器の予測結果を掛け合わせた合計となる。  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc724b7-ad8e-4cfd-9330-e09590b71356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
