{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6e07c4-025f-4fe0-a230-4e72d81d6768",
   "metadata": {},
   "source": [
    "# 機械学習に関するTips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f95498-f97b-434d-b24a-0aba92ef3154",
   "metadata": {},
   "source": [
    "## ○ テーブルデータの予測は深層学習よりもツリー系モデルの方が精度が良いらしい\n",
    "\n",
    "\n",
    "深層学習がテーブルデータで精度が出ない場合の理由は主に以下二つ。\n",
    "* 外れ値が含まれる、目的変数の分布が正規分布でないといったことがあるとそれら不正規な部分を過学習してしまう  \n",
    "  深層学習では標準化が必要であり、(scikit-learnの)標準化は正規分布を前提にしているため上手く標準化できない。  \n",
    "  下記の参考サイトの非正規分布の標準化をする必要があるかも。\n",
    "* 予測に役立たない特徴量もしっかり使おうとしてしまう。\n",
    "\n",
    "参考：\n",
    "* 元記事： https://exploratory.io/note/kanaugust/XGBoost-knM4aqw2IL  \n",
    "* 非正規分布の標準化について： https://biolab.sakura.ne.jp/robust-z-score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcaa28f-bae2-4866-8268-04194775789c",
   "metadata": {},
   "source": [
    "## ○ 線形モデルにおいて目的変数と特徴量が正規分布に近いと精度が上がるかも \n",
    "\n",
    "元文：  \n",
    "線形モデルは目標値や特徴量が正規分布である必要はないが、目的変数や特徴量の変換（logなど）によって正規分布に近づけることで、適合の質が向上する場合がある。  \n",
    "これは、外れ値の影響を減らすことができるからである。強く歪んだ特徴量は、一般的にターゲットと線形関係にない（これは線形モデルの仮定の一つである）。  \n",
    "\n",
    "特徴量は確かに標準化する際に正規分布を仮定しているため、変換によって良い効果が得られそう。  \n",
    "目的変数も変換するのは意外だが、確かに外れ値との残差を減らそうと学習するのは汎化性能が落ちてしまいそうな気がする。  \n",
    "特徴量、目的変数ともにlogに変換した後の特徴量空間における線形回帰の予測は線形に見えるが、変換前の特徴量空間では非線形に見えるようになる。  \n",
    "\n",
    "参考:  \n",
    "* [KaggleのKernel](https://www.kaggle.com/code/dejavu23/house-prices-eda-to-ml-beginner/comments)\n",
    "* [Scikit-Learnのドキュメント](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "* [目的変数をlogに変換する話](https://tomoshige-n.hatenablog.com/entry/2014/08/14/013616)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41eb93-d12a-499c-ae36-b8b9d64c9e4c",
   "metadata": {},
   "source": [
    "## ○ 目的変数と特徴量の関係性の調べ方\n",
    "量的変数であれば目的変数と特徴量で2次元の散布図を書くと概ね傾向、外れ値が分かる。  \n",
    "このとき一応ピアソンの相関係数も算出しておくと、相関を定量的に示せる。  \n",
    "ただし、ピアソンの相関係数は線形の相関しか評価できないので注意。2次関数の相関だと、0に近い値になる。  \n",
    "非線形の相関はMICを使えば評価できるが、導入が面倒なのと特徴量が多いと計算に時間がかかるのが難点。  \n",
    "そもそも非線形の相関関係は散布図みれば明らかである。  \n",
    "https://qiita.com/m-hayashi/items/2204e9d7e4e6837c6140\n",
    "\n",
    "カテゴリ変数（質的変数）であればboxplotなどでカテゴリごとの目的変数の様子を見る。  \n",
    "カテゴリによって四分位範囲が大きく変わるようであれば、目的変数との相関が強いと言えそう。  \n",
    "注意点として、カテゴリごとは（順位をつけるようなカテゴリでなければ）基本的に等価であるため、カテゴリの並びは関係ないということ。  \n",
    "左から右にカテゴリを見て目的変数が増加しているかどうかなどを見ても意味がない。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd42fb3-1707-4a25-98d4-d99fb5244714",
   "metadata": {},
   "source": [
    "## ○ 多重共線性（マルチコ）について\n",
    "マルチコは重回帰モデルのときに起きるので、重回帰以外のモデル（決定木など）では気にしなくてよい。  \n",
    "https://yolo-kiyoshi.com/2019/05/27/post-1160/  \n",
    "https://qiita.com/wakichi/items/b68bad8e533dc45d467b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
