{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-8 回帰タスクにおけるBackpropagation\n",
    "回帰タスクの場合の誤差逆伝播を計算し、スクラッチで実装したbackwardとautogradの結果が等しくなることを確認する。   \n",
    "誤差逆伝播の各表式は変わらず、損失関数がsoftmax→MSEに変えれば良いだけ。  \n",
    "回帰のタスクだが、autogradとの比較をするだけなので、ここではMNISTデータセットを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# python debugerをインポート\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ backward（逆伝播）のスクラッチ実装結果\n",
    "※現場ではこれらの関数は別ファイルでモジュール化して一元管理すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形変換部分(パラメタ)の勾配の算出関数。Aはl-1層目の値で他はl層目の値。\n",
    "def linear_backward(A, W, b, Z):\n",
    "     # pytorch.tensorの.grad属性と区別するために.grad_として属性を定義している\n",
    "     W.grad_ = Z.grad_.T @ A\n",
    "     # bの値が一つ変わると、全データ分Lへ影響を及ぼすので、行方向(データ数の方向)にZ.grad_を足す。\n",
    "     b.grad_ = torch.sum(Z.grad_, dim=0)\n",
    "     # l-1層目のAはl層目の誤差（=Z.grad_）とl層目のWから計算できる。\n",
    "     A.grad_ = Z.grad_ @ W\n",
    "\n",
    "\n",
    "# ReLU関数\n",
    "def relu_backward(Z, A):\n",
    "     # 上式の\"l番目の層における損失の誤差\"における2行目と3行目を組み合わせてdL/dZを算出している\n",
    "     # A.grad_はdL/dA, (Z > 0).float()はReLu関数の偏微分に相当する。\n",
    "     Z.grad_ = A.grad_ * ( Z > 0 ).float()\n",
    "\n",
    "\n",
    "# softmax関数と交差エントロピーの計算を同じ関数で実装する（pytorchでもこうなっている）\n",
    "def softmax_cross_entropy(X, y_true):\n",
    "    '''\n",
    "    X: input tensor.行は各データ、列は各クラスを想定。\n",
    "    y_true: tensor。One-Hot Encoding済みの正解ラベル。  \n",
    "    '''\n",
    "    max_val = X.max(dim=1, keepdim=True).values\n",
    "    # 各要素のe^xを計算（これが分子になる）\n",
    "    e_x = (X - max_val).exp()\n",
    "    denominator = e_x.sum(dim=1, keepdim=True) + 1e-10\n",
    "    softmax_out = e_x / denominator\n",
    "    loss = - (y_true * torch.log(softmax_out + 1e-10)).sum() / y_true.shape[0]\n",
    "\n",
    "    return loss, softmax_out\n",
    "\n",
    "\n",
    "# tensorの線形結合を返す関数\n",
    "def linear_comb(X, W, B):\n",
    "    '''\n",
    "    X, W, B: torch.tensor\n",
    "    '''\n",
    "    return X @ W.T + B\n",
    "\n",
    "\n",
    "# ReLUの実装\n",
    "def ReLU(Z):\n",
    "    '''\n",
    "    Z: torch.tensor\n",
    "    '''\n",
    "    # torch.whereによって要素ごとに条件が真・偽のときで別の値を返せる\n",
    "    # 下記では0より大きい要素はzの値そのままで、0以下は0.になる。\n",
    "    return torch.where(Z > 0 , Z, 0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSEと恒等関数の実装\n",
    "def MSE_Identity_func(y_pred, y_true):\n",
    "    return torch.sum( (y_pred - y_true)**2 ) / y_true.shape[0], y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隠れ層１層の場合のMLP（スクラッチ）\n",
    "def forward_and_backward(X, W_1, W_2, B_1, B_2, y):\n",
    "    '''\n",
    "    X: features\n",
    "    W: List of weights(each edge)\n",
    "    B: List of Bias(each Layer)\n",
    "    '''\n",
    "\n",
    "    #########################\n",
    "    #### 順伝播（forward）####\n",
    "    #########################\n",
    "\n",
    "    # 入力層→隠れ層\n",
    "    Z1 = linear_comb(X, W_1, B_1)\n",
    "    Z1.retain_grad()\n",
    "\n",
    "    # 活性化関数適用\n",
    "    A1 = ReLU(Z1)\n",
    "    A1.retain_grad()\n",
    "\n",
    "    # 隠れ層→出力層\n",
    "    Z2 = linear_comb(A1, W_2, B_2)\n",
    "    Z2.retain_grad()\n",
    "\n",
    "    # 最終出力。回帰なので恒等関数の結果(Z2そのまま)がA2になる。その後、MSEを計算。\n",
    "    loss, A2 = MSE_Identity_func(Z2, y)\n",
    "\n",
    "\n",
    "    #########################\n",
    "    #### 逆伝播（backward）####\n",
    "    #########################\n",
    "\n",
    "    # 最終出力→出力層の出力のbackward。ここで各層の出力の勾配を求める。\n",
    "    # ここでは出力層の活性化関数は恒等関数とし、\n",
    "    # その出力にsoftmax_cross_entropyを適用した結果をモデルの最終出力として考えている。\n",
    "    Z2.grad_ = 2 * (A2 - y) / X.shape[0]\n",
    "    # 出力層→隠れ層のbackward\n",
    "    linear_backward(A1, W_2, B_2, Z2)\n",
    "    # 隠れ層の出力側のbackward\n",
    "    relu_backward(Z1, A1)\n",
    "    # 隠れ層の入力側のbackward\n",
    "    linear_backward(X, W_1, B_1, Z1)\n",
    "    \n",
    "    return loss, Z1, A1, Z2, A2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※↑autogradの結果との比較はしないので、.retain_grad()は削除した"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ MLPの学習ループへ組み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([1797, 64])\n",
      "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
      "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
      "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
      "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
      "         0.,  0.,  0., 11., 16., 10.,  0.,  0.])\n",
      "==========================\n",
      "shape of y_train: torch.Size([1797])\n",
      "tensor([0, 1, 2,  ..., 8, 9, 8])\n",
      "shape of train data: X_train:torch.Size([1437, 64]), y_train:torch.Size([1437, 1])\n",
      "shape of validation data: X_val:torch.Size([360, 64]), y_val:torch.Size([360, 1])\n"
     ]
    }
   ],
   "source": [
    "# 変数定義\n",
    "learning_rate = 0.03\n",
    "loss_log = []  #学習時の損失記録用のリスト\n",
    "\n",
    "# データロード\n",
    "dataset = datasets.load_digits()\n",
    "feature_names = dataset['feature_names']\n",
    "X = torch.tensor(dataset['data'], dtype=torch.float32)\n",
    "y_true = torch.tensor(dataset['target'])\n",
    "\n",
    "# shape確認\n",
    "print(f'shape of X: {X.shape}')\n",
    "print(X[1])\n",
    "print('==========================')\n",
    "print(f'shape of y_train: {y_true.shape}')\n",
    "print(y_true)\n",
    "\n",
    "# 予期しないブロードキャスティングを防ぐために、yのRankはXやパラメタと合わせた方がよい。\n",
    "y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "# 学習データと検証データを8:2に分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_true, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'shape of train data: X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'shape of validation data: X_val:{X_val.shape}, y_val:{y_val.shape}')\n",
    "\n",
    "\n",
    "# 学習データ・検証データの標準化。検証データの標準化には学習データの平均、標準偏差を使用することに注意。  \n",
    "X_mean = X_train.mean()\n",
    "X_std = X_train.std()\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_val = (X_val - X_mean) / X_std\n",
    "\n",
    "# データ数、特徴量数、隠れ層のノード数、最終的な出力数（回帰では1列）を定義\n",
    "m, nf = X_train.shape\n",
    "nh = 30\n",
    "n_out = 1\n",
    "\n",
    "# 入力層→隠れ層の重みW、バイアス項bの初期化\n",
    "W_1 = torch.randn((nh, nf)) * torch.sqrt(torch.tensor(2./nf))\n",
    "W_1.requires_grad = True\n",
    "B_1 = torch.zeros(size=(1, nh), requires_grad=True) # 1 x 出力\n",
    "\n",
    "# 隠れ層→出力層の重みW、バイアス項bの初期化\n",
    "W_2 = torch.randn((n_out, nh)) * torch.sqrt(torch.tensor(2./nf))\n",
    "W_2.requires_grad = True\n",
    "B_2 = torch.zeros(size=(1, n_out), requires_grad=True) # 1 x 出力\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ※ブロードキャスティングの注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回帰のため、yのone-hotエンコーディングが不要になったので、  \n",
    "読み込んだy(Rank1のTensor)をそのまま計算に使用すると、逆伝播の計算(下記)でZ2.grad_のshapeがおかしくなり、  \n",
    "行列積の計算が出来なくなってエラーになる。  \n",
    "これは不用意なブロードキャスティングが原因。\n",
    "\n",
    "```\n",
    "    Z2.grad_ = 2 * (A2 - y) / X.shape[0]\n",
    "    # 出力層→隠れ層のbackward\n",
    "    linear_backward(A1, W_2, B_2, Z2)  \n",
    "```  \n",
    "\n",
    "試しに下記の通り計算すると、欲しいshapeは(1437, 1)であるはずが、(1437, 1437)になってしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1437, 1437])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_trainを読み込んだままのshapeで計算してみる。\n",
    "Z2_grad_temp = 2 * (torch.randn((1437, 1)) - y_train.reshape(-1)) / X_train.shape[0]\n",
    "Z2_grad_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1437, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A2と同じRank2に合わせる\n",
    "Z2_grad_temp = 2 * (torch.randn((1437, 1)) - y_train.reshape(-1, 1)) / X_train.shape[0]\n",
    "Z2_grad_temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のようになるのは、ブロードキャスティングの下記のルールによる。  \n",
    "読み込んだ時のy_trainのshapeは(1437)なので、ブロードキャスティングの際にまず(1, 1437)になり、  \n",
    "その後A2のshape(1437, 1)に合うように(1437, 1437)へy_trainがブロードキャスティングされてしまう。  \n",
    "このような不意なブロードキャスティングを防ぐためにも計算前にあるべき形にshapeしておいた方がよい。  \n",
    "\n",
    "* ブロードキャスティングのルール  \n",
    "    * rank数が異なる場合，少ない方の配列のshapeの左側にサイズ1の次元を追加する(例: (2, 3) -> (1, 2, 3))  \n",
    "    * shapeの右側から値(サイズ数)を比較し、数が一致するか、サイズが1であればブロードキャスティングが可能  \n",
    "    * サイズ1の次元を大きい方の次元のサイズへ拡大する。この際、値はコピーされる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, Z1, A1, Z2, A2 = forward_and_backward(X_train, W_1, W_2, B_1, B_2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# autogradの結果と比較\n",
    "print(torch.allclose(Z1.grad_, Z1.grad))\n",
    "print(torch.allclose(A1.grad_, A1.grad))\n",
    "print(torch.allclose(W_1.grad_, W_1.grad))\n",
    "print(torch.allclose(B_1.grad_, B_1.grad))\n",
    "print(torch.allclose(Z2.grad_, Z2.grad))\n",
    "print(torch.allclose(A2.grad_, A2.grad))\n",
    "print(torch.allclose(W_2.grad_, W_2.grad))\n",
    "print(torch.allclose(B_2.grad_, B_2.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autogradの結果と等しいことが分かる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
