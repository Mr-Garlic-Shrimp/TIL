{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-4 ミニバッチ勾配降下法\n",
    "DL_notebook-3_Multinomial_Logistic_Regression.ipynbのコードをベースに  \n",
    "ミニバッチ勾配降下法を用いてMNISTを学習させる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●前処理・初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# python debugerをインポート\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数定義\n",
    "learning_rate = 0.03\n",
    "loss_log = []  #損失記録用のリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([1797, 64])\n",
      "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
      "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
      "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
      "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
      "         0.,  0.,  0., 11., 16., 10.,  0.,  0.])\n",
      "==========================\n",
      "shape of y_true: torch.Size([1797])\n",
      "tensor([0, 1, 2,  ..., 8, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "# データロード\n",
    "dataset = datasets.load_digits()\n",
    "feature_names = dataset['feature_names']\n",
    "X = torch.tensor(dataset['data'], dtype=torch.float32)\n",
    "target = torch.tensor(dataset['target'])\n",
    "\n",
    "# shape確認\n",
    "print(f'shape of X: {X.shape}')\n",
    "print(X[1])\n",
    "print('==========================')\n",
    "print(f'shape of y_true: {target.shape}')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_true: torch.Size([1797, 10])\n"
     ]
    }
   ],
   "source": [
    "# 目的変数のエンコーディング\n",
    "y_true = F.one_hot(target, num_classes=10)\n",
    "print(f'shape of y_true: {y_true.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの標準化（ピクセル値を標準化）\n",
    "X = torch.nan_to_num((X - X.mean(dim=0)) / X.std(dim=0), 0.)\n",
    "\n",
    "# 学習データの標準化（ピクセル値を標準化）\n",
    "# for i in range(X.shape[1]):\n",
    "#     mean = X[:, i].mean()\n",
    "#     std = X[:, i].std()\n",
    "#     if(std == 0.):\n",
    "#         X[:, i] = 0.\n",
    "#     else:\n",
    "#         X[:, i] = (X[:, i] - mean) / std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コメントアウトしている部分のようにfor文でもできるが、  \n",
    "上記のようにdim=0を指定して各列ごとに平均、標準偏差を求めた方が簡単。  \n",
    "ゼロ割はnan_to_num(tensor, 0)で変えておけばよい。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重みW^T、バイアス項bの初期化\n",
    "W = torch.rand(size=(10, 64) ,requires_grad=True) #出力×入力\n",
    "b = torch.rand(size=(1, 10), requires_grad=True) # 1 x 出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax関数の実装\n",
    "def softmax_func(X):\n",
    "    '''\n",
    "    X: input tensor.行は各データ、列は各クラスを想定。\n",
    "    '''\n",
    "    # x_kが大きすぎると、e^xがinfになるのでmax(x_1, x_2,...,x_K)を各x_kから引く。\n",
    "    # 各データ（各行）について最大値を求める必要があるので、dimにRank1(列方向で比較)を指定する。\n",
    "    max_val = X.max(dim=1, keepdim=True).values\n",
    "    # 各要素のe^xを計算（これが分子になる）\n",
    "    e_x = (X - max_val).exp()\n",
    "\n",
    "    # softmax関数の分母の計算\n",
    "    # 各データについて合計したいので、dim=1を設定。また、分母が0になることを防ぐために分母の式に1e-10を足しておく\n",
    "    denominator = e_x.sum(dim=1, keepdim=True) + 1e-10\n",
    "\n",
    "    return e_x / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多クラス分類に対応した交差エントロピー\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: tensor。One-Hot Encoding済みの正解ラベル。\n",
    "    y_pred: tensor。予測値。softmax関数の出力(0~100%)。\n",
    "    '''\n",
    "    # 損失を計算。最終的な損失はスカラーなので、dimを指定する必要はない。\n",
    "    return - (y_true * torch.log(y_pred + 1e-10)).sum() / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●ミニバッチ勾配降下法による学習\n",
    "実装の方針：\n",
    "* 毎回のepochの開始時に学習データをシャッフルし，毎回のepochで異なるミニバッチ群を作成\n",
    "    * Xとyで別々にシャッフルすると、特徴量と目的変数の組み合わせが変わってしまうので、  \n",
    "    Xかyのindexをシャッフルしてそこからミニバッチサイズ分取り出すという実装にした方が楽。  \n",
    "    (もちろんX, yは初期状態でindexが対応している前提)\n",
    "* 各バッチ毎の損失の平均を累積し， epochの最後にそのepochでの損失の平均を計算する(1データの平均損失を求める)\n",
    "* バッチサイズには32(2^5)を指定 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装前の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 953, 1183, 1662, ..., 1149,  612,  497])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 補足\n",
    "shuffled_indices = np.random.permutation(len(target))\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.permutation(整数)とすると、np.arange(整数)の結果をシャッフルして出力する。  \n",
    "リストの場合はそのままリストの要素がシャッフルされる。  \n",
    "なので、上記のように目的変数の数（＝データの数）を引数に入れれば、  \n",
    "index=0～データ数-1までのインデックスがシャッフルされた形で取得できる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start idx: 0, end idx: 32\n",
      "[ 953 1183 1662 1436  841 1527  174  830 1238 1515 1028 1142 1012  294\n",
      " 1150  154  663  115 1065   37  932  370 1312  172 1417 1272  169  474\n",
      "  484   73  605  393]\n",
      "start idx: 32, end idx: 64\n",
      "[1359 1153  785 1107 1757  776   26 1772 1226  540  866  842  797  438\n",
      "  401 1102  304 1109 1597  965 1245  446 1159  652  656 1450  187 1618\n",
      "  988 1510  989 1574]\n",
      "start idx: 64, end idx: 96\n",
      "[ 516  674  102 1437  606 1068  394 1268 1485 1103  379 1055 1025   83\n",
      "  495 1760  481 1579 1743  826  959  916 1498   50  994 1528  193  575\n",
      " 1472 1449 1707 1302]\n"
     ]
    }
   ],
   "source": [
    "# ミニバッチのサイズ定義\n",
    "batch_size = 2**5\n",
    "# 全ミニバッチの数。ミニバッチサイズで割ったときの余りも考慮してプラス１\n",
    "batch_num = len(target) // batch_size + 1\n",
    "# 最後のミニバッチのデータ数\n",
    "batch_remainder_num = len(target) % batch_size\n",
    "\n",
    "start = 0\n",
    "end = batch_size\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'start idx: {start}, end idx: {end}')\n",
    "    print(shuffled_indices[start : end])\n",
    "    start += batch_size\n",
    "\n",
    "    if (end + batch_size) <= 96:\n",
    "        end += batch_size\n",
    "    else:\n",
    "        end += batch_remainder_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch(0): 2.278509156745777\n",
      "Loss of epoch(1): 1.0543543665032638\n",
      "Loss of epoch(2): 0.7077103074182544\n",
      "Loss of epoch(3): 0.5572720235377028\n",
      "Loss of epoch(4): 0.47082332508605823\n"
     ]
    }
   ],
   "source": [
    "# ミニバッチのサイズ定義\n",
    "batch_size = 32\n",
    "\n",
    "# 全ミニバッチの数。ミニバッチサイズで割ったときの余りも考慮してプラス１\n",
    "batch_num = len(target) // batch_size + 1\n",
    "\n",
    "# 最後のミニバッチのデータ数\n",
    "batch_remainder_num = len(target) % batch_size\n",
    "\n",
    "\n",
    "# for文で学習ループ作成\n",
    "for epoch in range(5):\n",
    "    # epochごとの損失を蓄積する用の変数\n",
    "    running_loss = 0\n",
    "\n",
    "    # バッチごとの処理対象データ開始・終了インデックスを初期化\n",
    "    batch_start_idx = 0\n",
    "    batch_end_idx = batch_size\n",
    "\n",
    "    # シャッフル後のindex\n",
    "    shuffled_indices = np.random.permutation(len(target))\n",
    "\n",
    "    # ミニバッチ勾配降下法\n",
    "    for _ in range(batch_num):\n",
    "        #print(f'batch{i}: start idx:{batch_start_idx}, end idx:{batch_end_idx}')\n",
    "        # シャッフル後のindexからy,Xで同じ範囲を取り出しだしてミニバッチ作成\n",
    "        batch_indices = shuffled_indices[batch_start_idx : batch_end_idx]\n",
    "        y_true_batch = y_true[batch_indices, :]\n",
    "        X_batch = X[batch_indices, :]\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        # zの計算\n",
    "        z = X_batch @ W.T + b # 1 x クラス数\n",
    "\n",
    "        # softmaxで予測値算出\n",
    "        y_pred = softmax_func(z)\n",
    "\n",
    "        # 損失(L)計算.lossはtensorなので.item()で値だけ取り出す\n",
    "        loss = cross_entropy(y_true_batch, y_pred)\n",
    "        loss_log.append(loss.item())\n",
    "        # 計算したlossを累積\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Lの勾配計算。これをすることでw,bによるlossの偏微分係数が求められるようになる\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメタ更新。更新するだけなので勾配の保持は不要。\n",
    "        with torch.no_grad():\n",
    "            W -= learning_rate * W.grad\n",
    "            b -= learning_rate * b.grad\n",
    "\n",
    "        # 勾配初期化\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "        # batch開始・終了インデックスを更新\n",
    "        batch_start_idx += batch_size\n",
    "\n",
    "        if (batch_end_idx + batch_size) <= len(y_true):\n",
    "            batch_end_idx += batch_size\n",
    "        else:\n",
    "            # もしbatchの終了インデックスがデータ数を超えてしまったら（最後のbatchに到達したら）、\n",
    "            # 残りのデータ数分だけ終了インデックスをずらす\n",
    "            batch_end_idx += batch_remainder_num\n",
    "\n",
    "    # epochの最終的な損失を出力。各バッチの損失の累積を全バッチ数で割って平均を求める\n",
    "    print(f'Loss of epoch({epoch}): {running_loss / batch_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※補足 ミニバッチサイズで割り切れなかった余りのデータについて\n",
    "ミニバッチサイズで割り切れなかった余りのデータを取り扱うために  \n",
    "endのインデックスについて工夫をしたが、実はこれはスライシングの仕様上不要である。  \n",
    "何故ならば下記の通り、スライシングの終点のインデックスが配列の最大インデックスを超えていても  \n",
    "存在する範囲まで取得して返してくれるため。(少し気持ち悪いが実装が楽になるので有効活用しよう)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 611 1518  808 ... 1271  860  459]\n",
      "[ 611 1518  808 ... 1271  860  459]\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_indices[0:])\n",
    "print(shuffled_indices[0:2000000000000000000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※補足 pdb(python debuger)モジュールについて\n",
    "pdbはデバッグのためのモジュール。  \n",
    "pdb.set_trace()を差し込んだ箇所までコードが実行（ブレイクポイントの役割）され、  \n",
    "ipyの形でインタラクティブに変数の中身などを確認することが出来て非常に便利。  \n",
    "抜けるときはexitを入力する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●予測結果を見てみる\n",
    "とりあえず学習に使用したXを使用して、どのような結果になっているか見てみる。  \n",
    "(当然、学習データをインプットとするので精度的には過学習気味になる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果確認用の特徴量を定義\n",
    "X_val = X\n",
    "\n",
    "# 学習後のW,bを使って線形変換の式を計算\n",
    "Z_val = X_val @ W.T + b\n",
    "\n",
    "# 予測確率を出力\n",
    "y_pred_proba_val = softmax_func(Z_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.6086e-01, 2.6243e-05, 2.8885e-03, 1.1751e-04, 8.5666e-03, 3.9422e-04,\n",
       "        2.7395e-03, 7.4099e-03, 3.7360e-04, 1.6619e-02],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソフトマックス関数により確率はわかるが、最終的にどのクラス（0～9）の判定となったかを知りたい場合もある。  \n",
    "このようなときはtorch.argmax(dim=1)で各行のどのインデックスが最大値をとるのかを確認すればよい。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2,  ..., 8, 9, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(y_pred_proba_val, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2,  ..., 8, 9, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot後の正解ラベルも同様\n",
    "y_true.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8943)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracyを計算。==をとることにより、True or Falseが返される。\n",
    "# pythonではTrueは1,Falseは0の扱いなので、合計をとることで正解数がカウントできる。\n",
    "(torch.argmax(y_pred_proba_val, dim=1) == y_true.argmax(dim=1)).sum() / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データを検証データとして使用したため、当然高い値になる。  \n",
    "少なくとも全く頓珍漢な予測値にはなっていなさそうということはわかる。  \n",
    "ちゃんとした精度の検証は別途。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
