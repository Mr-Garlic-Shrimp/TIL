{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-11 PytorchのNNモジュール\n",
    "深層学習の仕組みの勉強のためにMLPをスクラッチで実装してきたが、  \n",
    "これはPytorchのtorch.nnモジュールに実装されている。  \n",
    "ただ、MLPのクラスがまるごとあるわけではなく、線形変換の層、活性化関数(ReLU)、損失関数など、  \n",
    "MLPを構成するコンポーネントごとに実装されている。  \n",
    "ユーザとしてはこれらのコンポーネントを自由に組み合わせて学習モデルを構築していくことになる。  \n",
    "\n",
    "* torch.nnに様々な深層学習のアーキテクチャを構築するためのクラスと関数が入っている\n",
    "* 層：NNの基本的な層のblock\n",
    "    * nn.Linear: 全結合層またはFC(Fully Connected)層ともいう。$XW^T+b$が実装されている。\n",
    "    * nn.ReLU: ReLU層\n",
    "* 損失関数：ネットワークのパフォーマンスを評価する\n",
    "    * nn.MSELoss: 平均二乗誤差\n",
    "    * nn.CrossEntropy: 交差エントロピー\n",
    "* モジュール：全てのNNモジュールの基本クラス。カスタムでクラスを作成する時はnn.Moduleを継承する\n",
    "* 関数：単一の関数として層や損失関数, 活性化関数を提供する。torch.nn.funcbonalに入っていて，通常はFとしてimportする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ nn.Linear\n",
    "線形変換を行う部分の層を表すクラス。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# NNモジュールを読み込む\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力の次元と出力の次元（隠れ層１層目ならば入力は特徴量数、出力は１層目のノード数）を指定してインスタンス生成\n",
    "linear = nn.Linear(64, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.1046e-02,  5.5857e-03,  2.5025e-02,  ...,  8.1937e-03,\n",
       "          -5.6370e-02, -1.0976e-01],\n",
       "         [-7.9472e-02,  6.6317e-02,  1.1154e-01,  ...,  2.5045e-02,\n",
       "           7.5435e-02,  5.9789e-02],\n",
       "         [-8.0149e-02, -1.1696e-01, -1.0465e-01,  ...,  1.1494e-01,\n",
       "           9.0277e-02, -1.0800e-01],\n",
       "         ...,\n",
       "         [-1.2304e-01,  5.6580e-05, -3.3194e-03,  ..., -2.8344e-02,\n",
       "           5.6041e-02, -6.5417e-02],\n",
       "         [ 4.2861e-03, -5.8977e-02,  1.1517e-01,  ...,  8.6463e-02,\n",
       "          -8.2400e-02,  8.5337e-02],\n",
       "         [ 4.0806e-02,  7.5585e-02, -1.6125e-02,  ..., -9.0116e-02,\n",
       "          -1.9493e-02, -9.3259e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0726,  0.0846, -0.0918, -0.0318, -0.0923, -0.0496,  0.0174,  0.0853,\n",
       "         -0.0805,  0.0751, -0.0512, -0.0234,  0.1060,  0.0289,  0.0721,  0.0784,\n",
       "         -0.0364, -0.1085,  0.0797,  0.0177,  0.0273, -0.0026, -0.0316, -0.0169,\n",
       "          0.0304,  0.1206, -0.0483, -0.0564,  0.0212,  0.0984],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成したLayerのパラメタを確認\n",
    "list(linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1046e-02,  5.5857e-03,  2.5025e-02,  ...,  8.1937e-03,\n",
      "         -5.6370e-02, -1.0976e-01],\n",
      "        [-7.9472e-02,  6.6317e-02,  1.1154e-01,  ...,  2.5045e-02,\n",
      "          7.5435e-02,  5.9789e-02],\n",
      "        [-8.0149e-02, -1.1696e-01, -1.0465e-01,  ...,  1.1494e-01,\n",
      "          9.0277e-02, -1.0800e-01],\n",
      "        ...,\n",
      "        [-1.2304e-01,  5.6580e-05, -3.3194e-03,  ..., -2.8344e-02,\n",
      "          5.6041e-02, -6.5417e-02],\n",
      "        [ 4.2861e-03, -5.8977e-02,  1.1517e-01,  ...,  8.6463e-02,\n",
      "         -8.2400e-02,  8.5337e-02],\n",
      "        [ 4.0806e-02,  7.5585e-02, -1.6125e-02,  ..., -9.0116e-02,\n",
      "         -1.9493e-02, -9.3259e-02]], requires_grad=True)\n",
      "torch.Size([30, 64])\n"
     ]
    }
   ],
   "source": [
    "# 重みを確認\n",
    "print(linear.weight)\n",
    "print(linear.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かにWのshapeが出力x入力になっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0726,  0.0846, -0.0918, -0.0318, -0.0923, -0.0496,  0.0174,  0.0853,\n",
      "        -0.0805,  0.0751, -0.0512, -0.0234,  0.1060,  0.0289,  0.0721,  0.0784,\n",
      "        -0.0364, -0.1085,  0.0797,  0.0177,  0.0273, -0.0026, -0.0316, -0.0169,\n",
      "         0.0304,  0.1206, -0.0483, -0.0564,  0.0212,  0.0984],\n",
      "       requires_grad=True)\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(linear.bias)\n",
    "print(linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((5, 64)) # 64の特徴量を持つ5データを定義\n",
    "# インスタンスにtensorを入れてcallすることで、forwardが計算される。\n",
    "Z = linear(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1680,  0.5441, -1.7527, -0.2023,  0.3643, -0.2527,  0.3505,  0.4083,\n",
      "         -0.4886, -0.9790, -0.1874,  0.0556,  0.8950, -1.1019, -0.1446,  0.0945,\n",
      "         -0.5899,  0.1869,  0.1683, -0.8990, -0.5504,  0.4096,  0.5353,  0.2216,\n",
      "         -0.9816,  0.2632, -0.4898,  0.2971,  1.1802, -0.1012],\n",
      "        [-0.5620,  0.9237,  0.3814, -1.2170, -0.2731, -0.1448,  0.1121, -0.4421,\n",
      "         -0.4533, -0.2588, -0.2827, -0.6483,  0.5038, -0.1167,  0.2720,  0.4148,\n",
      "         -0.5469,  0.0335,  0.0597, -0.1675, -0.1984, -0.8196, -0.4251,  0.2787,\n",
      "          0.6631,  0.1476, -0.6547, -0.2453, -0.3012,  0.0060],\n",
      "        [ 0.9574,  0.3612, -0.3115, -0.7415,  0.1366, -0.0830,  0.5199, -0.5133,\n",
      "          0.1683,  0.6514, -0.1524,  0.4644,  0.2014,  0.4153,  0.2770, -0.3030,\n",
      "          0.3074,  0.5833, -0.0500,  0.4514,  0.1932, -0.0686, -0.0691,  0.3298,\n",
      "          0.4217,  0.5895, -0.0710,  0.4347, -0.5996,  0.0111],\n",
      "        [ 0.2486,  0.8582, -1.4101,  0.6200,  0.2725, -0.0588,  0.3219, -0.1364,\n",
      "          0.0332, -1.2570, -0.1263,  0.4851,  0.6269, -0.5140, -0.0355,  0.6393,\n",
      "         -0.7267,  0.4634, -1.1640, -0.1204,  1.2300,  0.5006,  0.5641,  0.3972,\n",
      "         -0.0284,  0.2207, -0.1610,  0.3229,  0.3694, -0.2783],\n",
      "        [ 1.0617,  0.6707, -1.0252,  0.7843,  0.4062, -0.5579,  0.3583, -0.0216,\n",
      "          0.7448,  0.7394, -0.5654, -0.1382,  0.4584,  0.2618,  0.2230, -0.4487,\n",
      "          0.1406,  0.0697, -0.1194, -0.8515,  0.2580, -0.1495,  0.9872, -0.4788,\n",
      "         -0.4268,  0.3187, -0.4741, -0.6011,  1.1650,  0.4301]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "print(Z)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ nnモジュールでMLPモデルを作る\n",
    "作成の仕方は大きく３つあり、ここでは1.について実装する。  \n",
    "1. nn.<class>とF.<function>を組み合わせる  \n",
    "    * nn.Moduleクラスを継承してMLPクラスを作成\n",
    "    * パラメタを持つ層はnn.<class>で定義する。\n",
    "    * パラメタを持たない操作（活性化関数など）はF.<functional>で定義する  \n",
    "    →パラメタを持つ層の管理が楽というメリットがある。\n",
    "2. nn.<class>のみで構成する。\n",
    "3. nn.Sequentialを使う  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作成方法1.  nn.<class>とF.<function>を組み合わせる  \n",
    "* nn.Moduleクラスを継承してMLPクラスを作成\n",
    "* パラメタを持つ層はnn.<class>で定義する。\n",
    "* パラメタを持たない操作（活性化関数など）はF.<functional>で定義する  \n",
    "→パラメタを持つ層の管理が楽というメリットがある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1(nn.Module):\n",
    "    def __init__(self, num_in, num_hidden, num_out):\n",
    "        # 親クラスのinitを呼び出す。\n",
    "        super().__init__()\n",
    "        # 全結合層はパラメタを持つのでnn.で定義する\n",
    "        # num_in, num_hidden, num_outを渡すことにより全結合層を実体化させる\n",
    "        self.linear_1 = nn.Linear(num_in, num_hidden)\n",
    "        self.linear_2 = nn.Linear(num_hidden, num_out)\n",
    "\n",
    "    # nn.Moduleにもforwardメソッドがあり、ここでオーバーライドしている。\n",
    "    def forward(self, x):\n",
    "        # デバッグ用\n",
    "        # z1 = self.linear_1(z)\n",
    "        # a1 = F.relu(z1)\n",
    "        # z2 = self.linear_2(a1)\n",
    "        # 隠れ層の線形変換 →ReLu適用→ 隠れ層の線形変換を一気にやっている。\n",
    "        z = self.linear_2( F.relu( self.linear_1(x)) )\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_1(64, 30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0322, -0.1147,  0.2469, -0.1051, -0.0996,  0.0722, -0.0356,  0.1111,\n",
       "          0.1322,  0.0662]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.randn((1, 64))\n",
    "# forwardの計算\n",
    "model(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※補足  \n",
    "model()のような形で呼び出されるものは__call__()に実装するのがpythonの慣例。  \n",
    "上記のMLPクラスでは特に__call__を修正していないのに、  \n",
    "forwardが計算されたのは、親クラスのnn.Moduleにも__call__()があり、この中でforwardを呼び出す実装になっているため。  \n",
    "つまり、__call__は継承元のまま使い、forwardはオーバーライドしているということ。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作成方法2.  nn.<class>のみでモデルを構成する\n",
    "* パラメータを持たない操作もnn.<class>を使用する\n",
    "* モデル全体の構造に一貫性が生まれ，パラメータの有無を意識する必要がなくなる\n",
    "* 操作によっては，nn.<class>の方がF.<funcbon>よりも処理が重い場合もある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_2(nn.Module):\n",
    "    def __init__(self, num_in, num_hidden, num_out):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(num_in, num_hidden)\n",
    "        self.linear_2 = nn.Linear(num_hidden, num_out)\n",
    "        # ReLUは特に引数は必要ない\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2( self.relu( self.linear_1(x)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例では作成方法１とほぼ変わらない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_1(64, 30, 10)\n",
    "X = torch.randn(5, 64)\n",
    "Z = model(X)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作成方法3.  nn.Sequentialを使う\n",
    "* nn.Sequential()にnn.<class>のインスタンスを直列に引数に渡す\n",
    "* nn.Sequential()は，NNのアーキテクチャを直列に定義するための簡単な方法\n",
    "* 非常にシンプルなアーキテクチャを作成する際に便利。よく使う。\n",
    "* これ単体では複雑なモデルは作れないが、nn.Sequential()で作成したシンプルなモデルをいくつか組み合わせて  \n",
    "  複雑なモデルを表現することが可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in = 64\n",
    "num_hidden = 30\n",
    "num_out = 10\n",
    "\n",
    "# 下記のようにMLPのコンポーネントを順に書くだけ。\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(num_in, num_hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hidden, num_out)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = model(X)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3806, -0.0226, -0.2436, -0.0981, -0.0469, -0.4377, -0.0730, -0.0997,\n",
       "         -0.3881, -0.1324],\n",
       "        [-0.0778, -0.0197, -0.1164,  0.2043,  0.0170, -0.2359,  0.0838, -0.2144,\n",
       "         -0.1703,  0.1445],\n",
       "        [ 0.0878,  0.3312, -0.3318, -0.0419,  0.0191, -0.0939,  0.1373, -0.2027,\n",
       "         -0.2579,  0.3942],\n",
       "        [ 0.0815,  0.3040,  0.1592,  0.0576,  0.1506, -0.3425,  0.2689, -0.3373,\n",
       "         -0.1882,  0.0303],\n",
       "        [ 0.0986, -0.0412, -0.0029,  0.1724, -0.1277, -0.0636,  0.2368, -0.1094,\n",
       "         -0.0423,  0.2787]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ nnモジュールで作成したMLPモデルを学習させる\n",
    "上記で作成したnnモジュールによるMLPの実装を用いて実際に学習させてみる。  \n",
    "以降はGPUを使用するため、Google Colabで実行すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_1(\n",
       "  (linear_1): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (linear_2): Linear(in_features=30, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNISTデータセットの読み込み\n",
    "dataset = datasets.load_digits()\n",
    "X = torch.tensor( dataset.data , dtype=torch.float32) \n",
    "y = torch.tensor( dataset.target)\n",
    "y = F.one_hot(y, num_classes=10)\n",
    "\n",
    "# hold-out\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 標準化\n",
    "train_mean, train_std = X_train.mean(), X_train.std()\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_val = (X_val - train_mean) / train_std\n",
    "\n",
    "# モデルのコンストラクタに渡す引数を定義\n",
    "num_in = 64\n",
    "num_hidden = 30\n",
    "num_out = 10\n",
    "\n",
    "# モデル定義（２層のMLP）\n",
    "MLP_model = MLP_1(num_in=num_in, num_hidden=num_hidden, num_out=num_out)\n",
    "# requires_grad_をTrueに設定。最後に_がつくのは設定するという意味。\n",
    "MLP_model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズとバッチの個数を定義\n",
    "batch_size = 32\n",
    "num_batches = X_train.shape[0] // batch_size + 1\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "    #　インデックスをシャッフル\n",
    "    shuffled_indices = np.random.permutation(len(y_train))\n",
    "\n",
    "    # shuffled_indicesからの取り出し範囲初期化\n",
    "    idx_start = 0\n",
    "    idx_end = batch_size\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        indices_train = shuffled_indices[ idx_start:idx_end ]\n",
    "        X_train_batch = X_train[indices_train]\n",
    "        y_train_batch = y_train[indices_train]\n",
    "\n",
    "        # 予測\n",
    "        y_pred = F.softmax(MLP_model(X_train), dim=1)\n",
    "\n",
    "        # 損失計算\n",
    "        loss = F.cross_entropy(y_pred , y_train.to(torch.float32))\n",
    "        \n",
    "\n",
    "\n",
    "        # 取り出し範囲更新\n",
    "        idx_start += batch_size\n",
    "        idx_end += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = F.softmax(MLP_model(X_train), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.backward of tensor(2.3027, grad_fn=<DivBackward1>)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_temp.backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21985/463209734.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  temp.grad\n"
     ]
    }
   ],
   "source": [
    "temp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffled_indices[1230:1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[indices_train].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
