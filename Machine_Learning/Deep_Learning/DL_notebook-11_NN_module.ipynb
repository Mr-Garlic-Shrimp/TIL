{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-11 PytorchのNNモジュール\n",
    "深層学習の仕組みの勉強のためにMLPをスクラッチで実装してきたが、  \n",
    "これはPytorchのtorch.nnモジュールに実装されている。  \n",
    "ただ、MLPのクラスがまるごとあるわけではなく、線形変換の層、活性化関数(ReLU)、損失関数など、  \n",
    "MLPを構成するコンポーネントごとに実装されている。  \n",
    "ユーザとしてはこれらのコンポーネントを組み合わせて学習モデルを構築していくことになる。  \n",
    "\n",
    "* torch.nnに様々な深層学習のアーキテクチャを構築するためのクラスと関数が入っている\n",
    "* 層：NNの基本的な層のblock\n",
    "    * nn.Linear: 全結合層またはFC(Fully Connected)層ともいう。$XW^T+b$が実装されている。\n",
    "    * nn.ReLU: ReLU層\n",
    "* 損失関数：ネットワークのパフォーマンスを評価する\n",
    "    * nn.MSELoss: 平均二乗誤差\n",
    "    * nn.CrossEntropy: 交差エントロピー\n",
    "* モジュール：全てのNNモジュールの基本クラス。カスタムでクラスを作成する時はnn.Moduleを継承する\n",
    "* 関数：単一の関数として層や損失関数, 活性化関数を提供する。torch.nn.funcbonalに入っていて，通常はFとしてimportする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "線形変換を行う部分の層を表すクラス。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# NNモジュールを読み込む\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力の次元と出力の次元（隠れ層１層目ならば入力は特徴量数、出力は１層目のノード数）を指定してインスタンス生成\n",
    "linear = nn.Linear(64, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.1046e-02,  5.5857e-03,  2.5025e-02,  ...,  8.1937e-03,\n",
       "          -5.6370e-02, -1.0976e-01],\n",
       "         [-7.9472e-02,  6.6317e-02,  1.1154e-01,  ...,  2.5045e-02,\n",
       "           7.5435e-02,  5.9789e-02],\n",
       "         [-8.0149e-02, -1.1696e-01, -1.0465e-01,  ...,  1.1494e-01,\n",
       "           9.0277e-02, -1.0800e-01],\n",
       "         ...,\n",
       "         [-1.2304e-01,  5.6580e-05, -3.3194e-03,  ..., -2.8344e-02,\n",
       "           5.6041e-02, -6.5417e-02],\n",
       "         [ 4.2861e-03, -5.8977e-02,  1.1517e-01,  ...,  8.6463e-02,\n",
       "          -8.2400e-02,  8.5337e-02],\n",
       "         [ 4.0806e-02,  7.5585e-02, -1.6125e-02,  ..., -9.0116e-02,\n",
       "          -1.9493e-02, -9.3259e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0726,  0.0846, -0.0918, -0.0318, -0.0923, -0.0496,  0.0174,  0.0853,\n",
       "         -0.0805,  0.0751, -0.0512, -0.0234,  0.1060,  0.0289,  0.0721,  0.0784,\n",
       "         -0.0364, -0.1085,  0.0797,  0.0177,  0.0273, -0.0026, -0.0316, -0.0169,\n",
       "          0.0304,  0.1206, -0.0483, -0.0564,  0.0212,  0.0984],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成したLayerのパラメタを確認\n",
    "list(linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1046e-02,  5.5857e-03,  2.5025e-02,  ...,  8.1937e-03,\n",
      "         -5.6370e-02, -1.0976e-01],\n",
      "        [-7.9472e-02,  6.6317e-02,  1.1154e-01,  ...,  2.5045e-02,\n",
      "          7.5435e-02,  5.9789e-02],\n",
      "        [-8.0149e-02, -1.1696e-01, -1.0465e-01,  ...,  1.1494e-01,\n",
      "          9.0277e-02, -1.0800e-01],\n",
      "        ...,\n",
      "        [-1.2304e-01,  5.6580e-05, -3.3194e-03,  ..., -2.8344e-02,\n",
      "          5.6041e-02, -6.5417e-02],\n",
      "        [ 4.2861e-03, -5.8977e-02,  1.1517e-01,  ...,  8.6463e-02,\n",
      "         -8.2400e-02,  8.5337e-02],\n",
      "        [ 4.0806e-02,  7.5585e-02, -1.6125e-02,  ..., -9.0116e-02,\n",
      "         -1.9493e-02, -9.3259e-02]], requires_grad=True)\n",
      "torch.Size([30, 64])\n"
     ]
    }
   ],
   "source": [
    "# 重みを確認\n",
    "print(linear.weight)\n",
    "print(linear.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かにWのshapeが出力x入力になっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0726,  0.0846, -0.0918, -0.0318, -0.0923, -0.0496,  0.0174,  0.0853,\n",
      "        -0.0805,  0.0751, -0.0512, -0.0234,  0.1060,  0.0289,  0.0721,  0.0784,\n",
      "        -0.0364, -0.1085,  0.0797,  0.0177,  0.0273, -0.0026, -0.0316, -0.0169,\n",
      "         0.0304,  0.1206, -0.0483, -0.0564,  0.0212,  0.0984],\n",
      "       requires_grad=True)\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(linear.bias)\n",
    "print(linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((5, 64)) # 64の特徴量を持つ5データを定義\n",
    "# インスタンスにtensorを入れてcallすることで、forwardが計算される。\n",
    "Z = linear(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1680,  0.5441, -1.7527, -0.2023,  0.3643, -0.2527,  0.3505,  0.4083,\n",
      "         -0.4886, -0.9790, -0.1874,  0.0556,  0.8950, -1.1019, -0.1446,  0.0945,\n",
      "         -0.5899,  0.1869,  0.1683, -0.8990, -0.5504,  0.4096,  0.5353,  0.2216,\n",
      "         -0.9816,  0.2632, -0.4898,  0.2971,  1.1802, -0.1012],\n",
      "        [-0.5620,  0.9237,  0.3814, -1.2170, -0.2731, -0.1448,  0.1121, -0.4421,\n",
      "         -0.4533, -0.2588, -0.2827, -0.6483,  0.5038, -0.1167,  0.2720,  0.4148,\n",
      "         -0.5469,  0.0335,  0.0597, -0.1675, -0.1984, -0.8196, -0.4251,  0.2787,\n",
      "          0.6631,  0.1476, -0.6547, -0.2453, -0.3012,  0.0060],\n",
      "        [ 0.9574,  0.3612, -0.3115, -0.7415,  0.1366, -0.0830,  0.5199, -0.5133,\n",
      "          0.1683,  0.6514, -0.1524,  0.4644,  0.2014,  0.4153,  0.2770, -0.3030,\n",
      "          0.3074,  0.5833, -0.0500,  0.4514,  0.1932, -0.0686, -0.0691,  0.3298,\n",
      "          0.4217,  0.5895, -0.0710,  0.4347, -0.5996,  0.0111],\n",
      "        [ 0.2486,  0.8582, -1.4101,  0.6200,  0.2725, -0.0588,  0.3219, -0.1364,\n",
      "          0.0332, -1.2570, -0.1263,  0.4851,  0.6269, -0.5140, -0.0355,  0.6393,\n",
      "         -0.7267,  0.4634, -1.1640, -0.1204,  1.2300,  0.5006,  0.5641,  0.3972,\n",
      "         -0.0284,  0.2207, -0.1610,  0.3229,  0.3694, -0.2783],\n",
      "        [ 1.0617,  0.6707, -1.0252,  0.7843,  0.4062, -0.5579,  0.3583, -0.0216,\n",
      "          0.7448,  0.7394, -0.5654, -0.1382,  0.4584,  0.2618,  0.2230, -0.4487,\n",
      "          0.1406,  0.0697, -0.1194, -0.8515,  0.2580, -0.1495,  0.9872, -0.4788,\n",
      "         -0.4268,  0.3187, -0.4741, -0.6011,  1.1650,  0.4301]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "print(Z)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nnモジュールでMLPモデルを作る -1\n",
    "作成の仕方は大きく３つあり、ここでは1.について実装する。  \n",
    "1. nn.<class>とF.<function>を組み合わせる  \n",
    "    * nn.Moduleクラスを継承してMLPクラスを作成\n",
    "    * パラメタを持つ層はnn.<class>で定義する。\n",
    "    * パラメタを持たない操作（活性化関数など）はF.<functional>で定義する  \n",
    "    →パラメタを持つ層の管理が楽というメリットがある。\n",
    "2. nn.<class>のみで構成する。\n",
    "3. nn.Sequentialを使う  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_in, num_hidden, num_out):\n",
    "        # 親クラスのinitを呼び出す。\n",
    "        super().__init__()\n",
    "        # 全結合層はパラメタを持つのでnn.で定義する\n",
    "        self.linear_1 = nn.Linear(num_in, num_hidden)\n",
    "        self.linear_2 = nn.Linear(num_hidden, num_out)\n",
    "\n",
    "    # nn.Moduleにもforwardメソッドがあり、ここでオーバーライドしている。\n",
    "    def forward(self, x):\n",
    "        # デバッグ用\n",
    "        # z1 = self.linear_1(z)\n",
    "        # a1 = F.relu(z1)\n",
    "        # z2 = self.linear_2(a1)\n",
    "        # 隠れ層の線形変換 →ReLu適用→ 隠れ層の線形変換を一気にやっている。\n",
    "        z = self.linear_2( F.relu( self.linear_1(x)) )\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(64, 30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1167,  0.2524, -0.0680, -0.2275,  0.1086,  0.5086,  0.2032, -0.0763,\n",
       "          0.2407,  0.5020]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.randn((1, 64))\n",
    "# forwardの計算\n",
    "model(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※補足  \n",
    "model()のような形で呼び出されるものは__call__()に実装するのがpythonの慣例。  \n",
    "上記のMLPクラスでは特に__call__を修正していないのに、  \n",
    "forwardが計算されたのは、親クラスのnn.Moduleにも__call__()があり、この中でforwardを呼び出す実装になっているため。  \n",
    "つまり、__call__は継承元のまま使い、forwardはオーバーライドしているということ。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
