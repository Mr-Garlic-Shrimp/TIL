{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-6 誤差逆伝播(Backpropagation)\n",
    "誤差逆伝播をスクラッチで実装してみる。  \n",
    "まず大前提として、誤差逆伝播は損失が少なくなるようにパラメータを更新するためのものであるから、  \n",
    "$\\frac{\\partial{L}}{\\partial{W^{[l]}}}$ と $\\frac{\\partial{L}}{\\partial{B^{[l]}}}$ をいかに求めるかが実装する上で重要なところ。  \n",
    "\n",
    "* $l$番目の層における損失の誤差 \n",
    "$$\\delta^{[l]}=\\frac{\\partial{L}}{\\partial{Z^{[l]}}}$$  \n",
    "$$=\\frac{\\partial{L}}{\\partial{A^{[l]}}} \\bigodot \\frac{\\partial{A}}{\\partial{Z^{[l]}}}$$\n",
    "$$=(\\delta^{[l+1]}\\textbf{W}^{[l+1]}) \\bigodot \\sigma'(\\textbf{Z}^{[l]})$$  \n",
    "\n",
    "* パラメタの勾配\n",
    "$$\\frac{\\partial{L}}{\\partial{W^{[l]}}}=\\delta^{[l]T}\\textbf{A}^{[l-1]}$$  \n",
    "$$\\frac{\\partial{L}}{\\partial{b^{[l]}}}=\\sum_i\\delta_i^{[l]}$$  \n",
    "\n",
    "* パラメタの更新\n",
    "$$\\textbf{W}^{[l]}=\\textbf{W}^{[l]}-\\alpha\\frac{\\partial{L}}{\\partial{W}^{[l]}}$$  \n",
    "$$\\textbf{B}^{[l]}=\\textbf{B}^{[l]}-\\alpha\\frac{\\partial{L}}{\\partial{B}^{[l]}}$$  \n",
    "\n",
    "\n",
    "※ReLUの導関数\n",
    "\\begin{equation}\n",
    "\\sigma^{'}(z)= \\left \\{\n",
    "\\begin{array}{l}\n",
    "1　(if z > 0) \\\\\n",
    "0　(otherwise)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# python debugerをインポート\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ backward（逆伝播）の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形変換部分(パラメタ)の勾配の算出関数。Aはl-1層目の値で他はl層目の値。\n",
    "def linear_backward(A, W, b, Z):\n",
    "     # pytorch.tensorの.grad属性と区別するために.grad_として属性を定義している\n",
    "     W.grad_ = Z.grad_.T @ A\n",
    "     # bの値が一つ変わると、全データ分Lへ影響を及ぼすので、行方向(データ数の方向)にZ.grad_を足す。\n",
    "     b.grad_ = torch.sum(Z.grad_, dim=0)\n",
    "     # l-1層目のAはl層目の誤差（=Z.grad_）とl層目のWから計算できる。\n",
    "     A.grad_ = Z.grad_ @ W\n",
    "\n",
    "\n",
    "# ReLU関数\n",
    "def relu_backward(Z, A):\n",
    "     # 上式の\"l番目の層における損失の誤差\"における2行目と3行目を組み合わせてdL/dZを算出している\n",
    "     # A.grad_はdL/dA, (Z > 0).float()はReLu関数の偏微分に相当する。\n",
    "     Z.grad_ = A.grad_ * ( Z > 0 ).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補足1\n",
    "tensor * (tensor > 0)の計算について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1594,  1.7124, -1.4743],\n",
      "        [ 0.4990,  0.4816,  1.3858]])\n",
      "tensor([[ 0.5913, -1.5723, -0.5917],\n",
      "        [-0.5025, -0.0819, -0.7331]])\n",
      "tensor([[ True, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((2,3)) \n",
    "b = torch.randn((2,3)) \n",
    "print(a)\n",
    "print(b)\n",
    "print(b > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1594, 0.0000, -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * (b > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにb > 0がTrueの要素は1、Falseの要素は0として計算される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように損失、パラメタの勾配をコードで表わすことが出来た。  \n",
    "ではこれらの計算を実際に実行するにはどうすればよいか？  \n",
    "当然、具体的な計算を行うためには上記の関数の入力であるA,W,b,Zを求める必要がある。  \n",
    "すなわち順伝播(forward)の計算を行い、損失を計算してから逆伝播(backward)の計算を行うことになる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず損失を求める関数を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax関数と交差エントロピーの計算を同じ関数で実装する（pytorchでもこうなっている）\n",
    "def softmax_cross_entropy(X, y_true):\n",
    "    '''\n",
    "    X: input tensor.行は各データ、列は各クラスを想定。\n",
    "    y_true: tensor。One-Hot Encoding済みの正解ラベル。\n",
    "    '''\n",
    "    max_val = X.max(dim=1, keepdim=True).values\n",
    "    # 各要素のe^xを計算（これが分子になる）\n",
    "    e_x = (X - max_val).exp()\n",
    "    denominator = e_x.sum(dim=1, keepdim=True) + 1e-10\n",
    "    softmax_out = e_x / denominator\n",
    "    loss = - (y_true * torch.log(softmax_out + 1e-10)).sum() / y_true.shape[0]\n",
    "\n",
    "    return loss, softmax_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorの線形結合を返す関数\n",
    "def linear_comb(X, W, B):\n",
    "    '''\n",
    "    X, W, B: torch.tensor\n",
    "    '''\n",
    "    return X @ W.T + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLUの実装\n",
    "def ReLU(Z):\n",
    "    '''\n",
    "    Z: torch.tensor\n",
    "    '''\n",
    "    # torch.whereによって要素ごとに条件が真・偽のときで別の値を返せる\n",
    "    # 下記では0より大きい要素はzの値そのままで、0以下は0.になる。\n",
    "    return torch.where(Z > 0 , Z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "順伝播→逆伝播の計算を行う関数を定義。  \n",
    "中間層の勾配もスクラッチの結果と等しいか確認したいので、  \n",
    ".retain_grad()を次の計算の前に呼び出している。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 隠れ層１層の場合のMLP\n",
    "# def forward_and_backward(X, W_list, B_list, y):\n",
    "#     '''\n",
    "#     X: features\n",
    "#     W: List of weights(each edge)\n",
    "#     B: List of Bias(each Layer)\n",
    "#     '''\n",
    "\n",
    "#     #########################\n",
    "#     #### 順伝播（forward）####\n",
    "#     #########################\n",
    "\n",
    "#     # 入力層→隠れ層\n",
    "#     Z1 = linear_comb(X, W_list[0], B_list[0])\n",
    "#     Z1.retain_grad()\n",
    "\n",
    "#     # 活性化関数適用\n",
    "#     A1 = ReLU(Z1)\n",
    "#     A1.retain_grad()\n",
    "\n",
    "#     # 隠れ層→出力層\n",
    "#     Z2 = linear_comb(A1, W_list[1], B_list[1])\n",
    "#     Z2.retain_grad()\n",
    "\n",
    "#     # 最終出力。損失とソフトマックス関数の出力が返される\n",
    "#     loss, A2 = softmax_cross_entropy(Z2, y)\n",
    "\n",
    "\n",
    "#     #########################\n",
    "#     #### 逆伝播（backward）####\n",
    "#     #########################\n",
    "\n",
    "#     # 最終出力→出力層の出力のbackward。ここで各層の出力の勾配を求める。\n",
    "#     # ここでは出力層の活性化関数は恒等関数とし、\n",
    "#     # その出力にsoftmax_cross_entropyを適用した結果をモデルの最終出力として考えている。\n",
    "#     Z2.grad_ = (A2 - y) / X.shape[0]\n",
    "#     # 出力層→隠れ層のbackward\n",
    "#     linear_backward(A1, W_list[1], B_list[1], Z2)\n",
    "#     # 隠れ層の出力側のbackward\n",
    "#     relu_backward(Z1, A1)\n",
    "#     # 隠れ層の入力側のbackward\n",
    "#     linear_backward(X, W_list[0], B_list[0], Z1)\n",
    "    \n",
    "#     return loss, Z1, A1, Z2, A2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隠れ層１層の場合のMLP（スクラッチ）\n",
    "def forward_and_backward(X, W_1, W_2, B_1, B_2, y):\n",
    "    '''\n",
    "    X: features\n",
    "    W: List of weights(each edge)\n",
    "    B: List of Bias(each Layer)\n",
    "    '''\n",
    "\n",
    "    #########################\n",
    "    #### 順伝播（forward）####\n",
    "    #########################\n",
    "\n",
    "    # 入力層→隠れ層\n",
    "    Z1 = linear_comb(X, W_1, B_1)\n",
    "    Z1.retain_grad()\n",
    "\n",
    "    # 活性化関数適用\n",
    "    A1 = ReLU(Z1)\n",
    "    A1.retain_grad()\n",
    "\n",
    "    # 隠れ層→出力層\n",
    "    Z2 = linear_comb(A1, W_2, B_2)\n",
    "    Z2.retain_grad()\n",
    "\n",
    "    # 最終出力。損失とソフトマックス関数の出力が返される\n",
    "    loss, A2 = softmax_cross_entropy(Z2, y)\n",
    "\n",
    "\n",
    "    #########################\n",
    "    #### 逆伝播（backward）####\n",
    "    #########################\n",
    "\n",
    "    # 最終出力→出力層の出力のbackward。ここで各層の出力の勾配を求める。\n",
    "    # ここでは出力層の活性化関数は恒等関数とし、\n",
    "    # その出力にsoftmax_cross_entropyを適用した結果をモデルの最終出力として考えている。\n",
    "    Z2.grad_ = (A2 - y) / X.shape[0]\n",
    "    # 出力層→隠れ層のbackward\n",
    "    linear_backward(A1, W_2, B_2, Z2)\n",
    "    # 隠れ層の出力側のbackward\n",
    "    relu_backward(Z1, A1)\n",
    "    # 隠れ層の入力側のbackward\n",
    "    linear_backward(X, W_1, B_1, Z1)\n",
    "    \n",
    "    return loss, Z1, A1, Z2, A2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補足：Z2.grad_をデータ数X.shape[0]で割る理由  \n",
    "交差エントロピーで算出した全体の損失Lはデータ数で割った平均の形なので、右辺もデータ数で割る必要あり。  \n",
    "各データに対応する損失をデータ数で割ることで、最終的な全体の損失Lへ与える影響をならしているイメージ。  \n",
    "誤差逆伝播の講義資料のdL/dZ[2]の式を参照。両辺に1/mが実際はかかっているということ。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ Autogradの結果と一致することを確認\n",
    "MNISTデータセットを使って、順伝播後に逆伝播を計算する。  \n",
    "前処理部分は前回までのコードの使いまわし。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([1797, 64])\n",
      "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
      "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
      "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
      "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
      "         0.,  0.,  0., 11., 16., 10.,  0.,  0.])\n",
      "==========================\n",
      "shape of y_train: torch.Size([1797])\n",
      "tensor([0, 1, 2,  ..., 8, 9, 8])\n",
      "shape of y_true: torch.Size([1797, 10])\n",
      "shape of train data: X_train:torch.Size([1437, 64]), y_train:torch.Size([1437, 10])\n",
      "shape of validation data: X_val:torch.Size([360, 64]), y_val:torch.Size([360, 10])\n"
     ]
    }
   ],
   "source": [
    "# 変数定義\n",
    "learning_rate = 0.03\n",
    "loss_log = []  #学習時の損失記録用のリスト\n",
    "\n",
    "# データロード\n",
    "dataset = datasets.load_digits()\n",
    "feature_names = dataset['feature_names']\n",
    "X = torch.tensor(dataset['data'], dtype=torch.float32)\n",
    "target = torch.tensor(dataset['target'])\n",
    "\n",
    "# shape確認\n",
    "print(f'shape of X: {X.shape}')\n",
    "print(X[1])\n",
    "print('==========================')\n",
    "print(f'shape of y_train: {target.shape}')\n",
    "print(target)\n",
    "\n",
    "# 目的変数のエンコーディング\n",
    "y_true = F.one_hot(target, num_classes=10)\n",
    "print(f'shape of y_true: {y_true.shape}')\n",
    "\n",
    "# 学習データと検証データを8:2に分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_true, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'shape of train data: X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'shape of validation data: X_val:{X_val.shape}, y_val:{y_val.shape}')\n",
    "\n",
    "# 学習データ・検証データの標準化。検証データの標準化には学習データの平均、標準偏差を使用することに注意。  \n",
    "X_mean = X_train.mean()\n",
    "X_std = X_train.std()\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_val = (X_val - X_mean) / X_std\n",
    "\n",
    "# データ数、特徴量数、隠れ層のノード数、最終的な出力数（ここではクラス数）を定義\n",
    "m, nf = X_train.shape\n",
    "nh = 30\n",
    "n_out = 10\n",
    "\n",
    "# 入力層→隠れ層の重みW、バイアス項bの初期化\n",
    "W_1 = torch.randn(size=(nh, nf) ,requires_grad=True) #出力×入力\n",
    "B_1 = torch.zeros(size=(1, nh), requires_grad=True) # 1 x 出力\n",
    "\n",
    "# 隠れ層→出力層の重みW、バイアス項bの初期化\n",
    "W_2 = torch.randn(size=(n_out, nh) ,requires_grad=True) #出力×入力\n",
    "B_2 = torch.zeros(size=(1, n_out), requires_grad=True) # 1 x 出力\n",
    "# W_list = [W_1, W_2]\n",
    "# B_list = [B_1, B_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクラッチで実装した順伝播→逆伝播の関数を実行。\n",
    "# これでスクラッチバージョンの勾配(.grad_)が得られる。\n",
    "loss, Z1, A1, Z2, A2 = forward_and_backward(X_train, W_1, W_2, B_1, B_2, y_train)\n",
    "\n",
    "# autogradバージョンの逆伝播(.grad)を得たいのならば、普通に最終的なlossの結果に.backward()するでOK\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4096e-10,  7.6112e-17,  1.4405e-20,  ...,  6.9581e-04,\n",
       "          7.5372e-31,  7.2105e-34],\n",
       "        [-3.4070e-16,  7.9003e-38,  0.0000e+00,  ...,  3.4070e-16,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-1.3026e-14,  9.1311e-36,  0.0000e+00,  ...,  1.3026e-14,\n",
       "          4.2179e-43,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 2.0859e-20,  4.4915e-34, -1.5268e-13,  ...,  2.0861e-33,\n",
       "          1.1953e-42,  2.8026e-45],\n",
       "        [ 2.4493e-18,  2.7161e-06,  5.0713e-30,  ..., -6.9585e-04,\n",
       "          1.5190e-32,  1.2584e-23],\n",
       "        [ 2.1766e-26, -6.9558e-04,  7.1603e-21,  ...,  5.7043e-24,\n",
       "          1.7906e-29,  8.6660e-29]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autogradのdL/dZ2の結果\n",
    "Z2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4096e-10,  7.6112e-17,  1.4405e-20,  ...,  6.9581e-04,\n",
       "          7.5372e-31,  7.2105e-34],\n",
       "        [-6.9589e-04,  1.6137e-25,  1.0536e-37,  ...,  6.9589e-04,\n",
       "          1.6675e-43,  1.2831e-33],\n",
       "        [-6.9589e-04,  4.8779e-25,  2.1961e-35,  ...,  6.9589e-04,\n",
       "          2.2568e-32,  1.1918e-39],\n",
       "        ...,\n",
       "        [ 9.5073e-11,  2.0472e-24, -6.9589e-04,  ...,  9.5082e-24,\n",
       "          5.4504e-33,  1.0563e-35],\n",
       "        [ 2.4493e-18,  2.7161e-06,  5.0713e-30,  ..., -6.9585e-04,\n",
       "          1.5190e-32,  1.2584e-23],\n",
       "        [ 2.1766e-26, -6.9558e-04,  7.1603e-21,  ...,  5.7043e-24,\n",
       "          1.7906e-29,  8.6660e-29]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スクラッチのdL/dZ2の結果\n",
    "Z2.grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.allcloseで大体同じか否かを判定できる。\n",
    "torch.allclose(Z2.grad, Z2.grad_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15.1223,  -0.1928,  -8.7652,  ...,  29.6511, -32.4388, -39.3908],\n",
       "        [  5.4322,   6.9875, -21.0698,  ...,  56.8033, -34.4301, -11.6624],\n",
       "        [  7.9314,   6.9492, -16.8747,  ...,  55.6588,  -9.9397, -26.6963],\n",
       "        ...,\n",
       "        [ 19.7737, -11.6955,  -9.6862,  ..., -10.1598, -31.4395, -37.6856],\n",
       "        [ -2.7247,  25.0097, -29.6279,  ...,  20.9228, -35.4386, -14.9036],\n",
       "        [-16.7597,  27.3490,  -4.0560,  ..., -11.1911, -23.8626, -22.2858]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z2の勾配を計算すると、  \n",
    "スクラッチとautogradでデータによって同じだったり、異なったりしていることが分かる。  \n",
    "\n",
    "深層学習の問題点の一つとして、Xに始まる入力に対して各層で線形結合を繰り返すと    \n",
    "出力結果が元のXの分布からどんどん大きくなっていき、 大きくなり過ぎた出力を最終的にsoftmax関数へ入れることで、  \n",
    "eの累乗の計算が大きくなり過ぎて勾配計算に誤差が出るという点がある。  \n",
    "上記におけるsoftmax関数の入力はZ2であり、Z2を見ると指数としては大きい値になっていることが分かる。\n",
    "\n",
    "上記ではsoftmax関数への入力が非常に大きい場合の取り扱い方が、  \n",
    "スクラッチ実装とpytorchの.backward()の実装で恐らく違いがあるため、上記のようにところどころ計算が合わなくなったのだろう。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 一致しないことに対する対処\n",
    "出力が大きくなり過ぎないための工夫として、  \n",
    "重みの初期化の際にrandn（標準正規分布からのサンプリング）ではなく、  \n",
    "平均0, 分散$\\frac{2}{n}$(nは特徴量数) の正規分布から取り出すというものが知られている。（下記）  \n",
    "なぜこれでうまくいくのかは後述。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ数、特徴量数、隠れ層のノード数、最終的な出力数（ここではクラス数）を定義\n",
    "m, nf = X_train.shape\n",
    "nh = 30\n",
    "n_out = 10\n",
    "\n",
    "# 入力層→隠れ層の重みW、バイアス項bの初期化\n",
    "# W_1 = torch.randn(size=(nh, nf) ,requires_grad=True) #出力×入力\n",
    "\n",
    "# randnは分散が1なので、2/nのルートをかければよい。\n",
    "W_1 = torch.randn((nh, nf)) * torch.sqrt(torch.tensor(2./nf))\n",
    "W_1.requires_grad = True # 一つ上のrandnでTrueにしても計算後のtensorに引き継がれないので別途設定する必要あり。\n",
    "B_1 = torch.zeros(size=(1, nh), requires_grad=True) # 1 x 出力\n",
    "\n",
    "# 隠れ層→出力層の重みW、バイアス項bの初期化\n",
    "#W_2 = torch.randn(size=(n_out, nh) ,requires_grad=True) #出力×入力\n",
    "W_2 = torch.randn((n_out, nh)) * torch.sqrt(torch.tensor(2./nf))\n",
    "W_2.requires_grad = True \n",
    "\n",
    "B_2 = torch.zeros(size=(1, n_out), requires_grad=True) # 1 x 出力\n",
    "# W_list = [W_1, W_2]\n",
    "# B_list = [B_1, B_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記の工夫を踏まえて再計算\n",
    "loss, Z1, A1, Z2, A2 = forward_and_backward(X_train, W_1, W_2, B_1, B_2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0297, -0.0325, -0.0345,  ...,  0.0139, -0.0218, -0.0273],\n",
       "        [ 0.0149,  0.0135, -0.0075,  ...,  0.0006,  0.0145,  0.0135],\n",
       "        [-0.0163, -0.0158, -0.0057,  ...,  0.0050, -0.0110, -0.0192],\n",
       "        ...,\n",
       "        [-0.0068, -0.0050,  0.0362,  ...,  0.0109, -0.0066, -0.0065],\n",
       "        [-0.0153, -0.0136,  0.0118,  ...,  0.0141, -0.0152, -0.0173],\n",
       "        [-0.0013, -0.0012, -0.0020,  ..., -0.0097, -0.0072, -0.0025]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autogradの結果\n",
    "W_1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0297, -0.0325, -0.0345,  ...,  0.0139, -0.0218, -0.0273],\n",
       "        [ 0.0149,  0.0135, -0.0075,  ...,  0.0006,  0.0145,  0.0135],\n",
       "        [-0.0163, -0.0158, -0.0057,  ...,  0.0050, -0.0110, -0.0192],\n",
       "        ...,\n",
       "        [-0.0068, -0.0050,  0.0362,  ...,  0.0109, -0.0066, -0.0065],\n",
       "        [-0.0153, -0.0136,  0.0118,  ...,  0.0141, -0.0152, -0.0173],\n",
       "        [-0.0013, -0.0012, -0.0020,  ..., -0.0097, -0.0072, -0.0025]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スクラッチ実装の結果\n",
    "W_1.grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# autogradと大体等しいことを確認\n",
    "print(torch.allclose(W_1.grad_, W_1.grad))\n",
    "print(torch.allclose(B_1.grad_, B_1.grad))\n",
    "print(torch.allclose(W_2.grad_, W_2.grad))\n",
    "print(torch.allclose(B_2.grad_, B_2.grad))\n",
    "print(torch.allclose(Z1.grad_, Z1.grad))\n",
    "print(torch.allclose(Z1.grad_, Z1.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工夫前と異なり、autogradと結果が等しくなった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0273, -1.0354, -0.5734,  ..., -0.1903, -1.3035, -0.9548],\n",
       "        [ 0.2881, -0.1615, -0.9772,  ..., -0.0853, -0.3612, -0.9244],\n",
       "        [ 0.7111, -0.9237, -1.0729,  ...,  0.7416, -1.7103, -1.1776],\n",
       "        ...,\n",
       "        [ 0.7833, -0.5741,  0.4119,  ...,  0.0655, -1.5887,  0.3286],\n",
       "        [ 0.2352, -0.1118, -0.3671,  ...,  0.4156, -1.2270, -0.6012],\n",
       "        [-1.7943, -0.2663,  1.3029,  ..., -0.1775, -0.6772, -0.8382]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工夫の結果、softmaxへの入力Z2がそれほど大きくならなくなったことが分かる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
