{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-6 誤差逆伝播(Backpropagation)\n",
    "誤差逆伝播をスクラッチで実装してみる   \n",
    "* $l$番目の層における損失の誤差 \n",
    "$$\\delta^{[l]}=\\frac{\\partial{L}}{\\partial{Z^{[l]}}}$$  \n",
    "$$=\\frac{\\partial{L}}{\\partial{A^{[l]}}} \\bigodot \\frac{\\partial{A}}{\\partial{Z^{[l]}}}$$\n",
    "$$=(\\delta^{[l+1]}\\bm{W}^{[l+1]}) \\bigodot \\sigma'(\\bm{Z^{[l]}})$$  \n",
    "\n",
    "* パラメタの勾配\n",
    "$$\\frac{\\partial{L}}{\\partial{W^{[l]}}}=\\delta^{[l]T}\\bm{A}^{[l-1]}$$  \n",
    "$$\\frac{\\partial{L}}{\\partial{b^{[l]}}}=\\sum_i\\delta_i^{[l]}$$  \n",
    "\n",
    "* パラメタの更新\n",
    "$$\\bm{W^{[l]}}=\\bm{W^{[l]}}-\\alpha\\frac{\\partial{L}}{\\partial{W^{[l]}}}$$  \n",
    "$$\\bm{B^{[l]}}=\\bm{B^{[l]}}-\\alpha\\frac{\\partial{L}}{\\partial{B^{[l]}}}$$  \n",
    "\n",
    "\n",
    "※ReLUの導関数\n",
    "\\begin{equation}\n",
    "\\sigma^{'}(z)= \\left \\{\n",
    "\\begin{array}{l}\n",
    "1　(if z > 0) \\\\\n",
    "0　(otherwise)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# python debugerをインポート\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形変換部分(パラメタ)の勾配の算出関数。Aはl-1層目の値で他はl層目の値。\n",
    "def linear_backward(A, W, b, Z):\n",
    "     # pytorch.tensorの.grad属性と区別するために.grad_として属性を定義している\n",
    "     W.grad_ = Z.grad_ @ A\n",
    "     # bの値が一つ変わると、全データ分Lへ影響を及ぼすので、行方向(データ数の方向)にZ.grad_を足す。\n",
    "     b.grad_ = torch.sum(Z.grad_, dim=0)\n",
    "     # l-1層目のAはl層目の誤差（=Z.grad_）とl層目のWから計算できる。\n",
    "     A.grad_ = Z.grad_ @ W\n",
    "\n",
    "\n",
    "# ReLU関数\n",
    "def relu_backward(Z, A):\n",
    "     # 上式の\"l番目の層における損失の誤差\"における2行目と3行目を組み合わせてdL/dZを算出している\n",
    "     # A.grad_はdL/dA, (Z > 0).float()はReLu関数の偏微分に相当する。\n",
    "     Z.grad_ = A.grad_ * ( Z > 0 ).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 補足\n",
    "tensor * (tensor > 0)の計算について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8516, -1.0509,  0.1294],\n",
      "        [-0.1999,  1.1394,  1.4127]])\n",
      "tensor([[ 0.1299,  0.2585, -1.2203],\n",
      "        [ 0.6629, -0.2176, -1.2055]])\n",
      "tensor([[ True,  True, False],\n",
      "        [ True, False, False]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((2,3)) \n",
    "b = torch.randn((2,3)) \n",
    "print(a)\n",
    "print(b)\n",
    "print(b > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8516, -1.0509,  0.0000],\n",
       "        [-0.1999,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * (b > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のようにb > 0がTrueの要素は1、Falseの要素は0として計算される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
