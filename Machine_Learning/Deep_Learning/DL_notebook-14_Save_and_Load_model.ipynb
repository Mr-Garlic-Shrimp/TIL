{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[深層学習ノートブック-14 学習済みモデルの保存とロード](#toc0_)\n",
    "学習を終えたモデルを保存することで、他人に共有したり、システムに組み込んで実用化することが出来る。  \n",
    "保存するモデルとしては、学習曲線における検証データの損失が最も小さいモデルを保存するのが一般的。  \n",
    "このモデルの選定にはEaly Stoppingを用いると効率的。\n",
    "\n",
    "参考：  \n",
    "* [Udemy講座：「①米国AI開発者がやさしく教える深層学習超入門第一弾【Pythonで実践】」](https://www.udemy.com/course/deeplearning1/learn/lecture/40143418)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ early stopping\n",
    "学習を途中で止め、過学習を防ぐ正則化手法の一つ。  \n",
    "検証データに対する損失は学習を進めすぎると、あるepochから劣化し始める（過学習）ので、  \n",
    "適当な回数のepochで損失がそれより下がらなければ学習を終えるのが良い。  \n",
    "すなわち、各epochの終了時に学習データと検証データの両方で評価し、検証データにおける損失が一定のepoch数で減少しなかったら学習をストップする。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1(nn.Module):\n",
    "    def __init__(self, num_in, num_hidden, num_out):\n",
    "        # 親クラスのinitを呼び出す。\n",
    "        super().__init__()\n",
    "        # nn.Flattenにより[b, c, h, w] -> [b, c x h x w]（ミニバッチサイズ, 画像の特徴量数の積）に変換\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(num_in, num_hidden)\n",
    "        self.linear_2 = nn.Linear(num_hidden, num_out)\n",
    "\n",
    "    # nn.Moduleにもforwardメソッドがあり、ここでオーバーライドしている。\n",
    "    def forward(self, x):\n",
    "        # 最初の全結合層の計算に渡す際にnn.Flattenを実行しておく必要あり。\n",
    "        z = self.linear_2( F.relu( self.linear_1(self.flatten(x))) )\n",
    "        return z\n",
    "    \n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self): #lenを適用したときの挙動を定義\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx): #indexingしたときの挙動\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズを定義。今回は１ミニバッチ＝３２データとする\n",
    "batch_size = 32\n",
    "\n",
    "# データ読み込み。8x8のMNISTデータセットをカスタムデータセットとして読み込む。（練習のため）\n",
    "dataset = datasets.load_digits()\n",
    "X = dataset['images'] # 0~16の値をもつ\n",
    "X = ( X * (255. / 16.) ).astype(np.uint8) # 下記補足参照\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # 0~255 -> 0~1\n",
    "    transforms.Normalize((0.5,), (0.5)) # 0~1 -> -1~1\n",
    "])\n",
    "\n",
    "# Datasetとして読み込み\n",
    "train_dataset = MyDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = MyDataset(X_val, y_val, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, num_workers=2)\n",
    "\n",
    "# モデルのコンストラクタに渡す引数を定義。\n",
    "num_in = 64\n",
    "num_hidden = 30\n",
    "num_out = 10\n",
    "\n",
    "# モデル定義（２層のMLP）\n",
    "MLP_model = MLP_1(num_in=num_in, num_hidden=num_hidden, num_out=num_out)\n",
    "\n",
    "# requires_grad_をTrueに設定。最後に_がつくのは設定するという意味。\n",
    "MLP_model.requires_grad_(True)\n",
    "\n",
    "# torch.optimを使ってOptimizerを定義\n",
    "optimizer = optim.SGD(MLP_model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※torchvision.datasets.MNIST（28x28）の方のMNISTデータセットはサイズが大きく、学習に時間がかかるので、  \n",
    "　ここでは簡単のためにdatasets.load_digitsの方のMNISTデータセット（8x8）を用いている。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPの学習ループを関数化\n",
    "def mlp_learning_loop(MLP_model, train_loader, val_loader, optim, loss_func, epoch=10, early_stopping=None):\n",
    "    '''\n",
    "    MLP_model: 任意のMLPモデル\n",
    "    train_loader: 学習データのDataLoader\n",
    "    val_loader: 検証データのDataLoader\n",
    "    optim: optimizer\n",
    "    loss_func: 損失関数\n",
    "    epoch: epoch数。デフォルトは10。\n",
    "    early_stopping: early_stopping判定の際のepoch数。デフォルトはearly_stoppingしない。\n",
    "    '''\n",
    "\n",
    "    # 学習・検証結果格納用辞書\n",
    "    train_results = {}\n",
    "\n",
    "    # early_stoppingの判定用のカウンターとそのepoch時点での最小の損失を記録する用変数。初期値は無限大を指定。\n",
    "    early_stopping_counter = 0 \n",
    "    min_Loss_val = float('inf') # infinity\n",
    "\n",
    "    for i, _ in enumerate(range(epoch)):\n",
    "\n",
    "        # 各バッチでの学習データと検証データに対するloss、accuracyを累積する用の変数\n",
    "        cum_loss = 0\n",
    "        cum_loss_val = 0\n",
    "        cum_accuracy_val = 0\n",
    "\n",
    "        for num_batches, train_data in enumerate(train_loader):\n",
    "            # 学習データ定義\n",
    "            X_train_batch, y_train_batch = train_data\n",
    "\n",
    "            # 順伝播の計算\n",
    "            y_pred = MLP_model(X_train_batch)\n",
    "            loss = loss_func(y_pred , y_train_batch)\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            # 逆伝播の計算、パラメタ更新\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "\n",
    "        # 検証データに対する損失を計算。こちらもバッチ単位で評価するように変更。（検証データ数が多いときはこの方が効率的）\n",
    "        with torch.no_grad():\n",
    "            for num_batches_val, val_data in enumerate(val_loader):\n",
    "                X_val, y_val = val_data\n",
    "                y_pred_val = MLP_model(X_val)\n",
    "                loss_val = loss_func(y_pred_val, y_val)\n",
    "                accuracy_val = ( (torch.argmax(y_pred_val, dim=1) == y_val).sum() / len(y_val) )\n",
    "                # バッチごとの損失、accuracyを累積\n",
    "                cum_loss_val += loss_val.item()\n",
    "                cum_accuracy_val += accuracy_val.item()\n",
    "\n",
    "\n",
    "        # 損失、accuracyを記録。\n",
    "        # DataLoaderからのイテレーションの数＋１が実際のミニバッチ数になるので、累積した損失等をこれで割ってepochごとに計算\n",
    "        train_results[f\"epoch_{i}\"] = {\n",
    "            \"Loss_train\": cum_loss / (num_batches + 1),\n",
    "            \"Loss_val\": cum_loss_val / (num_batches_val + 1),\n",
    "            \"Accuracy\": cum_accuracy_val / (num_batches_val + 1),\n",
    "        }\n",
    "\n",
    "        print(f'epoch_{i}: {train_results[f\"epoch_{i}\"]}')\n",
    "\n",
    "\n",
    "        if early_stopping:\n",
    "            # i番目のepochの損失がその時点の最小の損失よりも大きい場合、early_stoppingのカウンターを1増やす。\n",
    "            # そうでない場合、最小の損失値を更新し、カウンターを0に戻す。(暫定チャンピオン方式)\n",
    "            if train_results[f\"epoch_{i}\"][\"Loss_val\"] >= min_Loss_val:\n",
    "                early_stopping_counter += 1\n",
    "            else:\n",
    "                min_Loss_val = train_results[f\"epoch_{i}\"][\"Loss_val\"]\n",
    "                epoch_min_Loss_val = i\n",
    "                early_stopping_counter = 0\n",
    "            \n",
    "            if early_stopping_counter >= early_stopping:\n",
    "                print(\"early stoppingにより学習を終了します。\")\n",
    "                print(f\"検証データに対する損失が最小となるepoch: {epoch_min_Loss_val}\")\n",
    "                break\n",
    "\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_0: {'Loss_train': 1.8538521776596706, 'Loss_val': 1.267676305439737, 'Accuracy': 0.8027777754598193}\n",
      "epoch_1: {'Loss_train': 0.8713603516419729, 'Loss_val': 0.5765743346677886, 'Accuracy': 0.8916666573948331}\n",
      "epoch_2: {'Loss_train': 0.4603702551167872, 'Loss_val': 0.33999547279543346, 'Accuracy': 0.916666661699613}\n",
      "epoch_3: {'Loss_train': 0.31454841140657663, 'Loss_val': 0.2623278707679775, 'Accuracy': 0.9249999937083986}\n",
      "epoch_4: {'Loss_train': 0.24678232637234032, 'Loss_val': 0.22033159103658465, 'Accuracy': 0.9416666593816545}\n",
      "epoch_5: {'Loss_train': 0.20694286479718155, 'Loss_val': 0.19046625143123996, 'Accuracy': 0.9472222146060731}\n",
      "epoch_6: {'Loss_train': 0.17851547618758762, 'Loss_val': 0.16301943765332302, 'Accuracy': 0.955555549926228}\n",
      "epoch_7: {'Loss_train': 0.15976522588688466, 'Loss_val': 0.15799811285816961, 'Accuracy': 0.9472222146060731}\n",
      "epoch_8: {'Loss_train': 0.14343565094491673, 'Loss_val': 0.14299924562995633, 'Accuracy': 0.955555549926228}\n",
      "epoch_9: {'Loss_train': 0.13285955233085486, 'Loss_val': 0.1359461573107789, 'Accuracy': 0.9555555482705435}\n",
      "epoch_10: {'Loss_train': 0.1212347681608258, 'Loss_val': 0.1254063564249211, 'Accuracy': 0.9666666620307498}\n",
      "epoch_11: {'Loss_train': 0.11417968365195622, 'Loss_val': 0.12376141421393388, 'Accuracy': 0.9638888852463828}\n",
      "epoch_12: {'Loss_train': 0.10660350119643328, 'Loss_val': 0.1146922063910299, 'Accuracy': 0.9666666620307498}\n",
      "epoch_13: {'Loss_train': 0.10211650151824062, 'Loss_val': 0.11045244672439164, 'Accuracy': 0.9666666620307498}\n",
      "epoch_14: {'Loss_train': 0.09674166577557723, 'Loss_val': 0.10791202828598519, 'Accuracy': 0.9638888835906982}\n",
      "epoch_15: {'Loss_train': 0.09344363512354903, 'Loss_val': 0.10465313243265781, 'Accuracy': 0.9694444404708015}\n",
      "epoch_16: {'Loss_train': 0.08833482846320193, 'Loss_val': 0.10502585432388717, 'Accuracy': 0.9638888835906982}\n",
      "epoch_17: {'Loss_train': 0.0828810828356331, 'Loss_val': 0.10136146049222185, 'Accuracy': 0.9694444404708015}\n",
      "epoch_18: {'Loss_train': 0.07921446770261456, 'Loss_val': 0.09833526630730678, 'Accuracy': 0.9694444404708015}\n",
      "epoch_19: {'Loss_train': 0.07402201686636545, 'Loss_val': 0.10467660456844088, 'Accuracy': 0.9749999973509047}\n",
      "epoch_20: {'Loss_train': 0.07271653846287841, 'Loss_val': 0.09749268516639455, 'Accuracy': 0.9749999956952201}\n",
      "epoch_21: {'Loss_train': 0.06837081248886534, 'Loss_val': 0.09656014210648006, 'Accuracy': 0.9694444388151169}\n",
      "epoch_22: {'Loss_train': 0.06656780701369927, 'Loss_val': 0.10078055129593445, 'Accuracy': 0.9638888835906982}\n",
      "epoch_23: {'Loss_train': 0.06360786174061811, 'Loss_val': 0.09943514964025882, 'Accuracy': 0.9694444388151169}\n",
      "epoch_24: {'Loss_train': 0.06093320519964133, 'Loss_val': 0.09566575176884523, 'Accuracy': 0.9722222172551684}\n",
      "epoch_25: {'Loss_train': 0.060178859162584156, 'Loss_val': 0.09162481728708372, 'Accuracy': 0.9777777741352717}\n",
      "epoch_26: {'Loss_train': 0.057687496612844474, 'Loss_val': 0.09053520239346351, 'Accuracy': 0.9749999956952201}\n",
      "epoch_27: {'Loss_train': 0.055381997296030425, 'Loss_val': 0.09064717588868614, 'Accuracy': 0.9694444404708015}\n",
      "epoch_28: {'Loss_train': 0.05342578306347908, 'Loss_val': 0.08939117576745856, 'Accuracy': 0.9749999956952201}\n",
      "epoch_29: {'Loss_train': 0.05277489406782681, 'Loss_val': 0.0879970304409249, 'Accuracy': 0.9749999956952201}\n",
      "epoch_30: {'Loss_train': 0.050216096621524126, 'Loss_val': 0.08596744573959667, 'Accuracy': 0.9749999956952201}\n",
      "epoch_31: {'Loss_train': 0.048671072702644674, 'Loss_val': 0.0869497583933278, 'Accuracy': 0.9722222172551684}\n",
      "epoch_32: {'Loss_train': 0.04647090420864212, 'Loss_val': 0.08777693393898921, 'Accuracy': 0.9777777741352717}\n",
      "epoch_33: {'Loss_train': 0.04476809634232066, 'Loss_val': 0.08621601748099136, 'Accuracy': 0.9777777741352717}\n",
      "epoch_34: {'Loss_train': 0.04396528288392195, 'Loss_val': 0.08782895413848262, 'Accuracy': 0.9749999956952201}\n",
      "epoch_35: {'Loss_train': 0.04247338374706386, 'Loss_val': 0.08629210359261681, 'Accuracy': 0.9749999956952201}\n",
      "epoch_36: {'Loss_train': 0.040987295612770445, 'Loss_val': 0.084093677591429, 'Accuracy': 0.9777777741352717}\n",
      "epoch_37: {'Loss_train': 0.038869621954897106, 'Loss_val': 0.08711763832252473, 'Accuracy': 0.9749999956952201}\n",
      "epoch_38: {'Loss_train': 0.0390096288259277, 'Loss_val': 0.08552404910895145, 'Accuracy': 0.9749999956952201}\n",
      "epoch_39: {'Loss_train': 0.03834636364141867, 'Loss_val': 0.0848753513128031, 'Accuracy': 0.9749999956952201}\n",
      "epoch_40: {'Loss_train': 0.03567155967983934, 'Loss_val': 0.08539370824453524, 'Accuracy': 0.9749999956952201}\n",
      "epoch_41: {'Loss_train': 0.03477168782733821, 'Loss_val': 0.08626851914515202, 'Accuracy': 0.9749999956952201}\n",
      "epoch_42: {'Loss_train': 0.03379948841433765, 'Loss_val': 0.08738540005611463, 'Accuracy': 0.9749999956952201}\n",
      "epoch_43: {'Loss_train': 0.03313528724902426, 'Loss_val': 0.0906721035264329, 'Accuracy': 0.9722222172551684}\n",
      "epoch_44: {'Loss_train': 0.03308992574279222, 'Loss_val': 0.08819376102959116, 'Accuracy': 0.9722222172551684}\n",
      "epoch_45: {'Loss_train': 0.03158700895599193, 'Loss_val': 0.08921725425170735, 'Accuracy': 0.9722222172551684}\n",
      "epoch_46: {'Loss_train': 0.029892397333191587, 'Loss_val': 0.08579693677731687, 'Accuracy': 0.9749999956952201}\n",
      "early stoppingにより学習を終了します。\n",
      "検証データに対する損失が最小となるepoch: 36\n"
     ]
    }
   ],
   "source": [
    "# 学習ループ実行\n",
    "result = mlp_learning_loop(MLP_model, train_loader, val_loader, optimizer, F.cross_entropy, epoch=100, early_stopping=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想定通りにearly stoppingが行われていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[●学習曲線の描画](#toc0_)\n",
    "学習データと検証データでの損失の推移を確認してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch_train = []\n",
    "loss_per_epoch_val = []\n",
    "\n",
    "for result in result.values():\n",
    "    loss_per_epoch_train.append(result['Loss_train'])\n",
    "    loss_per_epoch_val.append(result['Loss_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6bdee75b90>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_per_epoch_train, label=\"Loss_train\")\n",
    "plt.plot(loss_per_epoch_val, label=\"Loss_val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train,valどちらも損失が下がっていっており、学習が進んでいることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ モデルオブジェクトの保存とロード\n",
    "* torch.save(): モデルを指定したパスに保存する\n",
    "    * obj引数: 保存するモデルオブジェクト\n",
    "    * f引数: モデルの保存先を指定\n",
    "* torch.load(): 指定したパスからモデルをロードする\n",
    "    * f引数: ロードするモデルのファイル名やパス\n",
    "* 拡張子に‘.pth’を使う慣習がある\n",
    "* 異なる環境でロードした際に予期しない問題が起こる可能性があることに注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_1(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_1): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (linear_2): Linear(in_features=30, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記で学習済みのMLPモデルを\"sample_model.pth\"として保存\n",
    "torch.save(MLP_model, 'sample_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルを別のモデルオブジェクトとして読み込む\n",
    "loaded_model = torch.load('sample_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_1(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_1): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (linear_2): Linear(in_features=30, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様の層数、ノード数のモデルとして読み込まれていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ モデルのパラメータの保存とロード\n",
    "* モデルオブジェクトそのものではなく、学習の結果定まった $\\textbf{W}, \\textbf{B}$を保存し、ロードして利用する。\n",
    "* パラメータのみを切り出して保存することで，パラメータに関連するコード以外が変更されてもロードして使用できるため、  \n",
    "オブジェクトごとの保存/ロードするより一般に推奨される\n",
    "* モデルオブジェクトの.state_dict()でモデルのパラメータの情報のみを取り出す\n",
    "* Opbmizerについて同様にパラメータを保存することができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_1.weight',\n",
       "              tensor([[ 0.0099,  0.0349,  0.0222,  ...,  0.1807,  0.2009, -0.0766],\n",
       "                      [-0.1395, -0.0772, -0.1397,  ..., -0.0218, -0.0959, -0.1533],\n",
       "                      [-0.1792, -0.1058, -0.1075,  ..., -0.4447, -0.2377, -0.1605],\n",
       "                      ...,\n",
       "                      [ 0.1176, -0.0163,  0.0474,  ..., -0.1015, -0.0992,  0.0771],\n",
       "                      [-0.1451,  0.0351, -0.1060,  ...,  0.1092,  0.1848,  0.0138],\n",
       "                      [ 0.0689,  0.0063,  0.2765,  ..., -0.2749, -0.2611, -0.1532]])),\n",
       "             ('linear_1.bias',\n",
       "              tensor([ 0.1612,  0.0698, -0.0483, -0.0667,  0.0847,  0.0207,  0.0979,  0.1577,\n",
       "                       0.0645,  0.0949,  0.1516, -0.0631,  0.0356,  0.1133,  0.1539,  0.1727,\n",
       "                       0.1652, -0.0275, -0.0008,  0.0173,  0.0761,  0.0372,  0.0104,  0.0456,\n",
       "                      -0.0498, -0.0434,  0.1395, -0.0629, -0.0127,  0.1367])),\n",
       "             ('linear_2.weight',\n",
       "              tensor([[ 1.7064e-01,  9.5884e-02, -4.4995e-03, -2.1751e-01, -5.6033e-01,\n",
       "                        3.5311e-01,  2.7036e-01, -4.1272e-02, -7.0418e-01,  1.6706e-01,\n",
       "                       -3.9370e-01,  1.3650e-01, -5.2388e-01, -5.8764e-01, -1.5631e-01,\n",
       "                       -2.4404e-01,  7.6473e-01,  1.3326e-01, -5.6848e-04,  8.6739e-02,\n",
       "                        1.9467e-01,  2.9524e-01,  1.2319e-01, -3.3232e-02,  5.0549e-02,\n",
       "                        5.6511e-02, -5.6208e-01,  2.4170e-02,  4.9027e-01,  1.2463e+00],\n",
       "                      [-4.3105e-01, -6.4481e-01,  2.1540e-01,  1.7550e-01,  1.3388e-01,\n",
       "                       -9.1420e-01,  9.0304e-01,  4.7345e-02,  9.8243e-01, -5.5190e-02,\n",
       "                       -2.8743e-01, -9.6542e-02,  7.7753e-01,  5.3986e-01, -2.8646e-01,\n",
       "                       -8.7090e-01, -8.0192e-01, -1.2311e-01, -1.9085e-01, -5.2391e-02,\n",
       "                        6.8617e-01, -5.9848e-01, -5.4873e-02,  1.1586e-01,  1.5316e-01,\n",
       "                        8.6534e-02,  3.3426e-02,  3.1722e-02, -2.3182e-01, -2.6908e-01],\n",
       "                      [ 6.1994e-01,  3.4332e-02, -3.8317e-01, -4.7692e-02,  1.1441e+00,\n",
       "                       -6.5066e-02, -6.3029e-01, -4.9891e-01,  4.9338e-01,  1.1573e-01,\n",
       "                       -2.5294e-01,  8.3798e-02,  6.6187e-01, -4.9842e-01,  4.2353e-01,\n",
       "                       -5.8371e-01, -7.2600e-01, -6.0792e-02, -1.2714e-01, -7.9056e-02,\n",
       "                       -3.9419e-01,  2.8068e-01, -1.3890e-01,  8.1754e-02, -3.6923e-01,\n",
       "                        2.5048e-02, -8.4966e-01,  7.3099e-02,  8.0163e-01, -1.3664e-01],\n",
       "                      [-3.2760e-01,  5.1039e-01, -3.8317e-01,  2.0064e-01,  1.0090e+00,\n",
       "                        2.3359e-02, -1.0941e+00, -2.6417e-01, -2.4914e-01, -7.5631e-02,\n",
       "                        9.9193e-02,  1.5443e-01, -6.6839e-01,  7.8227e-01,  5.3594e-01,\n",
       "                        8.0208e-01, -3.0947e-02,  1.1752e-02, -5.4914e-02, -1.0166e-01,\n",
       "                        5.1495e-01,  3.9412e-02,  1.7279e-01,  7.1181e-02, -2.4935e-01,\n",
       "                        7.6778e-02, -5.0558e-01,  8.6326e-02, -1.4916e-01, -6.2383e-01],\n",
       "                      [ 1.9787e-02, -2.6074e-01,  1.2518e-01,  2.7965e-01, -8.2688e-01,\n",
       "                       -6.9807e-01,  3.8390e-01,  8.2798e-01, -3.3669e-01, -1.5011e-01,\n",
       "                        1.8437e-01, -1.0612e-01,  9.0573e-01, -7.7251e-02, -2.4691e-01,\n",
       "                        1.2779e-01,  9.3229e-01, -7.1412e-02,  8.8338e-02, -8.8459e-02,\n",
       "                       -7.7744e-01, -3.5900e-01, -1.7085e-02, -6.9523e-01, -9.8839e-03,\n",
       "                        4.9161e-02,  4.8312e-01, -1.3111e-01,  7.1260e-02,  2.0377e-01],\n",
       "                      [-3.1944e-01, -3.0376e-01,  7.3807e-03,  2.5611e-01, -7.4449e-01,\n",
       "                        2.9784e-01, -7.4264e-01, -5.2609e-01,  3.4906e-01,  1.6514e-01,\n",
       "                        7.6618e-01, -1.4103e-01, -1.9376e-01,  5.2154e-02,  2.8799e-02,\n",
       "                        8.4714e-01, -4.4045e-01, -1.5300e-01,  1.1769e-01, -5.2065e-02,\n",
       "                        5.1739e-01, -1.5078e-01, -7.3257e-02, -7.1924e-02,  8.1850e-01,\n",
       "                        2.2327e-02,  2.6918e-01,  1.4590e-02, -3.1945e-01,  9.0770e-01],\n",
       "                      [-3.2733e-01, -3.1267e-03, -1.6677e-01, -1.6724e-01,  2.8941e-01,\n",
       "                       -1.5321e-01,  2.7257e-01, -4.7733e-01,  4.6132e-01,  1.6189e-01,\n",
       "                        4.7347e-01,  1.0882e-02, -2.2492e-02, -4.0551e-01, -4.2507e-01,\n",
       "                       -3.4339e-01,  1.1331e+00, -3.2882e-02, -2.1499e-01, -3.4155e-02,\n",
       "                       -4.6301e-01, -2.7860e-01, -1.5711e-01,  8.8449e-01,  6.7761e-02,\n",
       "                       -2.0525e-03, -1.2939e-01, -1.2560e-02, -1.2472e-01,  7.0253e-02],\n",
       "                      [-1.4900e-01, -2.6871e-01,  1.1391e+00, -3.3488e-01, -3.0500e-01,\n",
       "                       -3.2695e-01, -3.7398e-01,  8.1738e-01, -4.7086e-01,  5.5507e-02,\n",
       "                       -1.3109e-02, -6.3775e-02, -2.8645e-01, -2.0038e-01, -7.6087e-02,\n",
       "                        4.5412e-01, -3.6479e-01, -1.2198e-01,  7.4469e-02,  4.2870e-02,\n",
       "                       -2.8535e-01,  1.0299e+00,  9.5781e-02,  3.9112e-02, -4.0694e-01,\n",
       "                       -1.6219e-01,  3.7734e-01,  1.8907e-01,  2.3360e-01, -3.8606e-01],\n",
       "                      [-3.5929e-02,  4.8278e-01, -3.9285e-01, -7.2764e-02,  7.7668e-02,\n",
       "                        8.3546e-01,  8.9752e-01,  5.4221e-01,  4.9079e-01, -1.3036e-01,\n",
       "                       -1.6797e-01,  9.8448e-02, -3.1212e-01,  2.9294e-01,  2.8313e-01,\n",
       "                       -7.3053e-02, -5.7963e-02, -1.5530e-01,  9.7556e-02, -2.3302e-01,\n",
       "                       -2.8336e-01, -5.7421e-01, -8.3865e-02,  3.4476e-03, -3.8102e-02,\n",
       "                        1.4132e-01, -4.8914e-01, -1.1811e-01, -3.9560e-01, -6.6372e-01],\n",
       "                      [ 6.0893e-01,  2.5259e-01, -5.6751e-01,  1.4144e-01, -4.6198e-01,\n",
       "                        5.9840e-01,  6.9788e-01, -2.7666e-01, -9.3993e-01,  2.6917e-02,\n",
       "                       -2.8559e-01, -5.5425e-02, -6.7813e-01,  4.9469e-01,  3.0562e-01,\n",
       "                        8.0731e-02, -7.4014e-01,  6.4464e-02, -1.4488e-01,  1.3487e-01,\n",
       "                        7.1189e-01,  3.5514e-01,  1.2067e-01,  1.9806e-01, -6.5665e-02,\n",
       "                        7.3097e-02,  7.3288e-01,  1.7709e-01, -2.3867e-01, -4.3734e-02]])),\n",
       "             ('linear_2.bias',\n",
       "              tensor([ 0.2166,  0.0235,  0.2511, -0.0406, -0.0308,  0.0405, -0.0492,  0.1224,\n",
       "                       0.1209, -0.2426]))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのパラメタ確認\n",
    "MLP_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_1(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_1): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (linear_2): Linear(in_features=30, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメタ保存\n",
    "torch.save(MLP_model.state_dict(), 'sample_model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメタ読み込み前に、パラメタを読み込ませたいモデルオブジェクトを作成しておく必要あり。\n",
    "another_model = MLP_1(64, 30, 10)\n",
    "\n",
    "# パラメタ読み込み。読み込む際はload_state_dictメソッドを用いる\n",
    "another_model.load_state_dict(torch.load('sample_model_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_1.weight',\n",
       "              tensor([[ 0.0099,  0.0349,  0.0222,  ...,  0.1807,  0.2009, -0.0766],\n",
       "                      [-0.1395, -0.0772, -0.1397,  ..., -0.0218, -0.0959, -0.1533],\n",
       "                      [-0.1792, -0.1058, -0.1075,  ..., -0.4447, -0.2377, -0.1605],\n",
       "                      ...,\n",
       "                      [ 0.1176, -0.0163,  0.0474,  ..., -0.1015, -0.0992,  0.0771],\n",
       "                      [-0.1451,  0.0351, -0.1060,  ...,  0.1092,  0.1848,  0.0138],\n",
       "                      [ 0.0689,  0.0063,  0.2765,  ..., -0.2749, -0.2611, -0.1532]])),\n",
       "             ('linear_1.bias',\n",
       "              tensor([ 0.1612,  0.0698, -0.0483, -0.0667,  0.0847,  0.0207,  0.0979,  0.1577,\n",
       "                       0.0645,  0.0949,  0.1516, -0.0631,  0.0356,  0.1133,  0.1539,  0.1727,\n",
       "                       0.1652, -0.0275, -0.0008,  0.0173,  0.0761,  0.0372,  0.0104,  0.0456,\n",
       "                      -0.0498, -0.0434,  0.1395, -0.0629, -0.0127,  0.1367])),\n",
       "             ('linear_2.weight',\n",
       "              tensor([[ 1.7064e-01,  9.5884e-02, -4.4995e-03, -2.1751e-01, -5.6033e-01,\n",
       "                        3.5311e-01,  2.7036e-01, -4.1272e-02, -7.0418e-01,  1.6706e-01,\n",
       "                       -3.9370e-01,  1.3650e-01, -5.2388e-01, -5.8764e-01, -1.5631e-01,\n",
       "                       -2.4404e-01,  7.6473e-01,  1.3326e-01, -5.6848e-04,  8.6739e-02,\n",
       "                        1.9467e-01,  2.9524e-01,  1.2319e-01, -3.3232e-02,  5.0549e-02,\n",
       "                        5.6511e-02, -5.6208e-01,  2.4170e-02,  4.9027e-01,  1.2463e+00],\n",
       "                      [-4.3105e-01, -6.4481e-01,  2.1540e-01,  1.7550e-01,  1.3388e-01,\n",
       "                       -9.1420e-01,  9.0304e-01,  4.7345e-02,  9.8243e-01, -5.5190e-02,\n",
       "                       -2.8743e-01, -9.6542e-02,  7.7753e-01,  5.3986e-01, -2.8646e-01,\n",
       "                       -8.7090e-01, -8.0192e-01, -1.2311e-01, -1.9085e-01, -5.2391e-02,\n",
       "                        6.8617e-01, -5.9848e-01, -5.4873e-02,  1.1586e-01,  1.5316e-01,\n",
       "                        8.6534e-02,  3.3426e-02,  3.1722e-02, -2.3182e-01, -2.6908e-01],\n",
       "                      [ 6.1994e-01,  3.4332e-02, -3.8317e-01, -4.7692e-02,  1.1441e+00,\n",
       "                       -6.5066e-02, -6.3029e-01, -4.9891e-01,  4.9338e-01,  1.1573e-01,\n",
       "                       -2.5294e-01,  8.3798e-02,  6.6187e-01, -4.9842e-01,  4.2353e-01,\n",
       "                       -5.8371e-01, -7.2600e-01, -6.0792e-02, -1.2714e-01, -7.9056e-02,\n",
       "                       -3.9419e-01,  2.8068e-01, -1.3890e-01,  8.1754e-02, -3.6923e-01,\n",
       "                        2.5048e-02, -8.4966e-01,  7.3099e-02,  8.0163e-01, -1.3664e-01],\n",
       "                      [-3.2760e-01,  5.1039e-01, -3.8317e-01,  2.0064e-01,  1.0090e+00,\n",
       "                        2.3359e-02, -1.0941e+00, -2.6417e-01, -2.4914e-01, -7.5631e-02,\n",
       "                        9.9193e-02,  1.5443e-01, -6.6839e-01,  7.8227e-01,  5.3594e-01,\n",
       "                        8.0208e-01, -3.0947e-02,  1.1752e-02, -5.4914e-02, -1.0166e-01,\n",
       "                        5.1495e-01,  3.9412e-02,  1.7279e-01,  7.1181e-02, -2.4935e-01,\n",
       "                        7.6778e-02, -5.0558e-01,  8.6326e-02, -1.4916e-01, -6.2383e-01],\n",
       "                      [ 1.9787e-02, -2.6074e-01,  1.2518e-01,  2.7965e-01, -8.2688e-01,\n",
       "                       -6.9807e-01,  3.8390e-01,  8.2798e-01, -3.3669e-01, -1.5011e-01,\n",
       "                        1.8437e-01, -1.0612e-01,  9.0573e-01, -7.7251e-02, -2.4691e-01,\n",
       "                        1.2779e-01,  9.3229e-01, -7.1412e-02,  8.8338e-02, -8.8459e-02,\n",
       "                       -7.7744e-01, -3.5900e-01, -1.7085e-02, -6.9523e-01, -9.8839e-03,\n",
       "                        4.9161e-02,  4.8312e-01, -1.3111e-01,  7.1260e-02,  2.0377e-01],\n",
       "                      [-3.1944e-01, -3.0376e-01,  7.3807e-03,  2.5611e-01, -7.4449e-01,\n",
       "                        2.9784e-01, -7.4264e-01, -5.2609e-01,  3.4906e-01,  1.6514e-01,\n",
       "                        7.6618e-01, -1.4103e-01, -1.9376e-01,  5.2154e-02,  2.8799e-02,\n",
       "                        8.4714e-01, -4.4045e-01, -1.5300e-01,  1.1769e-01, -5.2065e-02,\n",
       "                        5.1739e-01, -1.5078e-01, -7.3257e-02, -7.1924e-02,  8.1850e-01,\n",
       "                        2.2327e-02,  2.6918e-01,  1.4590e-02, -3.1945e-01,  9.0770e-01],\n",
       "                      [-3.2733e-01, -3.1267e-03, -1.6677e-01, -1.6724e-01,  2.8941e-01,\n",
       "                       -1.5321e-01,  2.7257e-01, -4.7733e-01,  4.6132e-01,  1.6189e-01,\n",
       "                        4.7347e-01,  1.0882e-02, -2.2492e-02, -4.0551e-01, -4.2507e-01,\n",
       "                       -3.4339e-01,  1.1331e+00, -3.2882e-02, -2.1499e-01, -3.4155e-02,\n",
       "                       -4.6301e-01, -2.7860e-01, -1.5711e-01,  8.8449e-01,  6.7761e-02,\n",
       "                       -2.0525e-03, -1.2939e-01, -1.2560e-02, -1.2472e-01,  7.0253e-02],\n",
       "                      [-1.4900e-01, -2.6871e-01,  1.1391e+00, -3.3488e-01, -3.0500e-01,\n",
       "                       -3.2695e-01, -3.7398e-01,  8.1738e-01, -4.7086e-01,  5.5507e-02,\n",
       "                       -1.3109e-02, -6.3775e-02, -2.8645e-01, -2.0038e-01, -7.6087e-02,\n",
       "                        4.5412e-01, -3.6479e-01, -1.2198e-01,  7.4469e-02,  4.2870e-02,\n",
       "                       -2.8535e-01,  1.0299e+00,  9.5781e-02,  3.9112e-02, -4.0694e-01,\n",
       "                       -1.6219e-01,  3.7734e-01,  1.8907e-01,  2.3360e-01, -3.8606e-01],\n",
       "                      [-3.5929e-02,  4.8278e-01, -3.9285e-01, -7.2764e-02,  7.7668e-02,\n",
       "                        8.3546e-01,  8.9752e-01,  5.4221e-01,  4.9079e-01, -1.3036e-01,\n",
       "                       -1.6797e-01,  9.8448e-02, -3.1212e-01,  2.9294e-01,  2.8313e-01,\n",
       "                       -7.3053e-02, -5.7963e-02, -1.5530e-01,  9.7556e-02, -2.3302e-01,\n",
       "                       -2.8336e-01, -5.7421e-01, -8.3865e-02,  3.4476e-03, -3.8102e-02,\n",
       "                        1.4132e-01, -4.8914e-01, -1.1811e-01, -3.9560e-01, -6.6372e-01],\n",
       "                      [ 6.0893e-01,  2.5259e-01, -5.6751e-01,  1.4144e-01, -4.6198e-01,\n",
       "                        5.9840e-01,  6.9788e-01, -2.7666e-01, -9.3993e-01,  2.6917e-02,\n",
       "                       -2.8559e-01, -5.5425e-02, -6.7813e-01,  4.9469e-01,  3.0562e-01,\n",
       "                        8.0731e-02, -7.4014e-01,  6.4464e-02, -1.4488e-01,  1.3487e-01,\n",
       "                        7.1189e-01,  3.5514e-01,  1.2067e-01,  1.9806e-01, -6.5665e-02,\n",
       "                        7.3097e-02,  7.3288e-01,  1.7709e-01, -2.3867e-01, -4.3734e-02]])),\n",
       "             ('linear_2.bias',\n",
       "              tensor([ 0.2166,  0.0235,  0.2511, -0.0406, -0.0308,  0.0405, -0.0492,  0.1224,\n",
       "                       0.1209, -0.2426]))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じパラメタが読み込まれていることが分かる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 学習ループ中に最良のモデルを保存する\n",
    "early stoppingから得られた最もvalidation lossが低いモデルを保存するようにする。  \n",
    "→モデルのパラメータ、Optimizerのパラメータ、当該epochにおけるvalidation lossをdictionary形式で保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズを定義。今回は１ミニバッチ＝３２データとする\n",
    "batch_size = 32\n",
    "\n",
    "# データ読み込み。8x8のMNISTデータセットをカスタムデータセットとして読み込む。（練習のため）\n",
    "dataset = datasets.load_digits()\n",
    "X = dataset['images'] # 0~16の値をもつ\n",
    "X = ( X * (255. / 16.) ).astype(np.uint8) # 下記補足参照\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # 0~255 -> 0~1\n",
    "    transforms.Normalize((0.5,), (0.5)) # 0~1 -> -1~1\n",
    "])\n",
    "\n",
    "# Datasetとして読み込み\n",
    "train_dataset = MyDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = MyDataset(X_val, y_val, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, num_workers=2)\n",
    "\n",
    "# モデルのコンストラクタに渡す引数を定義。\n",
    "num_in = 64\n",
    "num_hidden = 30\n",
    "num_out = 10\n",
    "\n",
    "# モデル定義（２層のMLP）\n",
    "MLP_model = MLP_1(num_in=num_in, num_hidden=num_hidden, num_out=num_out)\n",
    "\n",
    "# requires_grad_をTrueに設定。最後に_がつくのは設定するという意味。\n",
    "MLP_model.requires_grad_(True)\n",
    "\n",
    "# torch.optimを使ってOptimizerを定義\n",
    "optimizer = optim.SGD(MLP_model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPの学習ループを関数化\n",
    "def mlp_learning_loop(MLP_model, train_loader, val_loader, optim, loss_func, epoch=10, early_stopping=None, save_path=None):\n",
    "    '''\n",
    "    MLP_model: 任意のMLPモデル\n",
    "    train_loader: 学習データのDataLoader\n",
    "    val_loader: 検証データのDataLoader\n",
    "    optim: optimizer\n",
    "    loss_func: 損失関数\n",
    "    epoch: epoch数。デフォルトは10。\n",
    "    early_stopping: early_stopping判定の際のepoch数。デフォルトはearly_stoppingしない。\n",
    "    '''\n",
    "\n",
    "    # 学習・検証結果格納用辞書\n",
    "    train_results = {}\n",
    "\n",
    "    # early_stoppingの判定用のカウンターとそのepoch時点での最小の損失を記録する用変数。初期値は無限大を指定。\n",
    "    early_stopping_counter = 0 \n",
    "    min_Loss_val = float('inf') # infinity\n",
    "\n",
    "    for i, _ in enumerate(range(epoch)):\n",
    "\n",
    "        # 各バッチでの学習データと検証データに対するloss、accuracyを累積する用の変数\n",
    "        cum_loss = 0\n",
    "        cum_loss_val = 0\n",
    "        cum_accuracy_val = 0\n",
    "\n",
    "        for num_batches, train_data in enumerate(train_loader):\n",
    "            # 学習データ定義\n",
    "            X_train_batch, y_train_batch = train_data\n",
    "\n",
    "            # 順伝播の計算\n",
    "            y_pred = MLP_model(X_train_batch)\n",
    "            loss = loss_func(y_pred , y_train_batch)\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            # 逆伝播の計算、パラメタ更新\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "\n",
    "        # 検証データに対する損失を計算。こちらもバッチ単位で評価するように変更。（検証データ数が多いときはこの方が効率的）\n",
    "        with torch.no_grad():\n",
    "            for num_batches_val, val_data in enumerate(val_loader):\n",
    "                X_val, y_val = val_data\n",
    "                y_pred_val = MLP_model(X_val)\n",
    "                loss_val = loss_func(y_pred_val, y_val)\n",
    "                accuracy_val = ( (torch.argmax(y_pred_val, dim=1) == y_val).sum() / len(y_val) )\n",
    "                # バッチごとの損失、accuracyを累積\n",
    "                cum_loss_val += loss_val.item()\n",
    "                cum_accuracy_val += accuracy_val.item()\n",
    "\n",
    "\n",
    "        # 損失、accuracyを記録。\n",
    "        # DataLoaderからのイテレーションの数＋１が実際のミニバッチ数になるので、累積した損失等をこれで割ってepochごとに計算\n",
    "        train_results[f\"epoch_{i}\"] = {\n",
    "            \"Loss_train\": cum_loss / (num_batches + 1),\n",
    "            \"Loss_val\": cum_loss_val / (num_batches_val + 1),\n",
    "            \"Accuracy\": cum_accuracy_val / (num_batches_val + 1),\n",
    "        }\n",
    "\n",
    "        print(f'epoch_{i}: {train_results[f\"epoch_{i}\"]}')\n",
    "\n",
    "\n",
    "        if early_stopping:\n",
    "            # i番目のepochの損失がその時点の最小の損失よりも大きい場合、early_stoppingのカウンターを1増やす。\n",
    "            # そうでない場合、最小の損失値を更新し、カウンターを0に戻す。(暫定チャンピオン方式)\n",
    "            if train_results[f\"epoch_{i}\"][\"Loss_val\"] >= min_Loss_val:\n",
    "                early_stopping_counter += 1\n",
    "            else:\n",
    "                min_Loss_val = train_results[f\"epoch_{i}\"][\"Loss_val\"]\n",
    "                epoch_min_Loss_val = i\n",
    "                early_stopping_counter = 0\n",
    "\n",
    "                # save_pathがNoneではなければ保存する\n",
    "                # 改善されていれば以下の辞書を更新する\n",
    "                if save_path:\n",
    "                    best_params_and_Loss = {\n",
    "                        \"epoch\": epoch_min_Loss_val,\n",
    "                        \"Model_param\": MLP_model.state_dict(),\n",
    "                        \"Optimizer_param\": optim.state_dict(),\n",
    "                        \"Loss_val\": min_Loss_val\n",
    "                    }\n",
    "            \n",
    "            if early_stopping_counter >= early_stopping:\n",
    "                print(\"early stoppingにより学習を終了します。\")\n",
    "                print(f\"検証データに対する損失が最小となるepoch: {epoch_min_Loss_val}\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # save_pathに最良結果の各種パラメタを保存する。※補足１\n",
    "    torch.save(best_params_and_Loss, save_path)\n",
    "\n",
    "    return train_results, best_params_and_Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※補足１：torch.saveはpythonのオブジェクトを保存するメソッドなので、モデルに限らず色々保存できる。  \n",
    "　ファイルに出力するところはループのところで実行すると処理時間が劣化するので、最後に一回出力すれば十分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_0: {'Loss_train': 1.9596559421883688, 'Loss_val': 1.4145205716292064, 'Accuracy': 0.7444444418781333}\n",
      "epoch_1: {'Loss_train': 0.943395180006822, 'Loss_val': 0.6018292696939574, 'Accuracy': 0.8999999910593033}\n",
      "epoch_2: {'Loss_train': 0.4765450304581059, 'Loss_val': 0.3674698898361789, 'Accuracy': 0.9222222136126624}\n",
      "epoch_3: {'Loss_train': 0.3244988319153587, 'Loss_val': 0.2737314845952723, 'Accuracy': 0.9333333240614997}\n",
      "epoch_4: {'Loss_train': 0.2525691797491163, 'Loss_val': 0.22340893869598707, 'Accuracy': 0.9444444361660216}\n",
      "epoch_5: {'Loss_train': 0.20892196622056267, 'Loss_val': 0.1917779922692312, 'Accuracy': 0.9499999913904402}\n",
      "epoch_6: {'Loss_train': 0.1808038928817647, 'Loss_val': 0.1736045569802324, 'Accuracy': 0.958333326710595}\n",
      "epoch_7: {'Loss_train': 0.1624720163203569, 'Loss_val': 0.16769810802199775, 'Accuracy': 0.9499999913904402}\n",
      "epoch_8: {'Loss_train': 0.14477047172840685, 'Loss_val': 0.15440016001876858, 'Accuracy': 0.958333326710595}\n",
      "epoch_9: {'Loss_train': 0.13422051039783078, 'Loss_val': 0.14627070719790128, 'Accuracy': 0.9527777698304918}\n",
      "epoch_10: {'Loss_train': 0.12242088044230412, 'Loss_val': 0.14325507331846488, 'Accuracy': 0.9527777698304918}\n",
      "epoch_11: {'Loss_train': 0.11423569216276519, 'Loss_val': 0.12795777593014968, 'Accuracy': 0.958333326710595}\n",
      "epoch_12: {'Loss_train': 0.10562661795281908, 'Loss_val': 0.11986218723985884, 'Accuracy': 0.9638888835906982}\n",
      "epoch_13: {'Loss_train': 0.10018934564626154, 'Loss_val': 0.1173300629016012, 'Accuracy': 0.9555555482705435}\n",
      "epoch_14: {'Loss_train': 0.09275748481094423, 'Loss_val': 0.11875935160141024, 'Accuracy': 0.9555555482705435}\n",
      "epoch_15: {'Loss_train': 0.08881987049244344, 'Loss_val': 0.11142585563680364, 'Accuracy': 0.9666666620307498}\n",
      "epoch_16: {'Loss_train': 0.08229108966447206, 'Loss_val': 0.11120723782935077, 'Accuracy': 0.9694444388151169}\n",
      "epoch_17: {'Loss_train': 0.07874292987334128, 'Loss_val': 0.10547757893800735, 'Accuracy': 0.9694444388151169}\n",
      "epoch_18: {'Loss_train': 0.07438103173788274, 'Loss_val': 0.1068471011498736, 'Accuracy': 0.9638888835906982}\n",
      "epoch_19: {'Loss_train': 0.07108411659040333, 'Loss_val': 0.09973761378528757, 'Accuracy': 0.9666666603750653}\n",
      "epoch_20: {'Loss_train': 0.06684325256113273, 'Loss_val': 0.10038393102068868, 'Accuracy': 0.9638888835906982}\n",
      "epoch_21: {'Loss_train': 0.06455484408959616, 'Loss_val': 0.09776746819261461, 'Accuracy': 0.9666666620307498}\n",
      "epoch_22: {'Loss_train': 0.06091103480947721, 'Loss_val': 0.0985849766422891, 'Accuracy': 0.9611111051506467}\n",
      "epoch_23: {'Loss_train': 0.05801685843794581, 'Loss_val': 0.09341928718559858, 'Accuracy': 0.9694444388151169}\n",
      "epoch_24: {'Loss_train': 0.056494608372708574, 'Loss_val': 0.09419906183352901, 'Accuracy': 0.9666666620307498}\n",
      "epoch_25: {'Loss_train': 0.053261709326761775, 'Loss_val': 0.09166678809560835, 'Accuracy': 0.9694444388151169}\n",
      "epoch_26: {'Loss_train': 0.05107609281872606, 'Loss_val': 0.09058128914330155, 'Accuracy': 0.9694444388151169}\n",
      "epoch_27: {'Loss_train': 0.04830365757839496, 'Loss_val': 0.09392881957400176, 'Accuracy': 0.9638888835906982}\n",
      "epoch_28: {'Loss_train': 0.04607155054448716, 'Loss_val': 0.0917579388931497, 'Accuracy': 0.9638888835906982}\n",
      "epoch_29: {'Loss_train': 0.044479077022212245, 'Loss_val': 0.09231917354433487, 'Accuracy': 0.9694444388151169}\n",
      "epoch_30: {'Loss_train': 0.04200986178814977, 'Loss_val': 0.08928478714208016, 'Accuracy': 0.9694444388151169}\n",
      "epoch_31: {'Loss_train': 0.039709927413948916, 'Loss_val': 0.09326580885357948, 'Accuracy': 0.9666666620307498}\n",
      "epoch_32: {'Loss_train': 0.03985234079057894, 'Loss_val': 0.08764682567561977, 'Accuracy': 0.9694444388151169}\n",
      "epoch_33: {'Loss_train': 0.038100153275687866, 'Loss_val': 0.086155170471304, 'Accuracy': 0.9694444388151169}\n",
      "epoch_34: {'Loss_train': 0.03704698674563486, 'Loss_val': 0.08483084026374854, 'Accuracy': 0.9749999956952201}\n",
      "epoch_35: {'Loss_train': 0.03581247813619686, 'Loss_val': 0.08440806780062202, 'Accuracy': 0.9694444388151169}\n",
      "epoch_36: {'Loss_train': 0.034494895495299716, 'Loss_val': 0.08544824959891331, 'Accuracy': 0.9694444388151169}\n",
      "epoch_37: {'Loss_train': 0.03209018025194786, 'Loss_val': 0.09008055880420013, 'Accuracy': 0.9694444388151169}\n",
      "epoch_38: {'Loss_train': 0.03172729557304087, 'Loss_val': 0.08480992559796302, 'Accuracy': 0.9694444388151169}\n",
      "epoch_39: {'Loss_train': 0.030408839894840237, 'Loss_val': 0.08399340709567898, 'Accuracy': 0.9694444388151169}\n",
      "epoch_40: {'Loss_train': 0.029692308269861516, 'Loss_val': 0.08416519725708188, 'Accuracy': 0.9694444388151169}\n",
      "epoch_41: {'Loss_train': 0.0283218305634768, 'Loss_val': 0.08613515164729, 'Accuracy': 0.9694444388151169}\n",
      "epoch_42: {'Loss_train': 0.027219061065504018, 'Loss_val': 0.08441522922819583, 'Accuracy': 0.9694444388151169}\n",
      "epoch_43: {'Loss_train': 0.027450515761302086, 'Loss_val': 0.08591041590423426, 'Accuracy': 0.9694444388151169}\n",
      "epoch_44: {'Loss_train': 0.026583700599480328, 'Loss_val': 0.08935351772945271, 'Accuracy': 0.9694444388151169}\n",
      "epoch_45: {'Loss_train': 0.02565842567330239, 'Loss_val': 0.08593320740166949, 'Accuracy': 0.9722222172551684}\n",
      "epoch_46: {'Loss_train': 0.024367912376166916, 'Loss_val': 0.08203410894687597, 'Accuracy': 0.9694444388151169}\n",
      "epoch_47: {'Loss_train': 0.023902489158192312, 'Loss_val': 0.0830544328976733, 'Accuracy': 0.9694444388151169}\n",
      "epoch_48: {'Loss_train': 0.02311871252580507, 'Loss_val': 0.08405431076082298, 'Accuracy': 0.9694444388151169}\n",
      "epoch_49: {'Loss_train': 0.022264462122823007, 'Loss_val': 0.08223152061853195, 'Accuracy': 0.9722222172551684}\n",
      "epoch_50: {'Loss_train': 0.021830433545498334, 'Loss_val': 0.08241829426778066, 'Accuracy': 0.9694444388151169}\n",
      "epoch_51: {'Loss_train': 0.020626629094395967, 'Loss_val': 0.08574136482073097, 'Accuracy': 0.9749999956952201}\n",
      "epoch_52: {'Loss_train': 0.020642408801399545, 'Loss_val': 0.08402464807376317, 'Accuracy': 0.9722222172551684}\n",
      "epoch_53: {'Loss_train': 0.020150860932416335, 'Loss_val': 0.0832617354592205, 'Accuracy': 0.9694444388151169}\n",
      "epoch_54: {'Loss_train': 0.019554389060431276, 'Loss_val': 0.08169264679357487, 'Accuracy': 0.9694444388151169}\n",
      "epoch_55: {'Loss_train': 0.01875116048136584, 'Loss_val': 0.08166031325688689, 'Accuracy': 0.9694444388151169}\n",
      "epoch_56: {'Loss_train': 0.018277620566550468, 'Loss_val': 0.08033851424665449, 'Accuracy': 0.9694444388151169}\n",
      "epoch_57: {'Loss_train': 0.017726206903615902, 'Loss_val': 0.08378865423664036, 'Accuracy': 0.9694444388151169}\n",
      "epoch_58: {'Loss_train': 0.01759358347347491, 'Loss_val': 0.0808627416295672, 'Accuracy': 0.9694444388151169}\n",
      "epoch_59: {'Loss_train': 0.01701847993738031, 'Loss_val': 0.08231164417520631, 'Accuracy': 0.9694444388151169}\n",
      "epoch_60: {'Loss_train': 0.017144617179939006, 'Loss_val': 0.0814358802834047, 'Accuracy': 0.9694444388151169}\n",
      "epoch_61: {'Loss_train': 0.01615387375285435, 'Loss_val': 0.08377098687277693, 'Accuracy': 0.9722222172551684}\n",
      "epoch_62: {'Loss_train': 0.016435426550338486, 'Loss_val': 0.08291818487567879, 'Accuracy': 0.9749999956952201}\n",
      "epoch_63: {'Loss_train': 0.01574382530634466, 'Loss_val': 0.08121816482808855, 'Accuracy': 0.9694444388151169}\n",
      "epoch_64: {'Loss_train': 0.015132234443323492, 'Loss_val': 0.08341774555285358, 'Accuracy': 0.9694444388151169}\n",
      "epoch_65: {'Loss_train': 0.015258223719683883, 'Loss_val': 0.08226787605154742, 'Accuracy': 0.9694444388151169}\n",
      "epoch_66: {'Loss_train': 0.014695993310346643, 'Loss_val': 0.08193310042325821, 'Accuracy': 0.9722222172551684}\n",
      "early stoppingにより学習を終了します。\n",
      "検証データに対する損失が最小となるepoch: 56\n"
     ]
    }
   ],
   "source": [
    "# 学習ループ実行\n",
    "_, params_result = mlp_learning_loop(MLP_model, train_loader, val_loader, optimizer, F.cross_entropy, epoch=100, early_stopping=10, save_path=\"MLP_result.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 56,\n",
       " 'Model_param': OrderedDict([('linear_1.weight',\n",
       "               tensor([[ 0.0700, -0.1004, -0.0958,  ...,  0.0803, -0.0952, -0.1111],\n",
       "                       [-0.0217, -0.0034,  0.0641,  ...,  0.1084, -0.2026,  0.0524],\n",
       "                       [-0.0530, -0.1565,  0.0017,  ..., -0.1727, -0.1910,  0.0113],\n",
       "                       ...,\n",
       "                       [ 0.0813, -0.1134, -0.1322,  ...,  0.0519,  0.1667, -0.0923],\n",
       "                       [ 0.0108, -0.0275,  0.0612,  ..., -0.1205,  0.0746, -0.1491],\n",
       "                       [-0.0228,  0.1435, -0.0761,  ...,  0.2858,  0.0682,  0.0191]])),\n",
       "              ('linear_1.bias',\n",
       "               tensor([-0.0830, -0.0070,  0.1434,  0.0112,  0.1515, -0.0662,  0.1211,  0.1132,\n",
       "                        0.0921,  0.1135,  0.1095, -0.0668,  0.0727,  0.0342, -0.0567, -0.0048,\n",
       "                        0.1008,  0.0768, -0.0148,  0.1612,  0.1404,  0.1004,  0.0127,  0.0010,\n",
       "                       -0.0450,  0.0162, -0.0276,  0.1602,  0.1638, -0.0307])),\n",
       "              ('linear_2.weight',\n",
       "               tensor([[-0.0318, -0.5894,  0.1456, -0.1555, -0.9568,  0.3701, -0.3410, -0.0286,\n",
       "                        -0.3724, -0.4933, -0.1625,  0.9660, -0.0342,  0.1317,  0.0707, -0.1142,\n",
       "                        -0.5325, -0.1342,  0.1449, -0.3716,  0.1038,  0.9506,  0.0465,  0.3959,\n",
       "                         0.6411,  0.0073, -0.0487, -0.2842, -0.0151, -0.2501],\n",
       "                       [ 0.0327,  0.0683,  0.6866,  0.1763, -0.2003, -0.0807, -0.1930,  0.0830,\n",
       "                        -0.2374,  0.4812,  0.5772, -0.0038,  0.1804,  0.7804, -0.1522,  0.5897,\n",
       "                         1.1089, -0.0220,  0.2558,  0.1514, -0.5734, -1.2495, -0.1267, -0.3113,\n",
       "                        -0.9910,  0.0760,  0.1469, -0.2387, -0.4049,  0.9614],\n",
       "                       [ 0.1088,  0.2004, -0.5474, -0.1400, -0.8510, -0.0250, -0.3883,  0.1756,\n",
       "                        -0.1119,  0.8575, -0.0235, -0.6610, -0.0368, -0.8062, -0.0378,  0.0377,\n",
       "                         0.8130,  0.0696, -0.3591,  0.6249,  0.4594, -0.4209, -0.0480,  1.4927,\n",
       "                         0.0964,  0.0842,  0.2333,  0.5050,  0.1258, -0.1495],\n",
       "                       [ 0.1078,  0.2152, -0.3248, -0.1773,  0.5239,  0.1627,  0.0234,  0.1249,\n",
       "                         1.0582, -0.0495, -0.1883, -0.4207,  0.1079, -0.9018, -0.1044, -0.5892,\n",
       "                        -0.1528, -0.1615, -0.2358, -0.4726,  0.2455, -0.3234, -0.0978,  0.1067,\n",
       "                         0.6508,  0.1580,  1.3225,  0.5765, -0.1405,  0.7186],\n",
       "                       [ 0.2458,  0.1072, -0.2119,  0.1028, -0.1651, -0.1893,  0.7406, -0.2147,\n",
       "                        -0.7133, -0.7397, -0.2037, -0.3446, -0.0257,  1.2206,  0.1270,  0.1283,\n",
       "                        -0.4997, -0.1301,  1.0124,  0.6494, -0.2147,  0.3403, -0.1518, -0.1588,\n",
       "                        -0.8238, -0.0790, -0.0592,  0.2155,  0.4029, -0.2347],\n",
       "                       [-0.0324, -0.4606,  0.2259, -0.2006,  0.4511,  0.4967, -0.6245,  0.5035,\n",
       "                         0.8981, -0.4544,  0.1213, -0.4492,  0.0144, -0.1700,  0.1067,  0.1795,\n",
       "                         0.3428, -0.1461, -0.3551, -0.9693, -0.0082,  1.2875, -0.0296, -0.5171,\n",
       "                        -0.7286, -0.0626, -0.5195, -0.2386, -0.1805, -0.1861],\n",
       "                       [-0.1479,  0.3855, -0.4237, -0.0352, -0.5678, -0.0039,  1.0784,  0.5914,\n",
       "                        -0.7283, -0.3756,  0.1443,  0.4805,  0.1735,  0.1106, -0.1003,  0.1804,\n",
       "                         0.6390, -0.1607, -0.6682, -0.3484,  0.5732, -0.1320, -0.1121, -0.5902,\n",
       "                         0.0501, -0.0772,  0.0488, -0.0804, -0.1794, -0.3713],\n",
       "                       [-0.1580,  0.1311,  0.4319,  0.1866,  0.2455, -0.1794,  0.3661, -0.0128,\n",
       "                        -0.1555,  1.0419, -0.1366, -0.4843, -0.1319, -0.4148, -0.0125,  0.0751,\n",
       "                        -1.3980,  0.0896,  0.9472,  0.2596,  0.3575,  0.3717, -0.0334, -0.2370,\n",
       "                        -0.4570,  0.1396,  0.1282,  0.3130,  0.1249, -0.0699],\n",
       "                       [ 0.0573,  0.3130, -0.0906,  0.1090,  0.7808, -0.3886, -0.0488, -0.9113,\n",
       "                        -0.1929,  0.1216, -0.3552,  0.6675, -0.0234,  0.3252,  0.1565, -0.4406,\n",
       "                         0.3639, -0.1769, -0.8252,  0.3665,  0.5408, -0.2392,  0.1606, -0.6015,\n",
       "                         0.5995, -0.0521, -0.2681, -0.7666,  0.0234, -0.0811],\n",
       "                       [-0.0485, -0.7409,  0.2692,  0.1422,  1.1155, -0.2259, -0.6274,  0.1017,\n",
       "                         0.8706, -0.4422,  0.1802,  0.6299,  0.0563, -0.0923,  0.1837,  0.1706,\n",
       "                        -0.7018,  0.1295, -0.0525,  0.5857, -1.0897, -0.3469, -0.0107,  0.3285,\n",
       "                         0.7542, -0.0777, -0.5974, -0.1037, -0.0575, -0.3800]])),\n",
       "              ('linear_2.bias',\n",
       "               tensor([-0.0255,  0.0740, -0.2414, -0.1335,  0.2480, -0.1888,  0.0037, -0.0825,\n",
       "                       -0.0884, -0.0151]))]),\n",
       " 'Optimizer_param': {'state': {0: {'momentum_buffer': None},\n",
       "   1: {'momentum_buffer': None},\n",
       "   2: {'momentum_buffer': None},\n",
       "   3: {'momentum_buffer': None}},\n",
       "  'param_groups': [{'lr': 0.03,\n",
       "    'momentum': 0,\n",
       "    'dampening': 0,\n",
       "    'weight_decay': 0,\n",
       "    'nesterov': False,\n",
       "    'maximize': False,\n",
       "    'foreach': None,\n",
       "    'differentiable': False,\n",
       "    'params': [0, 1, 2, 3]}]},\n",
       " 'Loss_val': 0.08033851424665449}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 56,\n",
       " 'Model_param': OrderedDict([('linear_1.weight',\n",
       "               tensor([[ 0.0700, -0.1004, -0.0958,  ...,  0.0803, -0.0952, -0.1111],\n",
       "                       [-0.0217, -0.0034,  0.0641,  ...,  0.1084, -0.2026,  0.0524],\n",
       "                       [-0.0530, -0.1565,  0.0017,  ..., -0.1727, -0.1910,  0.0113],\n",
       "                       ...,\n",
       "                       [ 0.0813, -0.1134, -0.1322,  ...,  0.0519,  0.1667, -0.0923],\n",
       "                       [ 0.0108, -0.0275,  0.0612,  ..., -0.1205,  0.0746, -0.1491],\n",
       "                       [-0.0228,  0.1435, -0.0761,  ...,  0.2858,  0.0682,  0.0191]])),\n",
       "              ('linear_1.bias',\n",
       "               tensor([-0.0830, -0.0070,  0.1434,  0.0112,  0.1515, -0.0662,  0.1211,  0.1132,\n",
       "                        0.0921,  0.1135,  0.1095, -0.0668,  0.0727,  0.0342, -0.0567, -0.0048,\n",
       "                        0.1008,  0.0768, -0.0148,  0.1612,  0.1404,  0.1004,  0.0127,  0.0010,\n",
       "                       -0.0450,  0.0162, -0.0276,  0.1602,  0.1638, -0.0307])),\n",
       "              ('linear_2.weight',\n",
       "               tensor([[-0.0318, -0.5894,  0.1456, -0.1555, -0.9568,  0.3701, -0.3410, -0.0286,\n",
       "                        -0.3724, -0.4933, -0.1625,  0.9660, -0.0342,  0.1317,  0.0707, -0.1142,\n",
       "                        -0.5325, -0.1342,  0.1449, -0.3716,  0.1038,  0.9506,  0.0465,  0.3959,\n",
       "                         0.6411,  0.0073, -0.0487, -0.2842, -0.0151, -0.2501],\n",
       "                       [ 0.0327,  0.0683,  0.6866,  0.1763, -0.2003, -0.0807, -0.1930,  0.0830,\n",
       "                        -0.2374,  0.4812,  0.5772, -0.0038,  0.1804,  0.7804, -0.1522,  0.5897,\n",
       "                         1.1089, -0.0220,  0.2558,  0.1514, -0.5734, -1.2495, -0.1267, -0.3113,\n",
       "                        -0.9910,  0.0760,  0.1469, -0.2387, -0.4049,  0.9614],\n",
       "                       [ 0.1088,  0.2004, -0.5474, -0.1400, -0.8510, -0.0250, -0.3883,  0.1756,\n",
       "                        -0.1119,  0.8575, -0.0235, -0.6610, -0.0368, -0.8062, -0.0378,  0.0377,\n",
       "                         0.8130,  0.0696, -0.3591,  0.6249,  0.4594, -0.4209, -0.0480,  1.4927,\n",
       "                         0.0964,  0.0842,  0.2333,  0.5050,  0.1258, -0.1495],\n",
       "                       [ 0.1078,  0.2152, -0.3248, -0.1773,  0.5239,  0.1627,  0.0234,  0.1249,\n",
       "                         1.0582, -0.0495, -0.1883, -0.4207,  0.1079, -0.9018, -0.1044, -0.5892,\n",
       "                        -0.1528, -0.1615, -0.2358, -0.4726,  0.2455, -0.3234, -0.0978,  0.1067,\n",
       "                         0.6508,  0.1580,  1.3225,  0.5765, -0.1405,  0.7186],\n",
       "                       [ 0.2458,  0.1072, -0.2119,  0.1028, -0.1651, -0.1893,  0.7406, -0.2147,\n",
       "                        -0.7133, -0.7397, -0.2037, -0.3446, -0.0257,  1.2206,  0.1270,  0.1283,\n",
       "                        -0.4997, -0.1301,  1.0124,  0.6494, -0.2147,  0.3403, -0.1518, -0.1588,\n",
       "                        -0.8238, -0.0790, -0.0592,  0.2155,  0.4029, -0.2347],\n",
       "                       [-0.0324, -0.4606,  0.2259, -0.2006,  0.4511,  0.4967, -0.6245,  0.5035,\n",
       "                         0.8981, -0.4544,  0.1213, -0.4492,  0.0144, -0.1700,  0.1067,  0.1795,\n",
       "                         0.3428, -0.1461, -0.3551, -0.9693, -0.0082,  1.2875, -0.0296, -0.5171,\n",
       "                        -0.7286, -0.0626, -0.5195, -0.2386, -0.1805, -0.1861],\n",
       "                       [-0.1479,  0.3855, -0.4237, -0.0352, -0.5678, -0.0039,  1.0784,  0.5914,\n",
       "                        -0.7283, -0.3756,  0.1443,  0.4805,  0.1735,  0.1106, -0.1003,  0.1804,\n",
       "                         0.6390, -0.1607, -0.6682, -0.3484,  0.5732, -0.1320, -0.1121, -0.5902,\n",
       "                         0.0501, -0.0772,  0.0488, -0.0804, -0.1794, -0.3713],\n",
       "                       [-0.1580,  0.1311,  0.4319,  0.1866,  0.2455, -0.1794,  0.3661, -0.0128,\n",
       "                        -0.1555,  1.0419, -0.1366, -0.4843, -0.1319, -0.4148, -0.0125,  0.0751,\n",
       "                        -1.3980,  0.0896,  0.9472,  0.2596,  0.3575,  0.3717, -0.0334, -0.2370,\n",
       "                        -0.4570,  0.1396,  0.1282,  0.3130,  0.1249, -0.0699],\n",
       "                       [ 0.0573,  0.3130, -0.0906,  0.1090,  0.7808, -0.3886, -0.0488, -0.9113,\n",
       "                        -0.1929,  0.1216, -0.3552,  0.6675, -0.0234,  0.3252,  0.1565, -0.4406,\n",
       "                         0.3639, -0.1769, -0.8252,  0.3665,  0.5408, -0.2392,  0.1606, -0.6015,\n",
       "                         0.5995, -0.0521, -0.2681, -0.7666,  0.0234, -0.0811],\n",
       "                       [-0.0485, -0.7409,  0.2692,  0.1422,  1.1155, -0.2259, -0.6274,  0.1017,\n",
       "                         0.8706, -0.4422,  0.1802,  0.6299,  0.0563, -0.0923,  0.1837,  0.1706,\n",
       "                        -0.7018,  0.1295, -0.0525,  0.5857, -1.0897, -0.3469, -0.0107,  0.3285,\n",
       "                         0.7542, -0.0777, -0.5974, -0.1037, -0.0575, -0.3800]])),\n",
       "              ('linear_2.bias',\n",
       "               tensor([-0.0255,  0.0740, -0.2414, -0.1335,  0.2480, -0.1888,  0.0037, -0.0825,\n",
       "                       -0.0884, -0.0151]))]),\n",
       " 'Optimizer_param': {'state': {0: {'momentum_buffer': None},\n",
       "   1: {'momentum_buffer': None},\n",
       "   2: {'momentum_buffer': None},\n",
       "   3: {'momentum_buffer': None}},\n",
       "  'param_groups': [{'lr': 0.03,\n",
       "    'momentum': 0,\n",
       "    'dampening': 0,\n",
       "    'weight_decay': 0,\n",
       "    'nesterov': False,\n",
       "    'maximize': False,\n",
       "    'foreach': None,\n",
       "    'differentiable': False,\n",
       "    'params': [0, 1, 2, 3]}]},\n",
       " 'Loss_val': 0.08033851424665449}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.loadはモデルのパラメタ等以外も読み込める。\n",
    "torch.load('MLP_result.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'momentum_buffer': None},\n",
       "  1: {'momentum_buffer': None},\n",
       "  2: {'momentum_buffer': None},\n",
       "  3: {'momentum_buffer': None}},\n",
       " 'param_groups': [{'lr': 0.03,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False,\n",
       "   'params': [0, 1, 2, 3]}]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_result['Optimizer_param']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実際に別モデルでパラメタ読み込み\n",
    "state = torch.load('MLP_result.pth')\n",
    "another_model = MLP_1(64, 30, 10)\n",
    "\n",
    "# パラメタ読み込み。読み込む際はload_state_dictメソッドを用いる。stateは辞書型であることに注意\n",
    "another_model.load_state_dict(state['Model_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_1.weight',\n",
       "              tensor([[ 0.0700, -0.1004, -0.0958,  ...,  0.0803, -0.0952, -0.1111],\n",
       "                      [-0.0217, -0.0034,  0.0641,  ...,  0.1084, -0.2026,  0.0524],\n",
       "                      [-0.0530, -0.1565,  0.0017,  ..., -0.1727, -0.1910,  0.0113],\n",
       "                      ...,\n",
       "                      [ 0.0813, -0.1134, -0.1322,  ...,  0.0519,  0.1667, -0.0923],\n",
       "                      [ 0.0108, -0.0275,  0.0612,  ..., -0.1205,  0.0746, -0.1491],\n",
       "                      [-0.0228,  0.1435, -0.0761,  ...,  0.2858,  0.0682,  0.0191]])),\n",
       "             ('linear_1.bias',\n",
       "              tensor([-0.0830, -0.0070,  0.1434,  0.0112,  0.1515, -0.0662,  0.1211,  0.1132,\n",
       "                       0.0921,  0.1135,  0.1095, -0.0668,  0.0727,  0.0342, -0.0567, -0.0048,\n",
       "                       0.1008,  0.0768, -0.0148,  0.1612,  0.1404,  0.1004,  0.0127,  0.0010,\n",
       "                      -0.0450,  0.0162, -0.0276,  0.1602,  0.1638, -0.0307])),\n",
       "             ('linear_2.weight',\n",
       "              tensor([[-0.0318, -0.5894,  0.1456, -0.1555, -0.9568,  0.3701, -0.3410, -0.0286,\n",
       "                       -0.3724, -0.4933, -0.1625,  0.9660, -0.0342,  0.1317,  0.0707, -0.1142,\n",
       "                       -0.5325, -0.1342,  0.1449, -0.3716,  0.1038,  0.9506,  0.0465,  0.3959,\n",
       "                        0.6411,  0.0073, -0.0487, -0.2842, -0.0151, -0.2501],\n",
       "                      [ 0.0327,  0.0683,  0.6866,  0.1763, -0.2003, -0.0807, -0.1930,  0.0830,\n",
       "                       -0.2374,  0.4812,  0.5772, -0.0038,  0.1804,  0.7804, -0.1522,  0.5897,\n",
       "                        1.1089, -0.0220,  0.2558,  0.1514, -0.5734, -1.2495, -0.1267, -0.3113,\n",
       "                       -0.9910,  0.0760,  0.1469, -0.2387, -0.4049,  0.9614],\n",
       "                      [ 0.1088,  0.2004, -0.5474, -0.1400, -0.8510, -0.0250, -0.3883,  0.1756,\n",
       "                       -0.1119,  0.8575, -0.0235, -0.6610, -0.0368, -0.8062, -0.0378,  0.0377,\n",
       "                        0.8130,  0.0696, -0.3591,  0.6249,  0.4594, -0.4209, -0.0480,  1.4927,\n",
       "                        0.0964,  0.0842,  0.2333,  0.5050,  0.1258, -0.1495],\n",
       "                      [ 0.1078,  0.2152, -0.3248, -0.1773,  0.5239,  0.1627,  0.0234,  0.1249,\n",
       "                        1.0582, -0.0495, -0.1883, -0.4207,  0.1079, -0.9018, -0.1044, -0.5892,\n",
       "                       -0.1528, -0.1615, -0.2358, -0.4726,  0.2455, -0.3234, -0.0978,  0.1067,\n",
       "                        0.6508,  0.1580,  1.3225,  0.5765, -0.1405,  0.7186],\n",
       "                      [ 0.2458,  0.1072, -0.2119,  0.1028, -0.1651, -0.1893,  0.7406, -0.2147,\n",
       "                       -0.7133, -0.7397, -0.2037, -0.3446, -0.0257,  1.2206,  0.1270,  0.1283,\n",
       "                       -0.4997, -0.1301,  1.0124,  0.6494, -0.2147,  0.3403, -0.1518, -0.1588,\n",
       "                       -0.8238, -0.0790, -0.0592,  0.2155,  0.4029, -0.2347],\n",
       "                      [-0.0324, -0.4606,  0.2259, -0.2006,  0.4511,  0.4967, -0.6245,  0.5035,\n",
       "                        0.8981, -0.4544,  0.1213, -0.4492,  0.0144, -0.1700,  0.1067,  0.1795,\n",
       "                        0.3428, -0.1461, -0.3551, -0.9693, -0.0082,  1.2875, -0.0296, -0.5171,\n",
       "                       -0.7286, -0.0626, -0.5195, -0.2386, -0.1805, -0.1861],\n",
       "                      [-0.1479,  0.3855, -0.4237, -0.0352, -0.5678, -0.0039,  1.0784,  0.5914,\n",
       "                       -0.7283, -0.3756,  0.1443,  0.4805,  0.1735,  0.1106, -0.1003,  0.1804,\n",
       "                        0.6390, -0.1607, -0.6682, -0.3484,  0.5732, -0.1320, -0.1121, -0.5902,\n",
       "                        0.0501, -0.0772,  0.0488, -0.0804, -0.1794, -0.3713],\n",
       "                      [-0.1580,  0.1311,  0.4319,  0.1866,  0.2455, -0.1794,  0.3661, -0.0128,\n",
       "                       -0.1555,  1.0419, -0.1366, -0.4843, -0.1319, -0.4148, -0.0125,  0.0751,\n",
       "                       -1.3980,  0.0896,  0.9472,  0.2596,  0.3575,  0.3717, -0.0334, -0.2370,\n",
       "                       -0.4570,  0.1396,  0.1282,  0.3130,  0.1249, -0.0699],\n",
       "                      [ 0.0573,  0.3130, -0.0906,  0.1090,  0.7808, -0.3886, -0.0488, -0.9113,\n",
       "                       -0.1929,  0.1216, -0.3552,  0.6675, -0.0234,  0.3252,  0.1565, -0.4406,\n",
       "                        0.3639, -0.1769, -0.8252,  0.3665,  0.5408, -0.2392,  0.1606, -0.6015,\n",
       "                        0.5995, -0.0521, -0.2681, -0.7666,  0.0234, -0.0811],\n",
       "                      [-0.0485, -0.7409,  0.2692,  0.1422,  1.1155, -0.2259, -0.6274,  0.1017,\n",
       "                        0.8706, -0.4422,  0.1802,  0.6299,  0.0563, -0.0923,  0.1837,  0.1706,\n",
       "                       -0.7018,  0.1295, -0.0525,  0.5857, -1.0897, -0.3469, -0.0107,  0.3285,\n",
       "                        0.7542, -0.0777, -0.5974, -0.1037, -0.0575, -0.3800]])),\n",
       "             ('linear_2.bias',\n",
       "              tensor([-0.0255,  0.0740, -0.2414, -0.1335,  0.2480, -0.1888,  0.0037, -0.0825,\n",
       "                      -0.0884, -0.0151]))])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
