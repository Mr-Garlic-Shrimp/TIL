{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPXyS4jWb9YQ"
      },
      "source": [
        "# 深層学習ノートブック-10 GPUの利用\n",
        "Google ColaboratoryのGPUを使ってtensorの計算を行ってみる。  \n",
        "※以下コードはGoogle ColaboratoryのGPU: T4を使って実行すること。\n",
        "\n",
        "* tensor.to()メソッドでtensorを特定のデバイスに移動させる(新しいtensorを作成)  \n",
        "  * .to(‘cuda’)でGPUに移動させる。（CUDAで計算できるようにする。）\n",
        "  * .to(‘cpu’)でCPUに移動させる (デフォルトではCPU上に作られる)\n",
        "  * .to()で移動したtensorは別の新たなtensorとなることに注意\n",
        "\n",
        "\n",
        "* torch.cuda.is_available()でGPUの有無を確認できる\n",
        "  * GPUが使える状態であればTrueを返す\n",
        "* 異なる演算デバイス（CPU,GPU）にあるtensor同士は演算できないことに注意。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tCXgTzkCrktp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueE6PSIQc4Cc"
      },
      "source": [
        "## ■ CPUとGPUの計算速度比較\n",
        "巨大なtensorの演算を行う際のCPUとGPUの計算速度の違いを見てみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp5ouxPnd7eS",
        "outputId": "46c9a70d-6c8e-4c1b-c33c-aa35d5f3e108"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# GPU使用可能\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXXTl4cpskJw",
        "outputId": "2db840e5-6653-45d2-de50-118f708b0a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken on CPU: 27.71705 seconds\n",
            "Time taken on GPU: 0.11138 seconds\n"
          ]
        }
      ],
      "source": [
        "# CPU上でtensorを作成\n",
        "tensor_cpu = torch.randn(10000, 10000)\n",
        "\n",
        "# GPU上でTensorを作成（もしGPUが利用可能ならば）\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tensor_gpu = tensor_cpu.to(device) # GPU上に新たに作成\n",
        "\n",
        "# CPU上での計算の時間を測定\n",
        "start_time = time.time()\n",
        "result_cpu = torch.mm(tensor_cpu, tensor_cpu)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken on CPU: {end_time - start_time:.5f} seconds\")\n",
        "\n",
        "# GPU上での計算の時間を測定（もしGPUが利用可能ならば）\n",
        "if device == 'cuda':\n",
        "  start_time = time.time()\n",
        "  result_gpu = torch.mm(tensor_gpu, tensor_gpu)\n",
        "  end_time = time.time()\n",
        "  print(f\"Time taken on GPU: {end_time - start_time:.5f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Ez9qF9sm56"
      },
      "source": [
        "上記のように行列の積演算において、GPUを使用した場合の方が圧倒的に早い。  \n",
        "確かに積演算は並列化しやすそう。  \n",
        "深層学習において、行列の積演算は頻繁に行うので、GPUによる計算は必須といえる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWJ2AzKofrFC"
      },
      "source": [
        "## ■ MNISTのMLP学習ループをGPUで実行\n",
        "学習でGPUを使用するためには全ての計算の元になる特徴量やパラメタのtensorについて、.to('cuda)しておく必要がある。  \n",
        "なお、以下のような2層のMLPのような単純なモデルならCPUとGPUで計算速度はそれほど変わらない。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2Vrj8X66hVAF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vSygNVBQhfk9"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "# 隠れ層・出力層の入力側に相当するクラス\n",
        "class Linear:\n",
        "    def __init__(self, n_input, n_output) -> None:\n",
        "        self.W = ( torch.randn((n_output, n_input)) * torch.sqrt(torch.tensor(2./n_input)) ).to(device) # GPUで計算するために.to('cuda')\n",
        "        self.W.requires_grad = False\n",
        "        self.B = torch.zeros(size=(1, n_output)).to(device) # GPUで計算するために.to('cuda')\n",
        "        self.B.requires_grad = False # .to(device)した後はrequires_gradの設定が引き継がれないので、.to()した後にrequires_gradを設定する。\n",
        "\n",
        "    def forward(self, X): # Xには特徴量および隠れ層のAが入る。\n",
        "        self.X = X # backwardでも計算に使いたいので、インスタンス変数に格納\n",
        "        self.Z = X @ self.W.T + self.B\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z):\n",
        "        self.W.grad_ = Z.grad_.T @ self.X\n",
        "        self.B.grad_ = torch.sum(Z.grad_, dim=0)\n",
        "        self.X.grad_ = Z.grad_ @ self.W\n",
        "        return self.X.grad_ # Xおよび隠れ層のAの勾配を返す\n",
        "\n",
        "\n",
        "# 隠れ層の出力側に相当するクラス\n",
        "class ReLU:\n",
        "    def forward(self, Z):\n",
        "        self.Z = Z\n",
        "        return torch.where(Z > 0 , Z, 0.) # Aを返す\n",
        "\n",
        "    def backward(self, A):\n",
        "        self.Z.grad_ = A.grad_ * ( self.Z > 0 ).float()\n",
        "        return self.Z.grad_\n",
        "\n",
        "\n",
        "# 損失関数を計算するクラス\n",
        "class SoftmaxCrossEntoropy:\n",
        "    def forward(self, X, y_true):\n",
        "        max_val = X.max(dim=1, keepdim=True).values\n",
        "        # 各要素のe^xを計算（これが分子になる）\n",
        "        e_x = (X - max_val).exp()\n",
        "        denominator = e_x.sum(dim=1, keepdim=True) + 1e-10\n",
        "        self.softmax_out = e_x / denominator\n",
        "        self.loss = - (y_true * torch.log(self.softmax_out + 1e-10)).sum() / y_true.shape[0]\n",
        "\n",
        "        return self.loss, self.softmax_out\n",
        "\n",
        "    def backward(self, y_true): # 損失から最終出力の勾配を計算する。\n",
        "        return (self.softmax_out - y_true) / y_true.shape[0] #softmax_outはyの予測値(Z2およびA2に同じ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I56DxF-6igz2"
      },
      "outputs": [],
      "source": [
        "# 隠れ層１層の場合のMLP（スクラッチ）\n",
        "class MlpModel:\n",
        "\n",
        "    # コンストラクタでは特徴量数、隠れ層のノード数、出力層のノード数を受け取る。\n",
        "    def __init__(self, n_features, hidden_units, output_units) -> None:\n",
        "        self.Linear_1 = Linear(n_features, hidden_units)\n",
        "        self.relu = ReLU()\n",
        "        self.Linear_2 = Linear(hidden_units, output_units)\n",
        "        self.loss_func = SoftmaxCrossEntoropy()\n",
        "\n",
        "\n",
        "    def forward(self, X, y):\n",
        "        self.X = X\n",
        "        self.Z1 = self.Linear_1.forward(X)\n",
        "        self.A1 = self.relu.forward(self.Z1)\n",
        "        self.Z2 = self.Linear_2.forward(self.A1)\n",
        "        # 出力層の活性化関数は恒等関数としているので、loss_funcの引数はA2ではなくZ2でOK\n",
        "        self.loss, y_pred = self.loss_func.forward(self.Z2, y)\n",
        "        return self.loss , y_pred\n",
        "\n",
        "\n",
        "    # 明にW,Bを計算していないが、メソッド内部で計算されてインスタンス変数として保持されていることに注意。\n",
        "    def backward(self, y):\n",
        "        self.Z2.grad_ = self.loss_func.backward(y) # lossが求まったということはひとつ前の出力Z2の勾配が求まる\n",
        "        self.A1.grad_ = self.Linear_2.backward(self.Z2) # Z2の勾配が求まったということはひとつ前のA1の勾配が求まる。（同時にW2,B2の勾配も求まる）\n",
        "        self.Z1.grad_ = self.relu.backward(self.A1) # A1の勾配が求まったということはひとつ前のZ1の勾配が求まる。\n",
        "        self.X.grad_ = self.Linear_1.backward(self.Z1) # Z1の勾配が求まったということはひとつ前のXの勾配が求まる。（Xの勾配自体は格納しなくてもOk）\n",
        "\n",
        "\n",
        "    # 勾配の初期化\n",
        "    def zero_grad(self):\n",
        "        self.Linear_1.W.grad_ = None\n",
        "        self.Linear_1.B.grad_ = None\n",
        "        self.Linear_2.W.grad_ = None\n",
        "        self.Linear_2.B.grad_ = None\n",
        "\n",
        "\n",
        "    # パラメタの更新\n",
        "    def step(self, learning_rate):\n",
        "        self.Linear_1.W -= learning_rate * self.Linear_1.W.grad_\n",
        "        self.Linear_1.B -= learning_rate * self.Linear_1.B.grad_\n",
        "        self.Linear_2.W -= learning_rate * self.Linear_2.W.grad_\n",
        "        self.Linear_2.B -= learning_rate * self.Linear_2.B.grad_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgp7QPQsij6u",
        "outputId": "d3cbbd9e-7c5d-4873-ac12-d8ccd4459b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X: torch.Size([1797, 64])\n",
            "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
            "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
            "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
            "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
            "         0.,  0.,  0., 11., 16., 10.,  0.,  0.], device='cuda:0')\n",
            "==========================\n",
            "shape of y_train: torch.Size([1797])\n",
            "tensor([0, 1, 2,  ..., 8, 9, 8], device='cuda:0')\n",
            "shape of y_true: torch.Size([1797, 10])\n",
            "shape of train data: X_train:torch.Size([1437, 64]), y_train:torch.Size([1437, 10])\n",
            "shape of validation data: X_val:torch.Size([360, 64]), y_val:torch.Size([360, 10])\n"
          ]
        }
      ],
      "source": [
        "# 変数定義\n",
        "learning_rate = 0.03\n",
        "loss_log = []  #学習時の損失記録用のリスト\n",
        "\n",
        "# データロード\n",
        "dataset = datasets.load_digits()\n",
        "feature_names = dataset['feature_names']\n",
        "X = torch.tensor(dataset['data'], dtype=torch.float32).to(device)\n",
        "target = torch.tensor(dataset['target']).to(device)\n",
        "\n",
        "# shape確認\n",
        "print(f'shape of X: {X.shape}')\n",
        "print(X[1])\n",
        "print('==========================')\n",
        "print(f'shape of y_train: {target.shape}')\n",
        "print(target)\n",
        "\n",
        "# 目的変数のエンコーディング\n",
        "y_true = F.one_hot(target, num_classes=10)\n",
        "print(f'shape of y_true: {y_true.shape}')\n",
        "\n",
        "# 学習データと検証データを8:2に分ける\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_true, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'shape of train data: X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
        "print(f'shape of validation data: X_val:{X_val.shape}, y_val:{y_val.shape}')\n",
        "\n",
        "# 学習データ・検証データの標準化。検証データの標準化には学習データの平均、標準偏差を使用することに注意。\n",
        "X_mean = X_train.mean()\n",
        "X_std = X_train.std()\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_val = (X_val - X_mean) / X_std\n",
        "\n",
        "# データ数、特徴量数、隠れ層のノード数、最終的な出力数（ここではクラス数）を定義\n",
        "m, n_features = X_train.shape\n",
        "hidden_units = 30\n",
        "output_units = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoBCHbvdjQis",
        "outputId": "20319a60-9c36-4d41-dfef-81d3c4c96d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0: train error: 1.8746029138565063, validation error: 1.4139400720596313, validation accuracy: 0.5861111283302307\n",
            "epoch: 1: train error: 1.1241812189420064, validation error: 0.8558472990989685, validation accuracy: 0.8361111283302307\n",
            "epoch: 2: train error: 0.7318691783481174, validation error: 0.5895389318466187, validation accuracy: 0.8777778148651123\n",
            "epoch: 3: train error: 0.5324937442938487, validation error: 0.44710439443588257, validation accuracy: 0.8916667103767395\n",
            "epoch: 4: train error: 0.419802502128813, validation error: 0.3682768940925598, validation accuracy: 0.9055556058883667\n",
            "epoch: 5: train error: 0.34980526632732817, validation error: 0.3144545257091522, validation accuracy: 0.925000011920929\n",
            "epoch: 6: train error: 0.3068399126331011, validation error: 0.27759966254234314, validation accuracy: 0.9277778267860413\n",
            "epoch: 7: train error: 0.2691398332516352, validation error: 0.2512103319168091, validation accuracy: 0.925000011920929\n",
            "epoch: 8: train error: 0.2441841584112909, validation error: 0.22773203253746033, validation accuracy: 0.9333333373069763\n",
            "epoch: 9: train error: 0.22338613255156411, validation error: 0.2140507847070694, validation accuracy: 0.944444477558136\n",
            "epoch: 10: train error: 0.20656465672784383, validation error: 0.1966790109872818, validation accuracy: 0.9500000476837158\n",
            "epoch: 11: train error: 0.19220946613285278, validation error: 0.187037855386734, validation accuracy: 0.9555555582046509\n",
            "epoch: 12: train error: 0.1797481550110711, validation error: 0.17708997428417206, validation accuracy: 0.9527778029441833\n",
            "epoch: 13: train error: 0.17102068927552966, validation error: 0.168095663189888, validation accuracy: 0.9611111283302307\n",
            "epoch: 14: train error: 0.16153104934427473, validation error: 0.16438113152980804, validation accuracy: 0.9583333730697632\n",
            "epoch: 15: train error: 0.1534242750869857, validation error: 0.1553162783384323, validation accuracy: 0.9555555582046509\n",
            "epoch: 16: train error: 0.14732288618882497, validation error: 0.1498843878507614, validation accuracy: 0.9611111283302307\n",
            "epoch: 17: train error: 0.14060237457354863, validation error: 0.1438826024532318, validation accuracy: 0.9666666984558105\n",
            "epoch: 18: train error: 0.1346823203066985, validation error: 0.14252826571464539, validation accuracy: 0.9583333730697632\n",
            "epoch: 19: train error: 0.12910795095894073, validation error: 0.13613061606884003, validation accuracy: 0.9611111283302307\n",
            "epoch: 20: train error: 0.12398465929759873, validation error: 0.13292236626148224, validation accuracy: 0.9611111283302307\n",
            "epoch: 21: train error: 0.1197108951707681, validation error: 0.13103902339935303, validation accuracy: 0.9666666984558105\n",
            "epoch: 22: train error: 0.11589238088991907, validation error: 0.12681788206100464, validation accuracy: 0.9694444537162781\n",
            "epoch: 23: train error: 0.11193919405341149, validation error: 0.12399964034557343, validation accuracy: 0.9694444537162781\n",
            "epoch: 24: train error: 0.10817074047194587, validation error: 0.12140414118766785, validation accuracy: 0.9694444537162781\n",
            "epoch: 25: train error: 0.10488661465545496, validation error: 0.12044388055801392, validation accuracy: 0.9694444537162781\n",
            "epoch: 26: train error: 0.10186717162529628, validation error: 0.11980783939361572, validation accuracy: 0.9666666984558105\n",
            "epoch: 27: train error: 0.0995460373039047, validation error: 0.11527993530035019, validation accuracy: 0.9722222685813904\n",
            "epoch: 28: train error: 0.09524117737180657, validation error: 0.11471695452928543, validation accuracy: 0.9666666984558105\n",
            "epoch: 29: train error: 0.09332718327641487, validation error: 0.11201915889978409, validation accuracy: 0.9722222685813904\n"
          ]
        }
      ],
      "source": [
        "# MLPクラスのインスタンス生成\n",
        "model = MlpModel(n_features=n_features, hidden_units=hidden_units, output_units=output_units)\n",
        "\n",
        "# ミニバッチのサイズ定義\n",
        "batch_size = 32\n",
        "\n",
        "# 全ミニバッチの数。ミニバッチサイズで割ったときの余りも考慮してプラス１\n",
        "batch_num = len(y_train) // batch_size + 1\n",
        "\n",
        "# 各epochごとの学習データ・検証データでの損失記録用\n",
        "loss_per_epoch_train = []\n",
        "loss_per_epoch_val = []\n",
        "\n",
        "# 各epochごとの検証データでのAccuracy結果格納用\n",
        "accuracy_log = {}\n",
        "\n",
        "\n",
        "# for文で学習ループ作成\n",
        "for epoch in range(30):\n",
        "    # epochごとの損失を蓄積する用の変数\n",
        "    running_loss = 0\n",
        "    running_loss_val = 0\n",
        "\n",
        "    # バッチごとの処理対象データ開始・終了インデックスを初期化\n",
        "    batch_start_idx = 0\n",
        "    batch_end_idx = batch_size\n",
        "\n",
        "    # シャッフル後のindex\n",
        "    shuffled_indices = np.random.permutation(len(y_train))\n",
        "\n",
        "    # ミニバッチ勾配降下法\n",
        "    for i in range(batch_num):\n",
        "        #シャッフル後のindexからy,Xで同じ範囲を取り出しだしてミニバッチ作成\n",
        "        batch_indices = shuffled_indices[batch_start_idx : batch_end_idx]\n",
        "        y_train_batch = y_train[batch_indices, :]\n",
        "        X_batch = X_train[batch_indices, :]\n",
        "\n",
        "        # 順伝播の計算\n",
        "        loss, y_pred = model.forward(X_batch, y_train_batch)\n",
        "        # 逆伝播の計算\n",
        "        model.backward(y_train_batch)\n",
        "        # パラメタの更新\n",
        "        model.step(learning_rate=learning_rate)\n",
        "        # 勾配を初期化\n",
        "        model.zero_grad()\n",
        "\n",
        "        # 学習データに対するlossの計算・記録\n",
        "        loss_log.append(loss.item())\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # batch開始・終了インデックスを更新。スライシングの仕様上、endがlen(y_train)を超えても問題ない。\n",
        "        batch_start_idx += batch_size\n",
        "        batch_end_idx += batch_size\n",
        "\n",
        "\n",
        "    # 検証データに対する予測、lossの計算・記録（1epochにつき1回算出）\n",
        "    loss_val, y_pred_val = model.forward(X_val, y_val)\n",
        "\n",
        "    # epochの最終的な損失を出力。各バッチの損失の累積を全バッチ数で割って平均を求める\n",
        "    loss_per_epoch_train.append(running_loss / batch_num)\n",
        "    loss_per_epoch_val.append(loss_val.item())\n",
        "\n",
        "    # 検証データに対するaccuracyの計算\n",
        "    accuracy_log[epoch] = ( (torch.argmax(y_pred_val, dim=1) == y_val.argmax(dim=1)).sum() / len(y_val) ).item()\n",
        "    print(f'epoch: {epoch}: train error: {running_loss/batch_num}, validation error: {loss_val.item()}, validation accuracy: {accuracy_log[epoch]}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
