{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-3 pytorchによる多項ロジスティック回帰\n",
    "参考：  \n",
    "* [行列積](https://w3e.kanazawa-it.ac.jp/math/category/gyouretu/senkeidaisu/henkan-tex.cgi?target=/math/category/gyouretu/senkeidaisu/gyouretu-no-seki.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchのAutograd, Tensorを用いて多項ロジスティック回帰による分類をスクラッチで実装する。  \n",
    "ここで実装する一連の流れは深層学習におけるモデル学習プロセスの基礎であり、非常に重要。  \n",
    "\n",
    "ここではMNIST（手書き数字(0~9)）データセットを用いた10クラス分類のタスクを多項ロジスティック回帰で解くことを考える。  \n",
    "MNISTデータセットにある各pixel値(8x8=64)を特徴量として扱う。  \n",
    "\n",
    "データ数をm、特徴量の行列を$\\bm{X}$、$\\bm{X}$に対応する重み行列を$W^T$※、バイアス項の行列を$\\bm{b}$とすると、  \n",
    "softmax関数への入力$\\bm{z}$は下記のように表せる。（pytorchではこの形を採用している。）  \n",
    "※行列積の形で表わすために転置をとっている。\n",
    "\n",
    "\n",
    "$\\bm{z} = \\bm{XW}^T+\\bm{b} $  \n",
    "$=\\left(\n",
    "\\begin{matrix} \n",
    "x_{1,1} & x_{1,2} & ... & x_{1,64}\\\\ \n",
    "x_{2,1} & x_{2,2} & ... & x_{2, 64}\\\\\n",
    ". & . & ... & .\\\\\n",
    "x_{m,1} & x_{m,2} & ... & x_{m, 64}\n",
    "\\end{matrix} \n",
    "\\right)$\n",
    "$\\left(\n",
    "\\begin{matrix} \n",
    "w_{1,1} & w_{1,2} & ... & w_{1,10}\\\\ \n",
    "w_{2,1} & w_{2,2} & ... & x_{2,10}\\\\\n",
    ". & . & ... & .\\\\\n",
    "w_{64,1} & x_{64,2} & ... & x_{64, 10}\n",
    "\\end{matrix} \n",
    "\\right)$\n",
    "$+\\left(\n",
    "\\begin{matrix} \n",
    "b_{1} & b_{2} & ... & b_{10}\\\\ \n",
    "b_{1} & b_{2} & ... & b_{10}\\\\\n",
    ". & . & ... & .\\\\\n",
    "b_{1} & b_{2} & ... & b_{10}\n",
    "\\end{matrix}\n",
    "\\right)$  \n",
    "\n",
    "$=\\left(\n",
    "\\begin{matrix} \n",
    "\\sum_{k=1}^{64} x_{1, k}w_{k, 1}+b_1 & \\sum_{k=1}^{64} x_{1, k}w_{k, 2}+b_2 & ... & \\sum_{k=1}^{64} x_{1, k}w_{k, 10}+b_{10}\\\\ \n",
    "\\sum_{k=1}^{64} x_{2, k}w_{k, 1}+b_1 & \\sum_{k=1}^{64} x_{2, k}w_{k, 2}+b_2 & ... & \\sum_{k=1}^{64} x_{2, k}w_{k, 10}+b_{10}\\\\ \n",
    ". & . & ... & .\\\\\n",
    "\\sum_{k=1}^{64} x_{m, k}w_{k, 1}+b_1 & \\sum_{k=1}^{64} x_{m, k}w_{k, 2}+b_2 & ... & \\sum_{k=1}^{64} x_{m, k}w_{k, 10}+b_{10}\\\\ \n",
    "\\end{matrix} \n",
    "\\right)$\n",
    "\n",
    "$=\\left(\n",
    "\\begin{matrix} \n",
    "z_{1,1} & z_{1,2} & ... & z_{1,10}\\\\ \n",
    "z_{2,1} & z_{2,2} & ... & z_{2,10}\\\\\n",
    ". & . & ... & .\\\\\n",
    "z_{m,1} & z_{m,2} & ... & z_{m, 10}\n",
    "\\end{matrix} \n",
    "\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bm{X}$のshapeはデータ数m × 特徴量数64  \n",
    "$\\bm{W}^T$は重みづけの対象パラメータ数（すなわち特徴量数64）× 最終的な出力列数（10クラス分類なので10）※、  \n",
    "$\\bm{b}$は1 × 最終的な出力列数となるが、上記ではbroadcastされた状態で書いている。  \n",
    "最終的に$\\bm{z}$は各データ数 × クラス数というshapeで表わされ、クラスごとの線形回帰の結果が各要素に対応する。  \n",
    "\n",
    "※転置前の$\\bm{W}$は10*64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理・初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数定義\n",
    "learning_rate = 0.03\n",
    "loss_log = []  #損失記録用のリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([1797, 64])\n",
      "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
      "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
      "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
      "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
      "         0.,  0.,  0., 11., 16., 10.,  0.,  0.], dtype=torch.float64)\n",
      "==========================\n",
      "shape of Y: torch.Size([1797, 10])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# データロード\n",
    "dataset = sklearn.datasets.load_digits()\n",
    "feature_names = dataset['feature_names']\n",
    "X = torch.tensor(dataset['data'])\n",
    "y_true = torch.tensor(dataset['target'])\n",
    "\n",
    "# shape確認\n",
    "print(f'shape of X: {X.shape}')\n",
    "print(X[1])\n",
    "print('==========================')\n",
    "print(f'shape of Y: {Y.shape}')\n",
    "print(Y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetのimagesは1797 x 8 x 8の形式になっており、reshapeする必要があるため、'data'から読み込んだ方が楽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Y: torch.Size([1797, 10])\n",
      "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 目的変数のエンコーディング\n",
    "y_true = F.one_hot(y_true, num_classes=10)\n",
    "print(f'shape of Y: {y_true.shape}')\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの標準化（ピクセル値を標準化）\n",
    "for i in range(X.shape[1]):\n",
    "    mean = X[:, i].mean()\n",
    "    std = X[:, i].std()\n",
    "    if(std == 0.):\n",
    "        X[:, i] = 0.\n",
    "    else:\n",
    "        X[:, i] = (X[:, i] - mean) / std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchには標準で標準化のためのクラスがないので自前で実装する必要あり。  \n",
    "※同じようなスケールのピクセル値の列なので、全列まとめて平均とって標準偏差で割るでも大差ないかも。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n",
      "tensor([0.8464, 0.9092, 0.3408, 0.4982, 0.5163, 0.9932, 0.5947, 0.5541, 0.0591,\n",
      "        0.3156, 0.8722, 0.9538, 0.3694, 0.3806, 0.9416, 0.4064, 0.9667, 0.9310,\n",
      "        0.9185, 0.4682, 0.8012, 0.6893, 0.2805, 0.0374, 0.9744, 0.3576, 0.9610,\n",
      "        0.6810, 0.3514, 0.9300, 0.6123, 0.3133, 0.1097, 0.3547, 0.0034, 0.9448,\n",
      "        0.9941, 0.0708, 0.4644, 0.0984, 0.3339, 0.3055, 0.3337, 0.6091, 0.4175,\n",
      "        0.5785, 0.8451, 0.1898, 0.0282, 0.9654, 0.8262, 0.2924, 0.2932, 0.8604,\n",
      "        0.4287, 0.3145, 0.0717, 0.2354, 0.7837, 0.2466, 0.5344, 0.4616, 0.7356,\n",
      "        0.2248], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tensor([[0.5165, 0.1129, 0.7141, 0.1706, 0.0847, 0.3603, 0.1588, 0.1910, 0.8762,\n",
      "         0.1424]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 重みW^T、バイアス項bの初期化\n",
    "W = torch.rand(size=(10, 64) ,requires_grad=True) #出力×入力\n",
    "b = torch.rand(size=(1, 10), requires_grad=True) # 1 x 出力\n",
    "\n",
    "print(W.shape)\n",
    "print(W[1])\n",
    "print()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax関数・損失関数実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
