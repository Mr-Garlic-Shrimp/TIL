{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-3 pytorchによる多項ロジスティック回帰\n",
    "参考：  \n",
    "* [行列積](https://w3e.kanazawa-it.ac.jp/math/category/gyouretu/senkeidaisu/henkan-tex.cgi?target=/math/category/gyouretu/senkeidaisu/gyouretu-no-seki.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchのAutograd, Tensorを用いて多項ロジスティック回帰による分類をスクラッチで実装する。  \n",
    "ここで実装する一連の流れは深層学習におけるモデル学習プロセスの基礎であり、非常に重要。  \n",
    "\n",
    "ここではMNIST（手書き数字(0~9)）データセットを用いた10クラス分類のタスクを多項ロジスティック回帰で解くことを考える。  \n",
    "MNISTデータセットにある各pixel値(8x8=64)を特徴量として扱う。  \n",
    "\n",
    "データ数をm、特徴量の行列を$\\bm{X}$、$\\bm{X}$に対応する重み行列を$W^T$※、バイアス項の行列を$\\bm{b}$とすると、  \n",
    "softmax関数への入力$\\bm{z}$は下記のように表せる。（pytorchではこの形を採用している。）  \n",
    "※行列積の形で表わすために転置をとっている。\n",
    "\n",
    "\n",
    "$\\bm{z} = \\bm{XW}^T+\\bm{b} $  \n",
    "$=\\left(\n",
    "\\begin{matrix} \n",
    "x_{1,1} & x_{1,2} & ... & x_{1,64}\\\\ \n",
    "x_{2,1} & x_{2,2} & ... & x_{2, 64}\\\\\n",
    ". & . & ... & .\\\\\n",
    "x_{m,1} & x_{m,2} & ... & x_{m, 64}\n",
    "\\end{matrix} \n",
    "\\right)$\n",
    "$\\left(\n",
    "\\begin{matrix} \n",
    "w_{1,1} & w_{1,2} & ... & w_{1,10}\\\\ \n",
    "w_{2,1} & w_{2,2} & ... & x_{2,10}\\\\\n",
    ". & . & ... & .\\\\\n",
    "w_{64,1} & x_{64,2} & ... & x_{64, 10}\n",
    "\\end{matrix} \n",
    "\\right)$\n",
    "$+\\left(\n",
    "\\begin{matrix} \n",
    "b_{1} & b_{2} & ... & b_{10}\\\\ \n",
    "b_{1} & b_{2} & ... & b_{10}\\\\\n",
    ". & . & ... & .\\\\\n",
    "b_{1} & b_{2} & ... & b_{10}\n",
    "\\end{matrix}\n",
    "\\right)$  \n",
    "\n",
    "$=\\left(\n",
    "\\begin{matrix} \n",
    "\\sum_{k=1}^{64} x_{1, k}w_{k, 1}+b_1 & \\sum_{k=1}^{64} x_{1, k}w_{k, 2}+b_2 & ... & \\sum_{k=1}^{64} x_{1, k}w_{k, 10}+b_{10}\\\\ \n",
    "\\sum_{k=1}^{64} x_{2, k}w_{k, 1}+b_1 & \\sum_{k=1}^{64} x_{2, k}w_{k, 2}+b_2 & ... & \\sum_{k=1}^{64} x_{2, k}w_{k, 10}+b_{10}\\\\ \n",
    ". & . & ... & .\\\\\n",
    "\\sum_{k=1}^{64} x_{m, k}w_{k, 1}+b_1 & \\sum_{k=1}^{64} x_{m, k}w_{k, 2}+b_2 & ... & \\sum_{k=1}^{64} x_{m, k}w_{k, 10}+b_{10}\\\\ \n",
    "\\end{matrix} \n",
    "\\right)$\n",
    "\n",
    "$=\\left(\n",
    "\\begin{matrix} \n",
    "z_{1,1} & z_{1,2} & ... & z_{1,10}\\\\ \n",
    "z_{2,1} & z_{2,2} & ... & z_{2,10}\\\\\n",
    ". & . & ... & .\\\\\n",
    "z_{m,1} & z_{m,2} & ... & z_{m, 10}\n",
    "\\end{matrix} \n",
    "\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bm{X}$のshapeはデータ数m × 特徴量数64  \n",
    "$\\bm{W}^T$は重みづけの対象パラメータ数（すなわち特徴量数64）× 最終的な出力列数（10クラス分類なので10）※、  \n",
    "$\\bm{b}$は1 × 最終的な出力列数となるが、上記ではbroadcastされた状態で書いている。  \n",
    "最終的に$\\bm{z}$は各データ数 × クラス数というshapeで表わされ、クラスごとの線形回帰の結果(をsoftmaxに入れたもの)が各要素に対応する。  \n",
    "\n",
    "※転置前の$\\bm{W}$は10*64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大まかな流れ\n",
    "\n",
    "1. データロード\n",
    "2. 前処理\n",
    "    * 目的変数のエンコーディング： torch.nn.functional.one_hot()\n",
    "    * ピクセル値の標準化  \n",
    "3. パラメータ初期化\n",
    "    * torch.rand(requires_grad=True)\n",
    "4. 損失関数とsoftmax関数実装\n",
    "5. for文で学習ループ作成（５回）。※このループの単位をepochという  \n",
    "6. 入力データ$\\bm{X}$および教師ラベル$\\bm{Y}$作成（以降6~12まで１データずつ実行していくことに注意）\n",
    "7. 出力結果$\\bm{Z}$計算\n",
    "8. softmaxで予測値計算  \n",
    "    $\\sigma(\\bm{z})_j=\\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$\n",
    "9. 損失計算  \n",
    "    $L= - \\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K t_{ik}log(p_k(x_i))$\n",
    "10. 勾配計算  \n",
    "    パラメタを更新するには下記の偏微分係数が必要なので、pytorchのAutogradを用いて計算する。  \n",
    "    * 重みに対する微小変動： $\\frac{\\partial{L}}{\\partial{w^{(k)}}}(=\\frac{\\partial{z}}{\\partial{w^{(k)}}}\\frac{\\partial{L}}{\\partial{z}})$  \n",
    "    * バイアス項に対する微小変動： $\\frac{\\partial{L}}{\\partial{b}}(=\\frac{\\partial{z}}{\\partial{b}}\\frac{\\partial{L}}{\\partial{z}})$\n",
    "11. パラメタ更新  \n",
    "    $w:= w - \\alpha \\frac{\\partial{L}}{\\partial{w^{(k)}}}$  \n",
    "\n",
    "    $b:= w - \\alpha \\frac{\\partial{L}}{\\partial{b}}$\n",
    "\n",
    "12. 勾配初期化\n",
    "    * ループごとの勾配が蓄積されないように.grad.zero_()を用いて勾配を初期化する。  \n",
    "    ※ちなみにpytorchの中で最後にアンダースコアが入るメソッドは読み出し元のインスタンスの属性に値を代入することを示す。  \n",
    "    この場合は.grad.zero_()を読みだしたtensorのgradにゼロが代入される。\n",
    "13. 損失ログ出力\n",
    "    \n",
    "※なお、今回は深層学習における学習の大まかな流れを理解するのが目的のため、検証データと学習データには分けない。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●前処理・初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数定義\n",
    "learning_rate = 0.03\n",
    "loss_log = []  #損失記録用のリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([1797, 64])\n",
      "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
      "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
      "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
      "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
      "         0.,  0.,  0., 11., 16., 10.,  0.,  0.])\n",
      "==========================\n",
      "shape of y_true: torch.Size([1797])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# データロード\n",
    "dataset = sklearn.datasets.load_digits()\n",
    "feature_names = dataset['feature_names']\n",
    "X = torch.tensor(dataset['data'], dtype=torch.float32)\n",
    "y_true = torch.tensor(dataset['target'])\n",
    "\n",
    "# shape確認\n",
    "print(f'shape of X: {X.shape}')\n",
    "print(X[1])\n",
    "print('==========================')\n",
    "print(f'shape of y_true: {y_true.shape}')\n",
    "print(y_true[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasetのimagesは1797 x 8 x 8の形式になっており、reshapeする必要があるため、'data'から読み込んだ方が楽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Y: torch.Size([1797, 10])\n",
      "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 目的変数のエンコーディング\n",
    "y_true = F.one_hot(y_true, num_classes=10)\n",
    "print(f'shape of Y: {y_true.shape}')\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの標準化（ピクセル値を標準化）\n",
    "for i in range(X.shape[1]):\n",
    "    mean = X[:, i].mean()\n",
    "    std = X[:, i].std()\n",
    "    if(std == 0.):\n",
    "        X[:, i] = 0.\n",
    "    else:\n",
    "        X[:, i] = (X[:, i] - mean) / std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchには標準で標準化のためのクラスがないので自前で実装する必要あり。  \n",
    "※同じようなスケールのピクセル値の列なので、全列まとめて平均とって標準偏差で割るでも大差ないかも。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64])\n",
      "tensor([0.0227, 0.9421, 0.5021, 0.8388, 0.9864, 0.5923, 0.1090, 0.0051, 0.8055,\n",
      "        0.0908, 0.5841, 0.4883, 0.8976, 0.0647, 0.4772, 0.6451, 0.3096, 0.9897,\n",
      "        0.9734, 0.4007, 0.9470, 0.9302, 0.2551, 0.4683, 0.0594, 0.6069, 0.8163,\n",
      "        0.1613, 0.6683, 0.1241, 0.4196, 0.4832, 0.6270, 0.4533, 0.5314, 0.7607,\n",
      "        0.6978, 0.6434, 0.7128, 0.2330, 0.9794, 0.9176, 0.4067, 0.8524, 0.3710,\n",
      "        0.9714, 0.1738, 0.8821, 0.9454, 0.7294, 0.4256, 0.7925, 0.1103, 0.6862,\n",
      "        0.2150, 0.6147, 0.6424, 0.3359, 0.8738, 0.4065, 0.2581, 0.3718, 0.6058,\n",
      "        0.8760], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tensor([[0.8001, 0.7339, 0.0662, 0.1323, 0.1642, 0.7555, 0.8267, 0.5230, 0.3125,\n",
      "         0.4634]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 重みW^T、バイアス項bの初期化\n",
    "W = torch.rand(size=(10, 64) ,requires_grad=True) #出力×入力\n",
    "b = torch.rand(size=(1, 10), requires_grad=True) # 1 x 出力\n",
    "\n",
    "print(W.shape)\n",
    "print(W[1])\n",
    "print()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●softmax関数実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax関数の実装\n",
    "def softmax_func(X):\n",
    "    '''\n",
    "    X: input tensor.行は各データ、列は各クラスを想定。\n",
    "    '''\n",
    "    # x_kが大きすぎると、e^xがinfになるのでmax(x_1, x_2,...,x_K)を各x_kから引く。\n",
    "    # 各データ（各行）について最大値を求める必要があるので、dimにRank1(列方向で比較)を指定する。\n",
    "    max_val = X.max(dim=1, keepdim=True).values\n",
    "    # 各要素のe^xを計算（これが分子になる）\n",
    "    e_x = (X - max_val).exp()\n",
    "\n",
    "    # softmax関数の分母の計算\n",
    "    # 各データについて合計したいので、dim=1を設定。また、分母が0になることを防ぐために分母の式に1e-10を足しておく\n",
    "    denominator = e_x.sum(dim=1, keepdim=True) + 1e-10\n",
    "\n",
    "    return e_x / denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ※dim=1の補足\n",
    "列方向や行方向について統計量を求められるので便利だが、デフォルトではshapeが変わるので注意が必要。  \n",
    "これを防ぐにはkeepdim=Trueに設定するとよい。こうすると次元は元のまま維持される。    \n",
    "仮に上記関数にWを入れた場合を例に考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.max(dim=1).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.max(dim=1, keepdim=True).values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※keepdimを使わずにデフォルトで計算してしまった場合  \n",
    "max(dim=1)の結果は1次元のtensorであるため、  \n",
    "W(shape: 10 x 64)とはブロードキャスティングで演算ができない。  \n",
    "なのでWのRank0に合わせるようにreshapeしてから計算する必要がある。  \n",
    "分母計算時のsumでも同様。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 計算不可\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 計算不可\n",
    "W - W.max(dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8109, -0.9049, -0.9427, -0.9684, -0.1856, -0.9454, -0.5918, -0.0172,\n",
       "        -0.0462, -0.5656, -0.5253, -0.6007, -0.7508, -0.7995, -0.8733, -0.9322,\n",
       "        -0.6036, -0.9642, -0.9464, -0.5793, -0.8477, -0.2772, -0.1476, -0.6274,\n",
       "        -0.3832, -0.6449,  0.0000, -0.4132, -0.8661, -0.7283, -0.2291, -0.2358,\n",
       "        -0.1362, -0.6353, -0.5935, -0.4778, -0.0388, -0.7224, -0.8076, -0.2210,\n",
       "        -0.8226, -0.4989, -0.7863, -0.5660, -0.6022, -0.1474, -0.4801, -0.7416,\n",
       "        -0.8781, -0.5226, -0.2101, -0.1882, -0.4413, -0.2285, -0.9543, -0.6227,\n",
       "        -0.3525, -0.7373, -0.8028, -0.5557, -0.3733, -0.6344, -0.2418, -0.8341],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算可能\n",
    "( W - W.max(dim=1).values.reshape((W.shape[0], -1)) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●損失関数（交差エントロピー）実装\n",
    "$$L= - \\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K t_{ik}log(p_k(x_i))$$  \n",
    "\n",
    "$t_{ik}$はOne-Hot Encodingされた目的変数、$p_k(x_i)$は予測値（ソフトマックス関数の出力）に相当する。  \n",
    "$p_k(x_i)$が0になるとlog(0)になり、負の無限大に発散するため、  \n",
    "実装上は1e-10等の小さい値を足しておく。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多クラス分類に対応した交差エントロピー\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: tensor。One-Hot Encoding済みの正解ラベル。\n",
    "    y_pred: tensor。予測値。softmax関数の出力(0~100%)。\n",
    "    '''\n",
    "    # 損失を計算。最終的な損失はスカラーなので、dimを指定する必要はない。\n",
    "    return - (y_true * torch.log(y_pred + 1e-10)).sum() / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_trueは該当するクラス以外ゼロなので、そのまま予測値（log）とのアダマール積を計算すれば損失が計算できる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●学習\n",
    "以上を踏まえて教師ラベルを学習していく。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch(0): 0.382154775434428\n",
      "Loss of epoch(1): 0.13160581678828043\n",
      "Loss of epoch(2): 0.10013087428616226\n",
      "Loss of epoch(3): 0.08341532431430665\n",
      "Loss of epoch(4): 0.0736482421932089\n"
     ]
    }
   ],
   "source": [
    "# for文で学習ループ作成\n",
    "for epoch in range(5):\n",
    "    # epochごとの損失を蓄積する用の変数\n",
    "    running_loss = 0\n",
    "    # 1データずつ処理(損失計算、勾配計算、パラメタ更新)していくことに注意\n",
    "    for i in range(len(y_true)):\n",
    "        # 入力データXおよび教師ラベルのYを作成\n",
    "        y_true_1row = y_true[i].reshape(-1, y_true.shape[1]) # データ数 x クラス数\n",
    "        X_1row = X[i].reshape(-1, X.shape[1]) # データ数 x 特徴量数\n",
    "\n",
    "        # zの計算\n",
    "        z = X_1row @ W.T + b # 1 x クラス数\n",
    "\n",
    "        # softmaxで予測値算出\n",
    "        y_pred = softmax_func(z)\n",
    "\n",
    "        # 損失(L)計算.lossはtensorなので.item()で値だけ取り出す\n",
    "        loss = cross_entropy(y_true_1row, y_pred)\n",
    "        loss_log.append(loss.item())\n",
    "        # 計算したlossを累積\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Lの勾配計算。これをすることでw,bによるlossの偏微分係数が求められるようになる\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメタ更新。更新するだけなので勾配の保持は不要。\n",
    "        with torch.no_grad():\n",
    "            W -= learning_rate * W.grad\n",
    "            b -= learning_rate * b.grad\n",
    "            # print(W.grad)\n",
    "            # print(W.shape)\n",
    "\n",
    "        # 勾配初期化\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    # epochの最終的な損失を出力\n",
    "    print(f'Loss of epoch({epoch}): {running_loss / len(y_true)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W, bは1データごとに損失が小さくなる方向へ更新されていく。  \n",
    "更新されたW, bはepochをまたいでも引き継がれるので、epochの回数を重ねるほど最終的な損失は小さくなっていく。  \n",
    "\n",
    "数学的にイメージするならば、lossの導関数があってそこにデータを入れたときの偏微分係数が  \n",
    "0に近づくように(極小値に近づくように)パラメタを更新していくイメージか。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
