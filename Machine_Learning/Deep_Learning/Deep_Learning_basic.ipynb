{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習- 1\n",
    "参考：  \n",
    "* [Udemy講座：「①米国AI開発者がやさしく教える深層学習超入門第一弾【Pythonで実践】」](https://www.udemy.com/course/deeplearning1/learn/lecture/40143418)\n",
    "* https://axa.biopapyrus.jp/deep-learning/\n",
    "* [パーセプトロン](https://axa.biopapyrus.jp/deep-learning/perceptron.html)\n",
    "* [ニューラルネットワークの基礎](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層学習とは\n",
    "深層学習は機械学習のアルゴリズムの一種である。  \n",
    "従来の機械学習手法（ロジスティック回帰やGBDTなど）は人が特徴量を作ってモデルに渡して学習させていたため、  \n",
    "良い特徴量をいかに作るかが重要であった。  \n",
    "一方、深層学習では入力データをモデルに渡すとモデル自身が特徴量を作って学習するため、  \n",
    "良い特徴量を作ってくれるモデルをいかに作るかが重要という点で従来の機械学習手法と異なる。  \n",
    "深層学習ではニューラルネットワーク(NN)という人間の神経を模したモデルを用いてデータを学習する。  \n",
    "このNNの層を深くしていくことで精度が高いモデル構築することが出来る。  \n",
    "\n",
    "深層学習の適用例は下記。  \n",
    "昨今の生成AIも深層学習の応用であり、社会に非常に大きな影響を与えている技術の一つ。\n",
    "* 音声認識\n",
    "* 医用画像診断\n",
    "* 顔認証\n",
    "* 株価予想\n",
    "* 画像生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層学習の文脈でよく使われる記号・表記\n",
    "同様の意味でも統計学や深層学習以外の機械学習モデルで使用していた記号と深層学習で用いる記号は異なる。  \n",
    "(ドキュメントや論文によって表記は異なる。)\n",
    "\n",
    "### 例）線形回帰のモデル式\n",
    "$\\hat{y}=\\hat{f}(x)=\\theta_0 + \\theta_1x$　→ $z = w_1x + b$  \n",
    "\n",
    "深層学習において、最終的なモデルの予測値は$z$で表わすことが多い。  \n",
    "機械学習では特徴量$x$に対する重みに相当する量は$\\theta$で表すことが多かったが、  \n",
    "深層学習ではそのまま$weight$の$w$を用いて表す。またbias項は$b$で表わす。  \n",
    "\n",
    "### 例）シグモイド関数\n",
    "$z=\\frac{1}{1+e^{-x}}$  →  $\\sigma(x)=\\frac{1}{1+e^{-x}}$  \n",
    "\n",
    "詳細は後述するが、この$\\sigma(x)$はニューラルネットワーク（NN）における”活性化関数”で良く用いられる表記である。  \n",
    "シグモイド関数はNNの活性化関数としてよく使われる。\n",
    "\n",
    "### 例）ロジスティック回帰の式\n",
    "$p(x)=\\frac{1}{1+e^{-(\\theta_0 + \\theta_1)}}$  →  $p(x)=a=\\sigma(z)=\\sigma(w_1x + b)=\\frac{1}{1+e^{-(w_1x + b)}}$  \n",
    "\n",
    "活性化関数からの出力は$a$で表わすことが多い。  \n",
    "\n",
    "### 例）softmax関数の式\n",
    "$p_k(\\hat{\\bm{y}})=\\frac{e^{\\hat{y}_k}}{\\sum_{k=1}^K e^{\\hat{y}_k}}$  →  $\\sigma(\\bm{z})_j=\\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$\n",
    "\n",
    "* $\\hat{y}_k, e^{z_j}, e^{z_k}$にはkまたはj番目のクラス（赤など）を真の値として学習した線形回帰モデルの出力が入る。  \n",
    "$\\hat{\\bm{y}}, \\bm{z}$はクラスごとに値を持つベクトルであることに注意。  \n",
    "例）$\\bm{z}=(z_1, z_2, z_3)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "深層学習ライブラリとしてはよくPytorchが用いられる。  \n",
    "Tensorflowも一時期使われていたが、最近はPytorchが主流。  \n",
    "深層学習に使用するデータはPytorchのTensorモジュールで取り扱う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 計算グラフ\n",
    "Pytorchではtensor同士の計算を行った際に”計算グラフ”(Computational Graph)というものを構築し、保持しておく機能がある。  \n",
    "計算グラフは演算の計算過程のようなもので、計算グラフを用いることで任意の関数の勾配を容易に求めることが出来る。  \n",
    "また、可視化することで人間にも視覚的に理解しやすい。詳細は講義資料参照。  \n",
    "深層学習において、損失関数は重み$w_i$や重みによって決まる値を変数とする多変数関数だとみなし、  \n",
    "これらの連鎖律（チェインルール）によって損失関数の勾配($\\partial L/ \\partial w_i$)を求める際に計算グラフを用いている。\n",
    "\n",
    "参考：\n",
    "https://manabitimes.jp/math/1303"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
