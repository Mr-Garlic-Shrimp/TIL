{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習ノートブック-5 MLP(多層パーセプトロン)\n",
    "MNISTデータセットについて、多層パーセプトロンをスクラッチで実装して予測値を算出する\n",
    "\n",
    "実装方針：\n",
    "* 隠れ層は1層のみ\n",
    "* 隠れ層のニューロンの数: 30\n",
    "    * MNISTでは最終的に１データ当たり10の予測値（確率）が欲しいので、  \n",
    "    隠れ層のニューロンの数が30の場合、パラメタ$\\bm{W}, \\bm{b}$のshapeは下記である必要がある。\n",
    "        * 入力層→隠れ層: \n",
    "            * $\\bm{W}$: 30 x 特徴量数\n",
    "            * $\\bm{b}$: 1 x 30\n",
    "        * 隠れ層→出力層: \n",
    "            * $\\bm{W}$: クラス数(MNISTでは10) x 30\n",
    "            * $\\bm{b}$: 1 x 10\n",
    "* 隠れ層の活性化関数にはReLUを使用\n",
    "* モデルの関数を作成し，順伝播（単に入力層→出力層まで左から右に計算すること）で予測した結果を返す  \n",
    "  今回は逆伝搬（損失を計算してパラメタを更新）はやらない。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●前処理・初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  #pytorchの便利関数はFでimportすることが多い。\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# python debugerをインポート\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数定義\n",
    "learning_rate = 0.03\n",
    "loss_log = []  #学習時の損失記録用のリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: torch.Size([1797, 64])\n",
      "tensor([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,  9.,\n",
      "         0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7., 15., 16.,\n",
      "        16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,  0.,  0.,  0.,\n",
      "         1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,\n",
      "         0.,  0.,  0., 11., 16., 10.,  0.,  0.])\n",
      "==========================\n",
      "shape of y_train: torch.Size([1797])\n",
      "tensor([0, 1, 2,  ..., 8, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "# データロード\n",
    "dataset = datasets.load_digits()\n",
    "feature_names = dataset['feature_names']\n",
    "X = torch.tensor(dataset['data'], dtype=torch.float32)\n",
    "target = torch.tensor(dataset['target'])\n",
    "\n",
    "# shape確認\n",
    "print(f'shape of X: {X.shape}')\n",
    "print(X[1])\n",
    "print('==========================')\n",
    "print(f'shape of y_train: {target.shape}')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_true: torch.Size([1797, 10])\n"
     ]
    }
   ],
   "source": [
    "# 目的変数のエンコーディング\n",
    "y_true = F.one_hot(target, num_classes=10)\n",
    "print(f'shape of y_true: {y_true.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: X_train:torch.Size([1437, 64]), y_train:torch.Size([1437, 10])\n",
      "shape of validation data: X_val:torch.Size([360, 64]), y_val:torch.Size([360, 10])\n"
     ]
    }
   ],
   "source": [
    "# 学習データと検証データを8:2に分ける\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_true, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'shape of train data: X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'shape of validation data: X_val:{X_val.shape}, y_val:{y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ・検証データの標準化。検証データの標準化には学習データの平均、標準偏差を使用することに注意。  \n",
    "# 今回のようなデータの場合、全体の平均・標準偏差で標準化するでOK\n",
    "X_mean = X_train.mean()\n",
    "X_std = X_train.std()\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_val = (X_val - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ数、特徴量数、隠れ層のノード数、最終的な出力数（ここではクラス数）を定義\n",
    "m, nf = X_train.shape\n",
    "nh = 30\n",
    "n_out = 10\n",
    "\n",
    "# 入力層→隠れ層の重みW、バイアス項bの初期化\n",
    "W_1 = torch.randn(size=(nh, nf) ,requires_grad=True) #出力×入力\n",
    "B_1 = torch.zeros(size=(1, nh), requires_grad=True) # 1 x 出力\n",
    "\n",
    "# 隠れ層→出力層の重みW、バイアス項bの初期化\n",
    "W_2 = torch.randn(size=(n_out, nh) ,requires_grad=True) #出力×入力\n",
    "B_2 = torch.zeros(size=(1, n_out), requires_grad=True) # 1 x 出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は重みの初期化に際し、randではなくrandn(正規分布からの乱数)を用いている。  \n",
    "またbiasは０で初期化することがおおいので、zerosを使用している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax関数の実装\n",
    "def softmax_func(X):\n",
    "    '''\n",
    "    X: input tensor.行は各データ、列は各クラスを想定。\n",
    "    '''\n",
    "    # x_kが大きすぎると、e^xがinfになるのでmax(x_1, x_2,...,x_K)を各x_kから引く。\n",
    "    # 各データ（各行）について最大値を求める必要があるので、dimにRank1(列方向で比較)を指定する。\n",
    "    max_val = X.max(dim=1, keepdim=True).values\n",
    "    # 各要素のe^xを計算（これが分子になる）\n",
    "    e_x = (X - max_val).exp()\n",
    "\n",
    "    # softmax関数の分母の計算\n",
    "    # 各データについて合計したいので、dim=1を設定。また、分母が0になることを防ぐために分母の式に1e-10を足しておく\n",
    "    denominator = e_x.sum(dim=1, keepdim=True) + 1e-10\n",
    "\n",
    "    return e_x / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多クラス分類に対応した交差エントロピー\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: tensor。One-Hot Encoding済みの正解ラベル。\n",
    "    y_pred: tensor。予測値。softmax関数の出力(0~100%)。\n",
    "    '''\n",
    "    # 損失を計算。最終的な損失はスカラーなので、dimを指定する必要はない。\n",
    "    return - (y_true * torch.log(y_pred + 1e-10)).sum() / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorの線形結合を返す関数\n",
    "def linear_comb(X, W, B):\n",
    "    '''\n",
    "    X, W, B: torch.tensor\n",
    "    '''\n",
    "    return X @ W.T + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLUの実装\n",
    "def ReLU(Z):\n",
    "    '''\n",
    "    Z: torch.tensor\n",
    "    '''\n",
    "    # torch.whereによって要素ごとに条件が真・偽のときで別の値を返せる\n",
    "    # 下記では0より大きい要素はzの値そのままで、0以下は0.になる。\n",
    "    return torch.where(Z > 0 , Z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUの実装別解\n",
    "Z.clamp_min(0.)でもOK。  \n",
    "clamp_min(val)はvalよりも小さい値をvalで置き換える関数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPをスクラッチで実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隠れ層1層のMLPを実装\n",
    "def MLP_model(X, W_list, B_list):\n",
    "    '''\n",
    "    X: features\n",
    "    W: List of weights(each edge)\n",
    "    B: List of Bias(each Layer)\n",
    "    '''\n",
    "\n",
    "    # 入力層→隠れ層\n",
    "    Z1 = linear_comb(X, W_list[0], B_list[0])\n",
    "\n",
    "    # 活性化関数適用\n",
    "    A1 = ReLU(Z1)\n",
    "\n",
    "    # 隠れ層→出力層\n",
    "    Z2 = linear_comb(A1, W_list[1], B_list[1])\n",
    "\n",
    "    # 最終出力。ソフトマックス関数を適用して確率の形にする。\n",
    "    # (実際はこの変換は損失関数計算時にやることが多い)\n",
    "    A2 = softmax_func(Z2)\n",
    "\n",
    "    return A2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "冗長だが今回は各層の処理を一つずつ書いた。  \n",
    "実際はループにすべし。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5532e-15, 8.0889e-28, 0.0000e+00,  ..., 3.1388e-27, 3.7840e-08,\n",
       "         1.0627e-26],\n",
       "        [2.7773e-06, 3.6767e-24, 3.6031e-37,  ..., 2.8949e-18, 7.4035e-13,\n",
       "         9.2826e-21],\n",
       "        [9.7890e-01, 1.6970e-29, 3.2763e-37,  ..., 3.5054e-22, 1.0644e-11,\n",
       "         2.6484e-22],\n",
       "        ...,\n",
       "        [3.6281e-01, 3.1607e-24, 0.0000e+00,  ..., 2.1724e-28, 6.2918e-08,\n",
       "         5.0386e-41],\n",
       "        [2.5202e-01, 2.7486e-33, 2.4813e-31,  ..., 7.0383e-12, 2.0191e-08,\n",
       "         7.7509e-10],\n",
       "        [1.6545e-16, 2.8026e-45, 0.0000e+00,  ..., 6.7412e-34, 1.2109e-17,\n",
       "         2.9692e-27]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測結果算出\n",
    "W_list = [W_1, W_2]\n",
    "B_list = [B_1, B_2]\n",
    "\n",
    "y_train_pred = MLP_model(X_train, W_list=W_list, B_list=B_list)\n",
    "y_train_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
