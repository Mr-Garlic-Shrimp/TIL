{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スタッキングの実装\n",
    "ここでは2値分類用のスタッキングのクラスを実装する。  \n",
    "scikit-learnにもスタッキング用のクラスが存在するが、学習時にKFold CVをしていない模様。なので過学習気味になる恐れ。    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, label_binarize, OrdinalEncoder\n",
    "# import statsmodels.api as sma\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split ,LeaveOneOut, cross_val_score, KFold, RepeatedKFold,StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, log_loss, confusion_matrix,ConfusionMatrixDisplay, \\\n",
    "accuracy_score, precision_score, recall_score,precision_recall_curve,f1_score,roc_curve,auc,get_scorer_names,roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram,fcluster\n",
    "from sklearn import tree\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# polarsでタイタニックデータを読み込み\n",
    "df = pl.from_pandas(sns.load_dataset('titanic'))\n",
    "\n",
    "# 欠損値は落とさなくてよい\n",
    "df = df.drop_nulls()\n",
    "\n",
    "# 学習データ、目的変数を定義\n",
    "X = df.drop(['survived', 'alive'])\n",
    "y = df.get_column('survived')\n",
    "\n",
    "# カテゴリ変数のカラム名をリスト化\n",
    "category_cols = X.select(pl.col([pl.Utf8, pl.Categorical, pl.Boolean])).columns\n",
    "\n",
    "# ラベルエンコーディング（LabelEncoderではなく、OrdinalEncoderを使う）\n",
    "oe = OrdinalEncoder()\n",
    "# pandasで返ってくるように指定。polarsは指定できない模様\n",
    "oe.set_output(transform='pandas')\n",
    "# カテゴリ変数をエンコーディング。polars.DFはそのまま入れられないのでpandasに変換する。\n",
    "X = X.with_columns( pl.from_pandas(oe.fit_transform(X.select(category_cols).to_pandas())) )\n",
    "\n",
    "# hold-out\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.to_pandas(), y.to_pandas(), test_size=0.3, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装前の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfoldの定義\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "estimator = XGBClassifier(early_stopping_rounds=10, learning_rate=0.01, eval_metric='auc',random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.85264\n",
      "[1]\tvalidation_0-auc:0.85382\n",
      "[2]\tvalidation_0-auc:0.90184\n",
      "[3]\tvalidation_0-auc:0.86733\n",
      "[4]\tvalidation_0-auc:0.86838\n",
      "[5]\tvalidation_0-auc:0.86838\n",
      "[6]\tvalidation_0-auc:0.86838\n",
      "[7]\tvalidation_0-auc:0.86838\n",
      "[8]\tvalidation_0-auc:0.86838\n",
      "[9]\tvalidation_0-auc:0.86798\n",
      "[10]\tvalidation_0-auc:0.86838\n",
      "[11]\tvalidation_0-auc:0.86825\n",
      "[0]\tvalidation_0-auc:0.82661\n",
      "[1]\tvalidation_0-auc:0.82990\n",
      "[2]\tvalidation_0-auc:0.82910\n",
      "[3]\tvalidation_0-auc:0.82896\n",
      "[4]\tvalidation_0-auc:0.82896\n",
      "[5]\tvalidation_0-auc:0.82923\n",
      "[6]\tvalidation_0-auc:0.82937\n",
      "[7]\tvalidation_0-auc:0.82977\n",
      "[8]\tvalidation_0-auc:0.82977\n",
      "[9]\tvalidation_0-auc:0.82795\n",
      "[10]\tvalidation_0-auc:0.82795\n",
      "[11]\tvalidation_0-auc:0.82822\n",
      "[0]\tvalidation_0-auc:0.87184\n",
      "[1]\tvalidation_0-auc:0.87270\n",
      "[2]\tvalidation_0-auc:0.87385\n",
      "[3]\tvalidation_0-auc:0.87457\n",
      "[4]\tvalidation_0-auc:0.87514\n",
      "[5]\tvalidation_0-auc:0.87514\n",
      "[6]\tvalidation_0-auc:0.87514\n",
      "[7]\tvalidation_0-auc:0.87514\n",
      "[8]\tvalidation_0-auc:0.87514\n",
      "[9]\tvalidation_0-auc:0.88182\n",
      "[10]\tvalidation_0-auc:0.88182\n",
      "[11]\tvalidation_0-auc:0.88168\n",
      "[12]\tvalidation_0-auc:0.88197\n",
      "[13]\tvalidation_0-auc:0.88628\n",
      "[14]\tvalidation_0-auc:0.88168\n",
      "[15]\tvalidation_0-auc:0.88685\n",
      "[16]\tvalidation_0-auc:0.88685\n",
      "[17]\tvalidation_0-auc:0.88614\n",
      "[18]\tvalidation_0-auc:0.88628\n",
      "[19]\tvalidation_0-auc:0.88606\n",
      "[20]\tvalidation_0-auc:0.88606\n",
      "[21]\tvalidation_0-auc:0.88592\n",
      "[22]\tvalidation_0-auc:0.88592\n",
      "[23]\tvalidation_0-auc:0.88621\n",
      "[24]\tvalidation_0-auc:0.88664\n",
      "[25]\tvalidation_0-auc:0.88678\n",
      "[0]\tvalidation_0-auc:0.85350\n",
      "[1]\tvalidation_0-auc:0.85673\n",
      "[2]\tvalidation_0-auc:0.86110\n",
      "[3]\tvalidation_0-auc:0.86083\n",
      "[4]\tvalidation_0-auc:0.86204\n",
      "[5]\tvalidation_0-auc:0.86144\n",
      "[6]\tvalidation_0-auc:0.86292\n",
      "[7]\tvalidation_0-auc:0.86184\n",
      "[8]\tvalidation_0-auc:0.86251\n",
      "[9]\tvalidation_0-auc:0.86332\n",
      "[10]\tvalidation_0-auc:0.86332\n",
      "[11]\tvalidation_0-auc:0.86386\n",
      "[12]\tvalidation_0-auc:0.86399\n",
      "[13]\tvalidation_0-auc:0.86305\n",
      "[14]\tvalidation_0-auc:0.86305\n",
      "[15]\tvalidation_0-auc:0.86318\n",
      "[16]\tvalidation_0-auc:0.86372\n",
      "[17]\tvalidation_0-auc:0.86359\n",
      "[18]\tvalidation_0-auc:0.86359\n",
      "[19]\tvalidation_0-auc:0.86359\n",
      "[20]\tvalidation_0-auc:0.86318\n",
      "[21]\tvalidation_0-auc:0.86292\n",
      "[22]\tvalidation_0-auc:0.86292\n",
      "[0]\tvalidation_0-auc:0.87234\n",
      "[1]\tvalidation_0-auc:0.87311\n",
      "[2]\tvalidation_0-auc:0.87272\n",
      "[3]\tvalidation_0-auc:0.87222\n",
      "[4]\tvalidation_0-auc:0.87890\n",
      "[5]\tvalidation_0-auc:0.87813\n",
      "[6]\tvalidation_0-auc:0.87877\n",
      "[7]\tvalidation_0-auc:0.87775\n",
      "[8]\tvalidation_0-auc:0.87826\n",
      "[9]\tvalidation_0-auc:0.87775\n",
      "[10]\tvalidation_0-auc:0.87845\n",
      "[11]\tvalidation_0-auc:0.87833\n",
      "[12]\tvalidation_0-auc:0.88036\n",
      "[13]\tvalidation_0-auc:0.88030\n",
      "[14]\tvalidation_0-auc:0.88004\n",
      "[15]\tvalidation_0-auc:0.88004\n",
      "[16]\tvalidation_0-auc:0.87966\n",
      "[17]\tvalidation_0-auc:0.87979\n",
      "[18]\tvalidation_0-auc:0.87966\n",
      "[19]\tvalidation_0-auc:0.87903\n",
      "[20]\tvalidation_0-auc:0.87915\n",
      "[21]\tvalidation_0-auc:0.87903\n",
      "CPU times: user 30.4 s, sys: 3.35 s, total: 33.8 s\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_pd = X.to_pandas()\n",
    "y_pd = y.to_pandas()\n",
    "y_pred_proba_li = []\n",
    "\n",
    "# Layer1\n",
    "for train_index, val_index in cv.split(X_pd):\n",
    "    X_train, X_val = X_pd.iloc[train_index], X_pd.iloc[val_index]\n",
    "    y_train, y_val = y_pd.iloc[train_index], y_pd.iloc[val_index]\n",
    "    \n",
    "    # モデル学習\n",
    "    eval_set = [(X_val, y_val)]\n",
    "    estimator.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "    # 検証データ(学習に使っていないデータ)に対する予測値算出\n",
    "    y_pred_proba = estimator.predict_proba(X_val)\n",
    "\n",
    "    # 予測値を追加していく\n",
    "    y_pred_proba_li.append(y_pred_proba)\n",
    "\n",
    "result = np.concatenate(y_pred_proba_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "○ メモ  \n",
    "逐次concatenateする場合繰り返しconcatenateのオーバーヘッドがあるので、  \n",
    "リストでまとめてconcatするよりも時間がかかるらしい。（大規模データの場合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.85264\n",
      "[1]\tvalidation_0-auc:0.85382\n",
      "[2]\tvalidation_0-auc:0.90184\n",
      "[3]\tvalidation_0-auc:0.86733\n",
      "[4]\tvalidation_0-auc:0.86838\n",
      "[5]\tvalidation_0-auc:0.86838\n",
      "[6]\tvalidation_0-auc:0.86838\n",
      "[7]\tvalidation_0-auc:0.86838\n",
      "[8]\tvalidation_0-auc:0.86838\n",
      "[9]\tvalidation_0-auc:0.86798\n",
      "[10]\tvalidation_0-auc:0.86838\n",
      "[11]\tvalidation_0-auc:0.86825\n",
      "[0]\tvalidation_0-auc:0.82661\n",
      "[1]\tvalidation_0-auc:0.82990\n",
      "[2]\tvalidation_0-auc:0.82910\n",
      "[3]\tvalidation_0-auc:0.82896\n",
      "[4]\tvalidation_0-auc:0.82896\n",
      "[5]\tvalidation_0-auc:0.82923\n",
      "[6]\tvalidation_0-auc:0.82937\n",
      "[7]\tvalidation_0-auc:0.82977\n",
      "[8]\tvalidation_0-auc:0.82977\n",
      "[9]\tvalidation_0-auc:0.82795\n",
      "[10]\tvalidation_0-auc:0.82795\n",
      "[0]\tvalidation_0-auc:0.87184\n",
      "[1]\tvalidation_0-auc:0.87270\n",
      "[2]\tvalidation_0-auc:0.87385\n",
      "[3]\tvalidation_0-auc:0.87457\n",
      "[4]\tvalidation_0-auc:0.87514\n",
      "[5]\tvalidation_0-auc:0.87514\n",
      "[6]\tvalidation_0-auc:0.87514\n",
      "[7]\tvalidation_0-auc:0.87514\n",
      "[8]\tvalidation_0-auc:0.87514\n",
      "[9]\tvalidation_0-auc:0.88182\n",
      "[10]\tvalidation_0-auc:0.88182\n",
      "[11]\tvalidation_0-auc:0.88168\n",
      "[12]\tvalidation_0-auc:0.88197\n",
      "[13]\tvalidation_0-auc:0.88628\n",
      "[14]\tvalidation_0-auc:0.88168\n",
      "[15]\tvalidation_0-auc:0.88685\n",
      "[16]\tvalidation_0-auc:0.88685\n",
      "[17]\tvalidation_0-auc:0.88614\n",
      "[18]\tvalidation_0-auc:0.88628\n",
      "[19]\tvalidation_0-auc:0.88606\n",
      "[20]\tvalidation_0-auc:0.88606\n",
      "[21]\tvalidation_0-auc:0.88592\n",
      "[22]\tvalidation_0-auc:0.88592\n",
      "[23]\tvalidation_0-auc:0.88621\n",
      "[24]\tvalidation_0-auc:0.88664\n",
      "[0]\tvalidation_0-auc:0.85350\n",
      "[1]\tvalidation_0-auc:0.85673\n",
      "[2]\tvalidation_0-auc:0.86110\n",
      "[3]\tvalidation_0-auc:0.86083\n",
      "[4]\tvalidation_0-auc:0.86204\n",
      "[5]\tvalidation_0-auc:0.86144\n",
      "[6]\tvalidation_0-auc:0.86292\n",
      "[7]\tvalidation_0-auc:0.86184\n",
      "[8]\tvalidation_0-auc:0.86251\n",
      "[9]\tvalidation_0-auc:0.86332\n",
      "[10]\tvalidation_0-auc:0.86332\n",
      "[11]\tvalidation_0-auc:0.86386\n",
      "[12]\tvalidation_0-auc:0.86399\n",
      "[13]\tvalidation_0-auc:0.86305\n",
      "[14]\tvalidation_0-auc:0.86305\n",
      "[15]\tvalidation_0-auc:0.86318\n",
      "[16]\tvalidation_0-auc:0.86372\n",
      "[17]\tvalidation_0-auc:0.86359\n",
      "[18]\tvalidation_0-auc:0.86359\n",
      "[19]\tvalidation_0-auc:0.86359\n",
      "[20]\tvalidation_0-auc:0.86318\n",
      "[21]\tvalidation_0-auc:0.86292\n",
      "[22]\tvalidation_0-auc:0.86292\n",
      "[0]\tvalidation_0-auc:0.87234\n",
      "[1]\tvalidation_0-auc:0.87311\n",
      "[2]\tvalidation_0-auc:0.87272\n",
      "[3]\tvalidation_0-auc:0.87222\n",
      "[4]\tvalidation_0-auc:0.87890\n",
      "[5]\tvalidation_0-auc:0.87813\n",
      "[6]\tvalidation_0-auc:0.87877\n",
      "[7]\tvalidation_0-auc:0.87775\n",
      "[8]\tvalidation_0-auc:0.87826\n",
      "[9]\tvalidation_0-auc:0.87775\n",
      "[10]\tvalidation_0-auc:0.87845\n",
      "[11]\tvalidation_0-auc:0.87833\n",
      "[12]\tvalidation_0-auc:0.88036\n",
      "[13]\tvalidation_0-auc:0.88030\n",
      "[14]\tvalidation_0-auc:0.88004\n",
      "[15]\tvalidation_0-auc:0.88004\n",
      "[16]\tvalidation_0-auc:0.87966\n",
      "[17]\tvalidation_0-auc:0.87979\n",
      "[18]\tvalidation_0-auc:0.87966\n",
      "[19]\tvalidation_0-auc:0.87903\n",
      "[20]\tvalidation_0-auc:0.87915\n",
      "[21]\tvalidation_0-auc:0.87903\n",
      "CPU times: user 41.7 s, sys: 3.89 s, total: 45.6 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_pd = X.to_pandas()\n",
    "y_pd = y.to_pandas()\n",
    "# 予測値を入れる初期の空の配列を作成\n",
    "result_2 = np.empty((0, 2))\n",
    "\n",
    "# Layer1\n",
    "for train_index, val_index in cv.split(X_pd ,y_pd):\n",
    "    X_train, X_val = X_pd.iloc[train_index], X_pd.iloc[val_index]\n",
    "    y_train, y_val = y_pd.iloc[train_index], y_pd.iloc[val_index]\n",
    "    \n",
    "    # モデル学習\n",
    "    eval_set = [(X_val, y_val)]\n",
    "    estimator.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "    # 検証データ(学習に使っていないデータ)に対する予測値算出\n",
    "    y_pred_proba = estimator.predict_proba(X_val)\n",
    "\n",
    "    # 予測値を追加していく\n",
    "    result_2 = np.concatenate((result, y_pred_proba))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 3]\n",
    "a += [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 改善前\n",
    "下記は最初にスクラッチで実装したときのコード。  \n",
    "CVによってX,yの元のインデックスから変わってしまうことに気づかずに進めてしまった。  \n",
    "X_valに対するpredict_probaの結果をそのままリストに追加してconcatすると、  \n",
    "numpyに合わせてインデックスが０からふり直されてしまうことになる。  \n",
    "これを正しく行うには元々のpandasのインデックスを保持したまま予測結果を格納する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStackingClassifierCV:\n",
    "    def __init__(self, estimators, final_estimator=None, cv=None):\n",
    "        self.estimators = estimators\n",
    "        self.final_estimator = final_estimator\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # CVでの学習済みモデルからの予測結果格納用（Layer2学習用）\n",
    "        self.y_pred_dict_for_layer2 = {}\n",
    "        # テストデータに対する予測のためのモデル格納用\n",
    "        self.estimators_for_test = {}\n",
    "\n",
    "        # Layer1の学習    \n",
    "        for estimator_name, estimator in self.estimators:\n",
    "            # モデル名と予測値のリストを対応させる。\n",
    "            self.y_pred_dict_for_layer2[estimator_name] = []\n",
    "            # テストデータに対するLayer1での予測値格納用\n",
    "\n",
    "            # Layer2へ渡す特徴量生成のための学習\n",
    "            for train_index, val_index in self.cv.split(X):\n",
    "                # 学習用データと予測値算出用データに分ける\n",
    "                X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                \n",
    "                if estimator_name == 'XGB':\n",
    "                    # XGBoostのモデル学習\n",
    "                    eval_set = [(X_val, y_val)]\n",
    "                    estimator.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "                elif estimator_name == 'LGBM':\n",
    "                    # LightGBMのモデル学習\n",
    "                    eval_set = [(X_val, y_val)]\n",
    "                    callbacks = [lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation()]\n",
    "                    estimator.fit(X_train, y_train, eval_set=eval_set, callbacks=callbacks)\n",
    "                else:\n",
    "                    # 決定木のモデル学習\n",
    "                    estimator.fit(X_train, y_train)\n",
    "\n",
    "                # 学習に使わなかったデータに対する予測値を算出し、リストに追加\n",
    "                self.y_pred_dict_for_layer2[estimator_name].append(estimator.predict_proba(X_val))\n",
    "\n",
    "            # 全foldでの予測値を結合してそのモデルの最終的な予測値を算出\n",
    "            # !!! 結合した結果は元の入力データX,yと順番は異なってしまっていることに注意(CV時にシャッフルしているため)。\n",
    "            self.y_pred_dict_for_layer2[estimator_name] = np.concatenate(self.y_pred_dict_for_layer2[estimator_name])\n",
    "\n",
    "            # テストデータに対する予測のための学習\n",
    "            if estimator_name == 'XGB':\n",
    "                # XGBoostのモデル学習\n",
    "                X_train2, X_val2, y_train2, y_val2 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "                eval_set = [(X_val2, y_val2)]\n",
    "                self.estimators_for_test[estimator_name] = estimator.fit(X_train2, y_train2, eval_set=eval_set, verbose=True)\n",
    "            elif estimator_name == 'LGBM':\n",
    "                # LightGBMのモデル学習\n",
    "                X_train2, X_val2, y_train2, y_val2 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "                eval_set = [(X_val2, y_val2)]\n",
    "                callbacks = [lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation()]\n",
    "                self.estimators_for_test[estimator_name] = estimator.fit(X_train2, y_train2, eval_set=eval_set, callbacks=callbacks)\n",
    "            else:\n",
    "                # 決定木のモデル学習\n",
    "                self.estimators_for_test[estimator_name] = estimator.fit(X, y)\n",
    "        \n",
    "        # Layer1での予測値をまとめる（Layer2へ渡す用）。array[[モデル1の予測結果×2列, モデル2の予測結果×2列・・・]]の形式。\n",
    "        # concatenateで横に結合していく。予測結果が×２列になるのはpredict_probaの結果だから。\n",
    "        self.result_layer1 = np.concatenate(list(self.y_pred_dict_for_layer2.values()), axis=1)\n",
    "\n",
    "        # Layer2の学習。元々の特徴量＋layer1の結果を特徴量とする。\n",
    "        X_train_layer2 = np.concatenate([X, self.result_layer1], axis=1)\n",
    "        self.final_estimator.fit(X_train_layer2, y)\n",
    "\n",
    "        #return self.y_pred_dict_for_layer2\n",
    "        return self.result_layer1\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        # Layer1での学習済みモデルを使ってテストデータに対して予測\n",
    "        # result_layer1_for_testの予測値・モデルの並びとresult_layer1の並びは同じなので、そのままlayer2に渡してOK\n",
    "        result_layer1_for_test = [estimator.predict_proba(X_test) for _, estimator in self.estimators_for_test.items()]\n",
    "        print(result_layer1_for_test)\n",
    "        result_layer1_for_test = np.concatenate(result_layer1_for_test, axis=1)\n",
    "        print(result_layer1_for_test)\n",
    "        \n",
    "        # テストデータに対する最終的な予測（Layer2）。元々の特徴量＋layer1の結果を特徴量とする。\n",
    "        X_test_layer2 = np.concatenate([X_test, result_layer1_for_test], axis=1)\n",
    "        result = self.final_estimator.predict_proba(X_test_layer2)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 呼び出し側\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "estimators = [\n",
    "    ('DT', tree.DecisionTreeClassifier(max_depth=2)),\n",
    "    ('XGB', XGBClassifier(early_stopping_rounds=10, learning_rate=0.01, eval_metric='auc',random_state=0)),\n",
    "    ('LGBM', lgb.LGBMClassifier(boosting_type='goss', max_depth=5, random_state=0))\n",
    "]\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# スタッキングのインスタンス生成\n",
    "model = MyStackingClassifierCV(estimators=estimators, final_estimator=final_estimator,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.80303\n",
      "[1]\tvalidation_0-auc:0.82121\n",
      "[2]\tvalidation_0-auc:0.82121\n",
      "[3]\tvalidation_0-auc:0.82121\n",
      "[4]\tvalidation_0-auc:0.82121\n",
      "[5]\tvalidation_0-auc:0.82121\n",
      "[6]\tvalidation_0-auc:0.81212\n",
      "[7]\tvalidation_0-auc:0.81212\n",
      "[8]\tvalidation_0-auc:0.78182\n",
      "[9]\tvalidation_0-auc:0.75152\n",
      "[10]\tvalidation_0-auc:0.73939\n",
      "[11]\tvalidation_0-auc:0.73333\n",
      "[0]\tvalidation_0-auc:0.81818\n",
      "[1]\tvalidation_0-auc:0.81818\n",
      "[2]\tvalidation_0-auc:0.81818\n",
      "[3]\tvalidation_0-auc:0.81818\n",
      "[4]\tvalidation_0-auc:0.82955\n",
      "[5]\tvalidation_0-auc:0.82955\n",
      "[6]\tvalidation_0-auc:0.82955\n",
      "[7]\tvalidation_0-auc:0.82955\n",
      "[8]\tvalidation_0-auc:0.82955\n",
      "[9]\tvalidation_0-auc:0.81250\n",
      "[10]\tvalidation_0-auc:0.81250\n",
      "[11]\tvalidation_0-auc:0.81250\n",
      "[12]\tvalidation_0-auc:0.81250\n",
      "[13]\tvalidation_0-auc:0.81250\n",
      "[0]\tvalidation_0-auc:0.83333\n",
      "[1]\tvalidation_0-auc:0.85417\n",
      "[2]\tvalidation_0-auc:0.85417\n",
      "[3]\tvalidation_0-auc:0.85417\n",
      "[4]\tvalidation_0-auc:0.85417\n",
      "[5]\tvalidation_0-auc:0.85417\n",
      "[6]\tvalidation_0-auc:0.85417\n",
      "[7]\tvalidation_0-auc:0.85417\n",
      "[8]\tvalidation_0-auc:0.85417\n",
      "[9]\tvalidation_0-auc:0.85417\n",
      "[10]\tvalidation_0-auc:0.85417\n",
      "[0]\tvalidation_0-auc:0.71333\n",
      "[1]\tvalidation_0-auc:0.73000\n",
      "[2]\tvalidation_0-auc:0.71000\n",
      "[3]\tvalidation_0-auc:0.73000\n",
      "[4]\tvalidation_0-auc:0.71000\n",
      "[5]\tvalidation_0-auc:0.72333\n",
      "[6]\tvalidation_0-auc:0.69667\n",
      "[7]\tvalidation_0-auc:0.72667\n",
      "[8]\tvalidation_0-auc:0.71333\n",
      "[9]\tvalidation_0-auc:0.72667\n",
      "[10]\tvalidation_0-auc:0.71333\n",
      "[11]\tvalidation_0-auc:0.72667\n",
      "[0]\tvalidation_0-auc:0.64931\n",
      "[1]\tvalidation_0-auc:0.59375\n",
      "[2]\tvalidation_0-auc:0.56597\n",
      "[3]\tvalidation_0-auc:0.56597\n",
      "[4]\tvalidation_0-auc:0.56597\n",
      "[5]\tvalidation_0-auc:0.56597\n",
      "[6]\tvalidation_0-auc:0.56597\n",
      "[7]\tvalidation_0-auc:0.56597\n",
      "[8]\tvalidation_0-auc:0.56597\n",
      "[9]\tvalidation_0-auc:0.56597\n",
      "[0]\tvalidation_0-auc:0.71429\n",
      "[1]\tvalidation_0-auc:0.72143\n",
      "[2]\tvalidation_0-auc:0.71000\n",
      "[3]\tvalidation_0-auc:0.70429\n",
      "[4]\tvalidation_0-auc:0.70429\n",
      "[5]\tvalidation_0-auc:0.70429\n",
      "[6]\tvalidation_0-auc:0.70429\n",
      "[7]\tvalidation_0-auc:0.70429\n",
      "[8]\tvalidation_0-auc:0.70429\n",
      "[9]\tvalidation_0-auc:0.70429\n",
      "[10]\tvalidation_0-auc:0.70429\n",
      "[11]\tvalidation_0-auc:0.70429\n",
      "[1]\tvalid_0's binary_logloss: 0.686294\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.673861\n",
      "[3]\tvalid_0's binary_logloss: 0.664223\n",
      "[4]\tvalid_0's binary_logloss: 0.657646\n",
      "[5]\tvalid_0's binary_logloss: 0.655938\n",
      "[6]\tvalid_0's binary_logloss: 0.654602\n",
      "[7]\tvalid_0's binary_logloss: 0.65703\n",
      "[8]\tvalid_0's binary_logloss: 0.658595\n",
      "[9]\tvalid_0's binary_logloss: 0.663487\n",
      "[10]\tvalid_0's binary_logloss: 0.667335\n",
      "[11]\tvalid_0's binary_logloss: 0.667335\n",
      "[12]\tvalid_0's binary_logloss: 0.667335\n",
      "[13]\tvalid_0's binary_logloss: 0.667335\n",
      "[14]\tvalid_0's binary_logloss: 0.667335\n",
      "[15]\tvalid_0's binary_logloss: 0.667335\n",
      "[16]\tvalid_0's binary_logloss: 0.667335\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.654602\n",
      "[1]\tvalid_0's binary_logloss: 0.540998\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.530399\n",
      "[3]\tvalid_0's binary_logloss: 0.517804\n",
      "[4]\tvalid_0's binary_logloss: 0.507342\n",
      "[5]\tvalid_0's binary_logloss: 0.491844\n",
      "[6]\tvalid_0's binary_logloss: 0.487342\n",
      "[7]\tvalid_0's binary_logloss: 0.480558\n",
      "[8]\tvalid_0's binary_logloss: 0.469684\n",
      "[9]\tvalid_0's binary_logloss: 0.463736\n",
      "[10]\tvalid_0's binary_logloss: 0.461882\n",
      "[11]\tvalid_0's binary_logloss: 0.461882\n",
      "[12]\tvalid_0's binary_logloss: 0.461882\n",
      "[13]\tvalid_0's binary_logloss: 0.461882\n",
      "[14]\tvalid_0's binary_logloss: 0.461882\n",
      "[15]\tvalid_0's binary_logloss: 0.461882\n",
      "[16]\tvalid_0's binary_logloss: 0.461882\n",
      "[17]\tvalid_0's binary_logloss: 0.461882\n",
      "[18]\tvalid_0's binary_logloss: 0.461882\n",
      "[19]\tvalid_0's binary_logloss: 0.461882\n",
      "[20]\tvalid_0's binary_logloss: 0.461882\n",
      "[21]\tvalid_0's binary_logloss: 0.461882\n",
      "[22]\tvalid_0's binary_logloss: 0.461882\n",
      "[23]\tvalid_0's binary_logloss: 0.461882\n",
      "[24]\tvalid_0's binary_logloss: 0.461882\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.461882\n",
      "[1]\tvalid_0's binary_logloss: 0.616359\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.584174\n",
      "[3]\tvalid_0's binary_logloss: 0.557037\n",
      "[4]\tvalid_0's binary_logloss: 0.533945\n",
      "[5]\tvalid_0's binary_logloss: 0.517011\n",
      "[6]\tvalid_0's binary_logloss: 0.500183\n",
      "[7]\tvalid_0's binary_logloss: 0.488461\n",
      "[8]\tvalid_0's binary_logloss: 0.477253\n",
      "[9]\tvalid_0's binary_logloss: 0.468466\n",
      "[10]\tvalid_0's binary_logloss: 0.457841\n",
      "[11]\tvalid_0's binary_logloss: 0.457841\n",
      "[12]\tvalid_0's binary_logloss: 0.457841\n",
      "[13]\tvalid_0's binary_logloss: 0.457841\n",
      "[14]\tvalid_0's binary_logloss: 0.457841\n",
      "[15]\tvalid_0's binary_logloss: 0.457841\n",
      "[16]\tvalid_0's binary_logloss: 0.457841\n",
      "[17]\tvalid_0's binary_logloss: 0.457841\n",
      "[18]\tvalid_0's binary_logloss: 0.457841\n",
      "[19]\tvalid_0's binary_logloss: 0.457841\n",
      "[20]\tvalid_0's binary_logloss: 0.457841\n",
      "[21]\tvalid_0's binary_logloss: 0.457841\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.457841\n",
      "[1]\tvalid_0's binary_logloss: 0.662147\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.643853\n",
      "[3]\tvalid_0's binary_logloss: 0.629689\n",
      "[4]\tvalid_0's binary_logloss: 0.618726\n",
      "[5]\tvalid_0's binary_logloss: 0.610287\n",
      "[6]\tvalid_0's binary_logloss: 0.603862\n",
      "[7]\tvalid_0's binary_logloss: 0.599155\n",
      "[8]\tvalid_0's binary_logloss: 0.595724\n",
      "[9]\tvalid_0's binary_logloss: 0.595271\n",
      "[10]\tvalid_0's binary_logloss: 0.593618\n",
      "[11]\tvalid_0's binary_logloss: 0.593618\n",
      "[12]\tvalid_0's binary_logloss: 0.593618\n",
      "[13]\tvalid_0's binary_logloss: 0.593618\n",
      "[14]\tvalid_0's binary_logloss: 0.593618\n",
      "[15]\tvalid_0's binary_logloss: 0.593618\n",
      "[16]\tvalid_0's binary_logloss: 0.593618\n",
      "[17]\tvalid_0's binary_logloss: 0.593618\n",
      "[18]\tvalid_0's binary_logloss: 0.593618\n",
      "[19]\tvalid_0's binary_logloss: 0.593618\n",
      "[20]\tvalid_0's binary_logloss: 0.593618\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.593618\n",
      "[1]\tvalid_0's binary_logloss: 0.628601\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.607729\n",
      "[3]\tvalid_0's binary_logloss: 0.593433\n",
      "[4]\tvalid_0's binary_logloss: 0.579753\n",
      "[5]\tvalid_0's binary_logloss: 0.570748\n",
      "[6]\tvalid_0's binary_logloss: 0.561546\n",
      "[7]\tvalid_0's binary_logloss: 0.552682\n",
      "[8]\tvalid_0's binary_logloss: 0.548416\n",
      "[9]\tvalid_0's binary_logloss: 0.542162\n",
      "[10]\tvalid_0's binary_logloss: 0.549153\n",
      "[11]\tvalid_0's binary_logloss: 0.549153\n",
      "[12]\tvalid_0's binary_logloss: 0.549153\n",
      "[13]\tvalid_0's binary_logloss: 0.549153\n",
      "[14]\tvalid_0's binary_logloss: 0.549153\n",
      "[15]\tvalid_0's binary_logloss: 0.549153\n",
      "[16]\tvalid_0's binary_logloss: 0.549153\n",
      "[17]\tvalid_0's binary_logloss: 0.549153\n",
      "[18]\tvalid_0's binary_logloss: 0.549153\n",
      "[19]\tvalid_0's binary_logloss: 0.549153\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.542162\n",
      "[1]\tvalid_0's binary_logloss: 0.628133\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.60981\n",
      "[3]\tvalid_0's binary_logloss: 0.595633\n",
      "[4]\tvalid_0's binary_logloss: 0.582404\n",
      "[5]\tvalid_0's binary_logloss: 0.572513\n",
      "[6]\tvalid_0's binary_logloss: 0.565611\n",
      "[7]\tvalid_0's binary_logloss: 0.560827\n",
      "[8]\tvalid_0's binary_logloss: 0.559934\n",
      "[9]\tvalid_0's binary_logloss: 0.556517\n",
      "[10]\tvalid_0's binary_logloss: 0.557592\n",
      "[11]\tvalid_0's binary_logloss: 0.557592\n",
      "[12]\tvalid_0's binary_logloss: 0.557592\n",
      "[13]\tvalid_0's binary_logloss: 0.557592\n",
      "[14]\tvalid_0's binary_logloss: 0.557592\n",
      "[15]\tvalid_0's binary_logloss: 0.557592\n",
      "[16]\tvalid_0's binary_logloss: 0.557592\n",
      "[17]\tvalid_0's binary_logloss: 0.557592\n",
      "[18]\tvalid_0's binary_logloss: 0.557592\n",
      "[19]\tvalid_0's binary_logloss: 0.557592\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.556517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.19883939,\n",
       "        0.80116061],\n",
       "       [0.54347826, 0.45652174, 0.50348496, 0.49651507, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.54347826, 0.45652174, 0.50399303, 0.49600694, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.54347826, 0.45652174, 0.49909711, 0.50090289, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.17161421,\n",
       "        0.82838579],\n",
       "       [0.54347826, 0.45652174, 0.50348496, 0.49651507, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.18468661,\n",
       "        0.81531339],\n",
       "       [0.54347826, 0.45652174, 0.49834168, 0.50165832, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.17161421,\n",
       "        0.82838579],\n",
       "       [0.54347826, 0.45652174, 0.49909711, 0.50090289, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.54347826, 0.45652174, 0.50498319, 0.49501678, 0.40412804,\n",
       "        0.59587196],\n",
       "       [1.        , 0.        , 0.49888629, 0.50111371, 0.40412804,\n",
       "        0.59587196],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.18468661,\n",
       "        0.81531339],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.19883939,\n",
       "        0.80116061],\n",
       "       [0.54347826, 0.45652174, 0.49788499, 0.50211501, 0.47315669,\n",
       "        0.52684331],\n",
       "       [0.54347826, 0.45652174, 0.50548482, 0.49451521, 0.40412804,\n",
       "        0.59587196],\n",
       "       [0.54347826, 0.45652174, 0.50399303, 0.49600694, 0.47315669,\n",
       "        0.52684331],\n",
       "       [1.        , 0.        , 0.49452913, 0.50547087, 0.19883939,\n",
       "        0.80116061],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.18468661,\n",
       "        0.81531339],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.21345125,\n",
       "        0.78654875],\n",
       "       [0.02040816, 0.97959184, 0.49452913, 0.50547087, 0.17161421,\n",
       "        0.82838579],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.21345125,\n",
       "        0.78654875],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.17161421,\n",
       "        0.82838579],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.21345125,\n",
       "        0.78654875],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.17161421,\n",
       "        0.82838579],\n",
       "       [0.02040816, 0.97959184, 0.49137974, 0.50862026, 0.21345125,\n",
       "        0.78654875],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [0.55769231, 0.44230769, 0.51592851, 0.48407146, 0.57133885,\n",
       "        0.42866115],\n",
       "       [0.55769231, 0.44230769, 0.5       , 0.5       , 0.4400992 ,\n",
       "        0.5599008 ],\n",
       "       [0.55769231, 0.44230769, 0.49344093, 0.50655907, 0.4644395 ,\n",
       "        0.5355605 ],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.16937155,\n",
       "        0.83062845],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.16937155,\n",
       "        0.83062845],\n",
       "       [0.55769231, 0.44230769, 0.51592851, 0.48407146, 0.51988285,\n",
       "        0.48011715],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [0.07142857, 0.92857143, 0.5       , 0.5       , 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.16937155,\n",
       "        0.83062845],\n",
       "       [1.        , 0.        , 0.49561238, 0.50438762, 0.43310058,\n",
       "        0.56689942],\n",
       "       [0.55769231, 0.44230769, 0.5       , 0.5       , 0.4400992 ,\n",
       "        0.5599008 ],\n",
       "       [1.        , 0.        , 0.49561238, 0.50438762, 0.48821318,\n",
       "        0.51178682],\n",
       "       [1.        , 0.        , 0.50431514, 0.49568486, 0.48821318,\n",
       "        0.51178682],\n",
       "       [1.        , 0.        , 0.49561238, 0.50438762, 0.44198678,\n",
       "        0.55801322],\n",
       "       [0.55769231, 0.44230769, 0.51217091, 0.48782906, 0.53396906,\n",
       "        0.46603094],\n",
       "       [0.55769231, 0.44230769, 0.51592851, 0.48407146, 0.525995  ,\n",
       "        0.474005  ],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.16937155,\n",
       "        0.83062845],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [0.55769231, 0.44230769, 0.49344093, 0.50655907, 0.48821318,\n",
       "        0.51178682],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.234739  ,\n",
       "        0.765261  ],\n",
       "       [1.        , 0.        , 0.49561238, 0.50438762, 0.48821318,\n",
       "        0.51178682],\n",
       "       [0.55769231, 0.44230769, 0.51592851, 0.48407146, 0.58015872,\n",
       "        0.41984128],\n",
       "       [0.07142857, 0.92857143, 0.48002058, 0.51997942, 0.16937155,\n",
       "        0.83062845],\n",
       "       [0.59574468, 0.40425532, 0.49909377, 0.50090623, 0.41679945,\n",
       "        0.58320055],\n",
       "       [0.59574468, 0.40425532, 0.50453091, 0.49546909, 0.45018464,\n",
       "        0.54981536],\n",
       "       [0.59574468, 0.40425532, 0.49584174, 0.50415826, 0.55115927,\n",
       "        0.44884073],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.24158013,\n",
       "        0.75841987],\n",
       "       [0.59574468, 0.40425532, 0.49834168, 0.50165832, 0.55115927,\n",
       "        0.44884073],\n",
       "       [0.59574468, 0.40425532, 0.5       , 0.5       , 0.51733168,\n",
       "        0.48266832],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.232327  ,\n",
       "        0.767673  ],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.17594718,\n",
       "        0.82405282],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.232327  ,\n",
       "        0.767673  ],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.17594718,\n",
       "        0.82405282],\n",
       "       [1.        , 0.        , 0.49146789, 0.50853211, 0.29156856,\n",
       "        0.70843144],\n",
       "       [0.        , 1.        , 0.50453091, 0.49546909, 0.55115927,\n",
       "        0.44884073],\n",
       "       [0.08      , 0.92      , 0.49834168, 0.50165832, 0.15302216,\n",
       "        0.84697784],\n",
       "       [0.        , 1.        , 0.50453091, 0.49546909, 0.45018464,\n",
       "        0.54981536],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.232327  ,\n",
       "        0.767673  ],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.15302216,\n",
       "        0.84697784],\n",
       "       [0.59574468, 0.40425532, 0.5       , 0.5       , 0.51733168,\n",
       "        0.48266832],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.17594718,\n",
       "        0.82405282],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.17594718,\n",
       "        0.82405282],\n",
       "       [0.08      , 0.92      , 0.49834168, 0.50165832, 0.15302216,\n",
       "        0.84697784],\n",
       "       [0.59574468, 0.40425532, 0.49601209, 0.50398791, 0.41679945,\n",
       "        0.58320055],\n",
       "       [0.59574468, 0.40425532, 0.49601209, 0.50398791, 0.41679945,\n",
       "        0.58320055],\n",
       "       [0.08      , 0.92      , 0.49146789, 0.50853211, 0.15302216,\n",
       "        0.84697784],\n",
       "       [0.59574468, 0.40425532, 0.50453091, 0.49546909, 0.55115927,\n",
       "        0.44884073],\n",
       "       [0.59574468, 0.40425532, 0.50622636, 0.49377364, 0.55115927,\n",
       "        0.44884073],\n",
       "       [0.65116279, 0.34883721, 0.49916065, 0.50083935, 0.43738339,\n",
       "        0.56261661],\n",
       "       [0.65116279, 0.34883721, 0.49946284, 0.50053716, 0.45944449,\n",
       "        0.54055551],\n",
       "       [0.65116279, 0.34883721, 0.50110805, 0.49889198, 0.55742278,\n",
       "        0.44257722],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.20270106,\n",
       "        0.79729894],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.14199107,\n",
       "        0.85800893],\n",
       "       [0.65116279, 0.34883721, 0.50110805, 0.49889198, 0.55742278,\n",
       "        0.44257722],\n",
       "       [0.05660377, 0.94339623, 0.49523669, 0.50476331, 0.14199107,\n",
       "        0.85800893],\n",
       "       [0.65116279, 0.34883721, 0.50214136, 0.49785861, 0.45944449,\n",
       "        0.54055551],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.13622368,\n",
       "        0.86377632],\n",
       "       [0.2       , 0.8       , 0.49667597, 0.50332403, 0.43738339,\n",
       "        0.56261661],\n",
       "       [0.65116279, 0.34883721, 0.49916065, 0.50083935, 0.43738339,\n",
       "        0.56261661],\n",
       "       [0.65116279, 0.34883721, 0.49495447, 0.50504553, 0.43738339,\n",
       "        0.56261661],\n",
       "       [0.65116279, 0.34883721, 0.50576645, 0.49423355, 0.5793043 ,\n",
       "        0.4206957 ],\n",
       "       [1.        , 0.        , 0.49820065, 0.50179935, 0.20270106,\n",
       "        0.79729894],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.14199107,\n",
       "        0.85800893],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.14199107,\n",
       "        0.85800893],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.13622368,\n",
       "        0.86377632],\n",
       "       [0.2       , 0.8       , 0.49667597, 0.50332403, 0.55742278,\n",
       "        0.44257722],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.13622368,\n",
       "        0.86377632],\n",
       "       [0.2       , 0.8       , 0.49667597, 0.50332403, 0.43738339,\n",
       "        0.56261661],\n",
       "       [0.65116279, 0.34883721, 0.49946284, 0.50053716, 0.45944449,\n",
       "        0.54055551],\n",
       "       [0.05660377, 0.94339623, 0.49140519, 0.50859481, 0.14199107,\n",
       "        0.85800893],\n",
       "       [0.65116279, 0.34883721, 0.50576645, 0.49423355, 0.5793043 ,\n",
       "        0.4206957 ],\n",
       "       [0.65116279, 0.34883721, 0.50214136, 0.49785861, 0.45944449,\n",
       "        0.54055551],\n",
       "       [0.05660377, 0.94339623, 0.49523669, 0.50476331, 0.14199107,\n",
       "        0.85800893],\n",
       "       [0.54054054, 0.45945946, 0.4975    , 0.5025    , 0.47322383,\n",
       "        0.52677617],\n",
       "       [0.75      , 0.25      , 0.50125003, 0.49875   , 0.22748188,\n",
       "        0.77251812],\n",
       "       [0.88888889, 0.11111111, 0.50055552, 0.49944445, 0.57868881,\n",
       "        0.42131119],\n",
       "       [0.54054054, 0.45945946, 0.4975    , 0.5025    , 0.47322383,\n",
       "        0.52677617],\n",
       "       [0.54054054, 0.45945946, 0.4975    , 0.5025    , 0.47322383,\n",
       "        0.52677617],\n",
       "       [0.54054054, 0.45945946, 0.49722224, 0.50277776, 0.47322383,\n",
       "        0.52677617],\n",
       "       [0.88888889, 0.11111111, 0.50346148, 0.49653852, 0.530518  ,\n",
       "        0.469482  ],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.22748188,\n",
       "        0.77251812],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.16606643,\n",
       "        0.83393357],\n",
       "       [0.54054054, 0.45945946, 0.49722224, 0.50277776, 0.42497664,\n",
       "        0.57502336],\n",
       "       [0.54054054, 0.45945946, 0.49722224, 0.50277776, 0.530518  ,\n",
       "        0.469482  ],\n",
       "       [0.54054054, 0.45945946, 0.50055552, 0.49944445, 0.530518  ,\n",
       "        0.469482  ],\n",
       "       [0.75      , 0.25      , 0.50125003, 0.49875   , 0.22748188,\n",
       "        0.77251812],\n",
       "       [0.88888889, 0.11111111, 0.50346148, 0.49653852, 0.57868881,\n",
       "        0.42131119],\n",
       "       [0.54054054, 0.45945946, 0.5       , 0.5       , 0.42497664,\n",
       "        0.57502336],\n",
       "       [0.54054054, 0.45945946, 0.4975    , 0.5025    , 0.42497664,\n",
       "        0.57502336],\n",
       "       [0.88888889, 0.11111111, 0.50055552, 0.49944445, 0.57868881,\n",
       "        0.42131119],\n",
       "       [0.54054054, 0.45945946, 0.5       , 0.5       , 0.47322383,\n",
       "        0.52677617],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.2468515 ,\n",
       "        0.7531485 ],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.2468515 ,\n",
       "        0.7531485 ],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.2468515 ,\n",
       "        0.7531485 ],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.16606643,\n",
       "        0.83393357],\n",
       "       [0.54054054, 0.45945946, 0.5       , 0.5       , 0.47322383,\n",
       "        0.52677617],\n",
       "       [0.05769231, 0.94230769, 0.49589294, 0.50410706, 0.18143483,\n",
       "        0.81856517],\n",
       "       [0.54054054, 0.45945946, 0.50346148, 0.49653852, 0.530518  ,\n",
       "        0.469482  ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.88888889, 0.11111111],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.88888889, 0.11111111],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.54716981, 0.45283019],\n",
      "       [0.06349206, 0.93650794],\n",
      "       [0.06349206, 0.93650794]]), array([[0.5059818 , 0.4940182 ],\n",
      "       [0.5063431 , 0.49365684],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.50191164, 0.49808836],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49401826, 0.50598174],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5009062 , 0.4990938 ],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.49401826, 0.50598174],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5028194 , 0.49718058],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5063431 , 0.49365684],\n",
      "       [0.5016073 , 0.4983927 ],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49999803, 0.50000197],\n",
      "       [0.49999803, 0.50000197],\n",
      "       [0.49999803, 0.50000197],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5016073 , 0.4983927 ],\n",
      "       [0.5009062 , 0.4990938 ],\n",
      "       [0.49999803, 0.50000197],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5016073 , 0.4983927 ],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.5009062 , 0.4990938 ],\n",
      "       [0.5009062 , 0.4990938 ],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.5059818 , 0.4940182 ],\n",
      "       [0.49177736, 0.50822264],\n",
      "       [0.49177736, 0.50822264]], dtype=float32), array([[0.48366647, 0.51633353],\n",
      "       [0.49568868, 0.50431132],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15234307, 0.84765693],\n",
      "       [0.45155556, 0.54844444],\n",
      "       [0.45143174, 0.54856826],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.24834318, 0.75165682],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.45155556, 0.54844444],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.45155556, 0.54844444],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.44126445, 0.55873555],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.52791705, 0.47208295],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.24834318, 0.75165682],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.48354161, 0.51645839],\n",
      "       [0.49568868, 0.50431132],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.52791705, 0.47208295],\n",
      "       [0.52791705, 0.47208295],\n",
      "       [0.52791705, 0.47208295],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15357602, 0.84642398],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.49568868, 0.50431132],\n",
      "       [0.45155556, 0.54844444],\n",
      "       [0.52791705, 0.47208295],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.45155556, 0.54844444],\n",
      "       [0.24931336, 0.75068664],\n",
      "       [0.49568868, 0.50431132],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.15234307, 0.84765693],\n",
      "       [0.48366647, 0.51633353],\n",
      "       [0.15290251, 0.84709749],\n",
      "       [0.15290251, 0.84709749]])]\n",
      "[[0.54716981 0.45283019 0.5059818  0.4940182  0.48366647 0.51633353]\n",
      " [0.54716981 0.45283019 0.50634313 0.49365684 0.49568868 0.50431132]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15234307 0.84765693]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.45155556 0.54844444]\n",
      " [0.88888889 0.11111111 0.50191164 0.49808836 0.45143174 0.54856826]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.48366647 0.51633353]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.24834318 0.75165682]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.88888889 0.11111111 0.49401826 0.50598174 0.45155556 0.54844444]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.54716981 0.45283019 0.50090623 0.4990938  0.45155556 0.54844444]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.48366647 0.51633353]\n",
      " [0.54716981 0.45283019 0.49401826 0.50598174 0.44126445 0.55873555]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.54716981 0.45283019 0.50281942 0.49718058 0.52791705 0.47208295]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.24834318 0.75165682]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.54716981 0.45283019 0.50634313 0.49365684 0.48354161 0.51645839]\n",
      " [0.54716981 0.45283019 0.5016073  0.4983927  0.49568868 0.50431132]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.54716981 0.45283019 0.49999803 0.50000197 0.52791705 0.47208295]\n",
      " [0.54716981 0.45283019 0.49999803 0.50000197 0.52791705 0.47208295]\n",
      " [0.54716981 0.45283019 0.49999803 0.50000197 0.52791705 0.47208295]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15357602 0.84642398]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.54716981 0.45283019 0.5016073  0.4983927  0.49568868 0.50431132]\n",
      " [0.54716981 0.45283019 0.50090623 0.4990938  0.45155556 0.54844444]\n",
      " [0.54716981 0.45283019 0.49999803 0.50000197 0.52791705 0.47208295]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.45155556 0.54844444]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.24931336 0.75068664]\n",
      " [0.54716981 0.45283019 0.5016073  0.4983927  0.49568868 0.50431132]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.48366647 0.51633353]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.48366647 0.51633353]\n",
      " [0.54716981 0.45283019 0.50090623 0.4990938  0.48366647 0.51633353]\n",
      " [0.54716981 0.45283019 0.50090623 0.4990938  0.48366647 0.51633353]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15234307 0.84765693]\n",
      " [0.54716981 0.45283019 0.5059818  0.4940182  0.48366647 0.51633353]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]\n",
      " [0.06349206 0.93650794 0.49177736 0.50822264 0.15290251 0.84709749]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3843401 , 0.6156599 ],\n",
       "       [0.52687786, 0.47312214],\n",
       "       [0.10489626, 0.89510374],\n",
       "       [0.11116677, 0.88883323],\n",
       "       [0.4437343 , 0.5562657 ],\n",
       "       [0.50809146, 0.49190854],\n",
       "       [0.25311693, 0.74688307],\n",
       "       [0.17218658, 0.82781342],\n",
       "       [0.095095  , 0.904905  ],\n",
       "       [0.07292245, 0.92707755],\n",
       "       [0.56234944, 0.43765056],\n",
       "       [0.13246107, 0.86753893],\n",
       "       [0.15124301, 0.84875699],\n",
       "       [0.07292245, 0.92707755],\n",
       "       [0.44183369, 0.55816631],\n",
       "       [0.38872737, 0.61127263],\n",
       "       [0.51353424, 0.48646576],\n",
       "       [0.19401893, 0.80598107],\n",
       "       [0.07417816, 0.92582184],\n",
       "       [0.72219955, 0.27780045],\n",
       "       [0.11909168, 0.88090832],\n",
       "       [0.2170787 , 0.7829213 ],\n",
       "       [0.07494327, 0.92505673],\n",
       "       [0.07475773, 0.92524227],\n",
       "       [0.15228487, 0.84771513],\n",
       "       [0.53633805, 0.46366195],\n",
       "       [0.61844972, 0.38155028],\n",
       "       [0.20914582, 0.79085418],\n",
       "       [0.59207036, 0.40792964],\n",
       "       [0.49626935, 0.50373065],\n",
       "       [0.48358784, 0.51641216],\n",
       "       [0.16293382, 0.83706618],\n",
       "       [0.06491681, 0.93508319],\n",
       "       [0.14152987, 0.85847013],\n",
       "       [0.15731234, 0.84268766],\n",
       "       [0.15239631, 0.84760369],\n",
       "       [0.10171042, 0.89828958],\n",
       "       [0.07568199, 0.92431801],\n",
       "       [0.62323276, 0.37676724],\n",
       "       [0.48844686, 0.51155314],\n",
       "       [0.50448569, 0.49551431],\n",
       "       [0.11066645, 0.88933355],\n",
       "       [0.39718135, 0.60281865],\n",
       "       [0.24846707, 0.75153293],\n",
       "       [0.59042712, 0.40957288],\n",
       "       [0.05052796, 0.94947204],\n",
       "       [0.06759046, 0.93240954],\n",
       "       [0.57553008, 0.42446992],\n",
       "       [0.38456301, 0.61543699],\n",
       "       [0.44556469, 0.55443531],\n",
       "       [0.45069775, 0.54930225],\n",
       "       [0.10372536, 0.89627464],\n",
       "       [0.54051977, 0.45948023],\n",
       "       [0.05942591, 0.94057409],\n",
       "       [0.10936347, 0.89063653]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94     1\n",
       "18     0\n",
       "33     1\n",
       "98     1\n",
       "181    1\n",
       "168    0\n",
       "7      0\n",
       "138    1\n",
       "61     1\n",
       "74     1\n",
       "5      1\n",
       "112    1\n",
       "177    1\n",
       "130    1\n",
       "164    0\n",
       "150    1\n",
       "118    0\n",
       "106    1\n",
       "80     1\n",
       "155    0\n",
       "71     1\n",
       "55     1\n",
       "37     1\n",
       "145    1\n",
       "4      1\n",
       "66     0\n",
       "90     0\n",
       "179    1\n",
       "45     0\n",
       "89     1\n",
       "110    0\n",
       "63     1\n",
       "26     1\n",
       "60     1\n",
       "170    1\n",
       "159    1\n",
       "8      1\n",
       "44     1\n",
       "96     0\n",
       "129    1\n",
       "83     0\n",
       "54     1\n",
       "24     0\n",
       "30     1\n",
       "97     0\n",
       "56     1\n",
       "123    1\n",
       "111    1\n",
       "19     0\n",
       "139    1\n",
       "135    0\n",
       "160    1\n",
       "16     1\n",
       "51     1\n",
       "162    1\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9102564102564104"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1., 42., ...,  3.,  2.,  0.],\n",
       "       [ 1.,  1., 44., ...,  2.,  1.,  0.],\n",
       "       [ 1.,  0., 22., ...,  1.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  0., 39., ...,  2.,  0.,  0.],\n",
       "       [ 1.,  0., 58., ...,  2.,  2.,  0.],\n",
       "       [ 1.,  0., 39., ...,  4.,  0.,  0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((X_train.shape[0],4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
