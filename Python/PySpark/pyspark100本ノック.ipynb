{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkでデータ前処理100本ノックをやってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 参考\n",
    "    * [PySparkでCSVファイルを読み込む方法](https://docs.kanaries.net/ja/articles/read-csv-dataframe)\n",
    "    * [PySparkでparquetファイルを読み込む方法](https://data-analysis-stats.jp/spark/pyspark%E3%81%A7%E3%83%87%E3%83%BC%E3%82%BF%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF/)\n",
    "    * [PolarsでCSVファイルをParquetに変換する方法](https://medium.com/@umesh.nagar92x/efficient-conversion-of-massive-csv-files-to-parquet-format-using-pandas-dask-duck-db-and-polars-5c998c47b43d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession。pythonからsparkを使う場合、セッションの作成が必要。\n",
    "spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n",
    "\n",
    "# デフォルトのログレベルだと大量にログが出力されるので限定する。\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarsを用いて入力ファイル(csv)をParquetに変換する\n",
    "PySparkはcsvを読むのが苦手なので、Parquetファイルに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../100knocks-preprocess/docker/work/data/store.csv',\n",
       " '../../../100knocks-preprocess/docker/work/data/receipt.csv',\n",
       " '../../../100knocks-preprocess/docker/work/data/category.csv',\n",
       " '../../../100knocks-preprocess/docker/work/data/product.csv',\n",
       " '../../../100knocks-preprocess/docker/work/data/customer.csv',\n",
       " '../../../100knocks-preprocess/docker/work/data/geocode.csv']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = glob('../../../100knocks-preprocess/docker/work/data/*.csv')\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'customer_id': str,\n",
    "    'gender_cd': str,\n",
    "    'postal_cd': str,\n",
    "    'application_store_cd': str,\n",
    "    'status_cd': str,\n",
    "    'category_major_cd': str,\n",
    "    'category_medium_cd': str,\n",
    "    'category_small_cd': str,\n",
    "    'product_cd': str,\n",
    "    'store_cd': str,\n",
    "    'prefecture_cd': str,\n",
    "    'tel_no': str,\n",
    "    'postal_cd': str,\n",
    "    'street': str,\n",
    "    'application_date': str,\n",
    "    'birth_day': pl.Date\n",
    "}\n",
    "\n",
    "# それぞれcsvを読み込んでからparquetとして出力\n",
    "for file in csv_files:\n",
    "    df = pl.read_csv(file, dtypes=dtypes)\n",
    "\n",
    "    # fileの絶対パスの拡張子をparquetにして出力\n",
    "    output_path = file.split(\".csv\")[0] + \".parquet\"\n",
    "    df.write_parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySparkでParquetファイルを読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receipt = (\n",
    "    spark.read\n",
    "    .parquet(\"../../../100knocks-preprocess/docker/work/data/receipt.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sales_ymd', 'bigint'),\n",
       " ('sales_epoch', 'bigint'),\n",
       " ('store_cd', 'string'),\n",
       " ('receipt_no', 'bigint'),\n",
       " ('receipt_sub_no', 'bigint'),\n",
       " ('customer_id', 'string'),\n",
       " ('product_cd', 'string'),\n",
       " ('quantity', 'bigint'),\n",
       " ('amount', 'bigint')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_receipt.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100本ノック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> P-001: レシート明細データ（df_receipt）から全項目の先頭10件を表示し、どのようなデータを保有しているか目視で確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+----------+--------------+--------------+----------+--------+------+\n",
      "|sales_ymd|sales_epoch|store_cd|receipt_no|receipt_sub_no|   customer_id|product_cd|quantity|amount|\n",
      "+---------+-----------+--------+----------+--------------+--------------+----------+--------+------+\n",
      "| 20181103| 1541203200|  S14006|       112|             1|CS006214000001|P070305012|       1|   158|\n",
      "| 20181118| 1542499200|  S13008|      1132|             2|CS008415000097|P070701017|       1|    81|\n",
      "| 20170712| 1499817600|  S14028|      1102|             1|CS028414000014|P060101005|       1|   170|\n",
      "| 20190205| 1549324800|  S14042|      1132|             1|ZZ000000000000|P050301001|       1|    25|\n",
      "| 20180821| 1534809600|  S14025|      1102|             2|CS025415000050|P060102007|       1|    90|\n",
      "| 20190605| 1559692800|  S13003|      1112|             1|CS003515000195|P050102002|       1|   138|\n",
      "| 20181205| 1543968000|  S14024|      1102|             2|CS024514000042|P080101005|       1|    30|\n",
      "| 20190922| 1569110400|  S14040|      1102|             1|CS040415000178|P070501004|       1|   128|\n",
      "| 20170504| 1493856000|  S13020|      1112|             2|ZZ000000000000|P071302010|       1|   770|\n",
      "| 20191010| 1570665600|  S14027|      1102|             1|CS027514000015|P071101003|       1|   680|\n",
      "+---------+-----------+--------+----------+--------------+--------------+----------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_receipt.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> P-002: レシート明細データ（df_receipt）から売上年月日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、10件表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+------+\n",
      "|sales_ymd|   customer_id|product_cd|amount|\n",
      "+---------+--------------+----------+------+\n",
      "| 20181103|CS006214000001|P070305012|   158|\n",
      "| 20181118|CS008415000097|P070701017|    81|\n",
      "| 20170712|CS028414000014|P060101005|   170|\n",
      "| 20190205|ZZ000000000000|P050301001|    25|\n",
      "| 20180821|CS025415000050|P060102007|    90|\n",
      "| 20190605|CS003515000195|P050102002|   138|\n",
      "| 20181205|CS024514000042|P080101005|    30|\n",
      "| 20190922|CS040415000178|P070501004|   128|\n",
      "| 20170504|ZZ000000000000|P071302010|   770|\n",
      "| 20191010|CS027514000015|P071101003|   680|\n",
      "+---------+--------------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_receipt.select(\n",
    "    \"sales_ymd\",\n",
    "    \"customer_id\",\n",
    "    \"product_cd\",\n",
    "    \"amount\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> P-003: レシート明細データ（df_receipt）から売上年月日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、  \n",
    "> 10件表示せよ。ただし、sales_ymdをsales_dateに項目名を変更しながら抽出すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+------+\n",
      "|sales_date|   customer_id|product_cd|amount|\n",
      "+----------+--------------+----------+------+\n",
      "|  20181103|CS006214000001|P070305012|   158|\n",
      "|  20181118|CS008415000097|P070701017|    81|\n",
      "|  20170712|CS028414000014|P060101005|   170|\n",
      "|  20190205|ZZ000000000000|P050301001|    25|\n",
      "|  20180821|CS025415000050|P060102007|    90|\n",
      "|  20190605|CS003515000195|P050102002|   138|\n",
      "|  20181205|CS024514000042|P080101005|    30|\n",
      "|  20190922|CS040415000178|P070501004|   128|\n",
      "|  20170504|ZZ000000000000|P071302010|   770|\n",
      "|  20191010|CS027514000015|P071101003|   680|\n",
      "+----------+--------------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_receipt.select(\n",
    "    F.col(\"sales_ymd\").alias(\"sales_date\"),\n",
    "    \"customer_id\",\n",
    "    \"product_cd\",\n",
    "    \"amount\"\n",
    ").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
