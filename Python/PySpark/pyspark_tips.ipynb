{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[PySparkのTips](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [PySparkのTips](#toc1_)    \n",
    "- [特定カラムのユニークな値をリストとしてすべて取得](#toc2_)    \n",
    "- [joinの結合条件にcontainsを用いる](#toc3_)    \n",
    "- [Dataframeの任意の行番号範囲を取り出す](#toc4_)    \n",
    "- [pyspark.sql.functions.coalesceの挙動](#toc5_)    \n",
    "- [Nullの置換](#toc6_)    \n",
    "  - [pyspark.sql.DataFrame.fillnaによる置換](#toc6_1_)    \n",
    "  - [coalesceによる置換](#toc6_2_)    \n",
    "- [FULL OUTER JOIN(外部結合の注意点)](#toc7_)    \n",
    "- [主キー候補を機械的に抽出する](#toc8_)    \n",
    "  - [考え方](#toc8_1_)    \n",
    "- [時間帯の重複箇所を識別](#toc9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "細かいTips、テクニックをまとめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/01 11:11:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Create a SparkSession。pythonからsparkを使う場合、セッションの作成が必要。\n",
    "spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n",
    "\n",
    "# デフォルトのログレベルだと大量にログが出力されるので限定する。\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 各データ読み込み\n",
    "df_receipt = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/receipt.parquet\")\n",
    "\n",
    "# 店舗データ\n",
    "df_store = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/store.parquet\")\n",
    "\n",
    "# 顧客データ\n",
    "df_customer = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/customer.parquet\")\n",
    "\n",
    "# 製品データ\n",
    "df_product = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/product.parquet\")\n",
    "\n",
    "# 製品データ\n",
    "df_category = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/category.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[特定カラムのユニークな値をリストとしてすべて取得](#toc0_)\n",
    "下記のような方法がある。どちらにせよめんどくさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '9', '1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collectの結果の各値は対象カラムをキーとする辞書のような形で取得できる\n",
    "[v[\"gender_cd\"] for v in df_customer.select(\"gender_cd\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '9', '1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.select(\"gender_cd\").dropDuplicates().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|gender_cd|\n",
      "+---------+\n",
      "|        0|\n",
      "|        9|\n",
      "|        1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframeとして取得したいなら.distinctでOK\n",
    "df_customer.select(\"gender_cd\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[joinの結合条件にcontainsを用いる](#toc0_)\n",
    "結構便利。left joinで左のカラムに右のカラムの値が含まれている行に対して結合したい場合などに使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの作成\n",
    "data_a = [\n",
    "    (1, \"This is a sample message\"),\n",
    "    (2, \"Another example message\"),\n",
    "    (3, \"Message with a keyword\"),\n",
    "    (4, \"No match here\"),\n",
    "    (5, \"samplesample\"),\n",
    "    (6, \"sample keyword\"),\n",
    "]\n",
    "columns_a = [\"id\", \"message\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "data_b = [\n",
    "    (1, \"sample\", \"MSG001\"),\n",
    "    (2, \"example\", \"MSG002\"),\n",
    "    (3, \"keyword\", \"MSG003\")\n",
    "]\n",
    "columns_b = [\"id\" ,\"message_key\", \"MSG_No\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|             message|\n",
      "+---+--------------------+\n",
      "|  1|This is a sample ...|\n",
      "|  2|Another example m...|\n",
      "|  3|Message with a ke...|\n",
      "|  4|       No match here|\n",
      "|  5|        samplesample|\n",
      "|  6|      sample keyword|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+\n",
      "| id|message_key|MSG_No|\n",
      "+---+-----------+------+\n",
      "|  1|     sample|MSG001|\n",
      "|  2|    example|MSG002|\n",
      "|  3|    keyword|MSG003|\n",
      "+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----+-----------+------+\n",
      "| id|             message|  id|message_key|MSG_No|\n",
      "+---+--------------------+----+-----------+------+\n",
      "|  1|This is a sample ...|   1|     sample|MSG001|\n",
      "|  2|Another example m...|   2|    example|MSG002|\n",
      "|  3|Message with a ke...|   3|    keyword|MSG003|\n",
      "|  4|       No match here|NULL|       NULL|  NULL|\n",
      "|  5|        samplesample|   1|     sample|MSG001|\n",
      "|  6|      sample keyword|   1|     sample|MSG001|\n",
      "|  6|      sample keyword|   3|    keyword|MSG003|\n",
      "+---+--------------------+----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.join(\n",
    "    df_b,\n",
    "    # 結合条件にcontainsを使用。messageにmessage_keyが含まれていれば紐づける\n",
    "    F.contains(df_a.message, df_b.message_key),\n",
    "    \"left\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"sample keyword\"のように２つの単語に紐づく場合は2行紐づく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Dataframeの任意の行番号範囲を取り出す](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+\n",
      "|sales_month|total_amount|row_num|\n",
      "+-----------+------------+-------+\n",
      "|     201701|      902056|      0|\n",
      "|     201702|      764413|      1|\n",
      "|     201703|      962945|      2|\n",
      "|     201704|      847566|      3|\n",
      "|     201705|      884010|      4|\n",
      "|     201706|      894242|      5|\n",
      "|     201707|      959205|      6|\n",
      "|     201708|      954836|      7|\n",
      "|     201709|      902037|      8|\n",
      "|     201710|      905739|      9|\n",
      "|     201711|      932157|     10|\n",
      "|     201712|      939654|     11|\n",
      "|     201801|      944509|     12|\n",
      "|     201802|      864128|     13|\n",
      "|     201803|      946588|     14|\n",
      "|     201804|      937099|     15|\n",
      "|     201805|     1004438|     16|\n",
      "|     201806|     1012329|     17|\n",
      "+-----------+------------+-------+\n",
      "only showing top 18 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = df_receipt.withColumn(\n",
    "    \"sales_month\",\n",
    "    F.col(\"sales_ymd\").substr(0, 6) # yyyyMMdd -> yyyyMM形式にする\n",
    ").groupBy(\"sales_month\").agg(   # 月次ごとに集計\n",
    "    F.sum(\"amount\").alias(\"total_amount\")\n",
    ").withColumn(\n",
    "    \"row_num\",\n",
    "    F.row_number().over(Window.orderBy(\"sales_month\")) - 1\n",
    ")\n",
    "\n",
    "dataset.show(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※PySparkのDataframeの部分行だけ取り出すには上記のようWindow関数を使って行番号を振るしかなさそう。  \n",
    "  メモリに読み込まずに先頭から指定したレコード数だけ取り出す場合はlimitが使えるが、どこからどこまで取り出すという指定はできない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 12\n",
    "val_size = 6\n",
    "offset = 6 # 次のtrainをどこから始めるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# 12か月ごとに学習データ、6か月ごとに検証データを定義する\n",
    "for i in range(3):\n",
    "    train_start = offset * i\n",
    "    train_end = train_start + train_size - 1\n",
    "    val_start = train_end + 1\n",
    "    val_end = val_start + offset - 1\n",
    "    train_data.append(dataset.filter(F.col(\"row_num\").between(train_start, train_end)))\n",
    "    val_data.append(dataset.filter(F.col(\"row_num\").between(val_start, val_end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[pyspark.sql.functions.coalesceの挙動](#toc0_)\n",
    "名前から何をするのかわかりづらいので、挙動をおさらいしておく。  \n",
    "coalesceは”合体する”といった意味の英単語で、引数を左端から調べて最初に現れた非NULL値を返す関数である。  \n",
    "PySparkだけでなくSQLの文法にもある。  \n",
    "\n",
    "[pyspark.sql.functions.coalesceのドキュメント](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html?highlight=coale#pyspark.sql.functions.coalesce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|NULL|NULL|NULL|\n",
      "|   1|NULL|NULL|\n",
      "|NULL|   2|NULL|\n",
      "|NULL|NULL|   3|\n",
      "|NULL|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (None, None, None),\n",
    "    (1, None, None),\n",
    "    (None, 2, None),\n",
    "    (None, None, 3),\n",
    "    (None, 4, 5),\n",
    "], schema='a long, b long, c long')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          NULL|\n",
      "|             1|\n",
      "|             2|\n",
      "|          NULL|\n",
      "|             4|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2つのカラムをcoalesceする\n",
    "df.select(F.coalesce(\"a\", \"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|coalesce(a, b, c)|\n",
      "+-----------------+\n",
      "|             NULL|\n",
      "|                1|\n",
      "|                2|\n",
      "|                3|\n",
      "|                4|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3つのカラムをcoalesceする\n",
    "df.select(F.coalesce(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように各行を左のカラムから右のカラムへ（a->b）調べていき、  \n",
    "最初にヒットしたNullでない値を採用して一つのカラムを返す。  \n",
    "全カラムNullの場合は単にNullになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Nullの置換](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[pyspark.sql.DataFrame.fillnaによる置換](#toc0_)\n",
    "王道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|NULL|NULL|NULL|\n",
      "|   1|NULL|NULL|\n",
      "|NULL|   2|NULL|\n",
      "|NULL|NULL|   3|\n",
      "|NULL|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|   c|\n",
      "+---+---+----+\n",
      "|  0|999|NULL|\n",
      "|  1|999|NULL|\n",
      "|  0|  2|NULL|\n",
      "|  0|999|   3|\n",
      "|  0|  4|   5|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数列のNullを置換\n",
    "df.fillna({\"a\": 0, \"b\": 999}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[coalesceによる置換](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|NULL|NULL|NULL|\n",
      "|   1|NULL|NULL|\n",
      "|NULL|   2|NULL|\n",
      "|NULL|NULL|   3|\n",
      "|NULL|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = {}\n",
    "for col in df.columns:\n",
    "    expr[col] = F.coalesce(col, F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  0|  0|  0|\n",
      "|  1|  0|  0|\n",
      "|  0|  2|  0|\n",
      "|  0|  0|  3|\n",
      "|  0|  4|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumns(\n",
    "    expr\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0の定数カラムとcoalesceするところがポイント。  \n",
    "複数行にやろうとすると面倒。  \n",
    "他にもF.whenを使用する方法があるが、当然なので割愛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[FULL OUTER JOIN(外部結合の注意点)](#toc0_)\n",
    "PySparkに限った話ではないが、JOIN後に残すカラムは慎重に選ぶべき。  \n",
    "例えば、テーブルAの主キーカラムAとテーブルBの主キーカラムBについて、FULL JOINで  \n",
    "カラムA==カラムBを条件としてJoinする場合、  \n",
    "Join後に残す主キーをカラムAだけとした場合、カラムBにしかなかった値はNULLになってしまう。  \n",
    "カラムBにあった値もJOIN後の主キーに残すためには追加の処理が必要。  \n",
    "JOINの仕組みを考えれば当たり前だが、注意すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルAのサンプルデータの作成\n",
    "data_a = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\")\n",
    "]\n",
    "columns_a = [\"id_a\", \"name_a\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "# テーブルBのサンプルデータの作成\n",
    "data_b = [\n",
    "    (3, \"David\"),\n",
    "    (4, \"Eve\"),\n",
    "    (5, \"Frank\")\n",
    "]\n",
    "columns_b = [\"id_b\", \"name_b\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================>                   (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+\n",
      "|id_a| name_a|id_b|name_b|\n",
      "+----+-------+----+------+\n",
      "|   1|  Alice|NULL|  NULL|\n",
      "|   2|    Bob|NULL|  NULL|\n",
      "|   3|Charlie|   3| David|\n",
      "|NULL|   NULL|   4|   Eve|\n",
      "|NULL|   NULL|   5| Frank|\n",
      "+----+-------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 残すカラムを指定しない場合\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|id_a| name_a|name_b|\n",
      "+----+-------+------+\n",
      "|   1|  Alice|  NULL|\n",
      "|   2|    Bob|  NULL|\n",
      "|   3|Charlie| David|\n",
      "|NULL|   NULL|   Eve|\n",
      "|NULL|   NULL| Frank|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 主キーとしてid_aだけ残す\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").select(\n",
    "    \"id_a\", \"name_a\", \"name_b\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思いとして、id_bに相当するname_bはあるので、id_aにid_bの値も残っててくれたらありがたいが、  \n",
    "当然そうはならない。対処としては下記のようにid_a, id_bをcoalesceに指定して、NULLを埋めて結合する方法と、  \n",
    "whenを使用する方法がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id| name_a|name_b|\n",
      "+---+-------+------+\n",
      "|  1|  Alice|  NULL|\n",
      "|  2|    Bob|  NULL|\n",
      "|  3|Charlie| David|\n",
      "|  4|   NULL|   Eve|\n",
      "|  5|   NULL| Frank|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# coalesceを使う方法\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").withColumn(\n",
    "    \"id\",\n",
    "    F.coalesce(\"id_a\", \"id_b\")\n",
    ").select(\n",
    "    \"id\", \"name_a\", \"name_b\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===================>                                     (4 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id| name_a|name_b|\n",
      "+---+-------+------+\n",
      "|  1|  Alice|  NULL|\n",
      "|  2|    Bob|  NULL|\n",
      "|  3|Charlie| David|\n",
      "|  4|   NULL|   Eve|\n",
      "|  5|   NULL| Frank|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# whenを使う方法\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").withColumn(\n",
    "    \"id\",\n",
    "    F.when(F.col(\"id_a\").isNull(), F.col(\"id_b\")).otherwise(F.col(\"id_a\"))\n",
    ").select(\n",
    "    \"id\", \"name_a\", \"name_b\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[主キー候補を機械的に抽出する](#toc0_)\n",
    "数百のような大量のカラムを持つデータがあり、  \n",
    "かつ主キー（全行について一意かつNullの列）が不明といったダーティデータの主キー候補を機械的に抽出する方法を考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの作成\n",
    "data = [\n",
    "    (1, \"Alice\", \"C\", \"001\", 170, \"apple\", \"AAA\", \"BBB\", \"CCC\"),\n",
    "    (2, \"Bob\", \"A\", \"001\", 170, \"orange\", \"BBB\", \"CCC\", \"BBB\"),\n",
    "    (3, None, \"B\", \"001\", 156, \"orange\", \"CCC\", \"CCC\", \"BBB\"),\n",
    "    (None, \"Mike\", \"B\", \"002\", 160, \"orange\", \"AAA\", \"\", \"AAA\"),\n",
    "    (5, \"Alice\", \"A\", \"002\", 156, \"apple\", \"AAA\", \"AAA\", \"BBB\"),\n",
    "    (6, \"John\", \"A\", \"003\", 170, \"lemon\", \"CCC\", \"AAA\", \"CCC\"),\n",
    "\n",
    "]\n",
    "cols = [\"id\", \"name\", \"Class\", \"student_number\", \"height\", \"favorite_fruits\", \"col_A\", \"col_B\", \"col_C\"]\n",
    "df = spark.createDataFrame(data, cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[考え方](#toc0_)\n",
    "1. 文字列型のカラムのみを対象にする\n",
    "2. Nullを含まないカラムを対象とする\n",
    "3. 対象カラムをユニーク件数が多い順に並び替える\n",
    "4. 対象カラムから一つずつカラムを取り出し、ユニーク件数が行数と一致するか判定する\n",
    "5. 一致しない場合、別のカラムを一つ選び複合キーを作成し、新たな主キー候補とする\n",
    "6. 一致するまで1.~2.を繰り返す\n",
    "\n",
    "総当たりで複合キーを作成すると、カラム数が大量のときに計算量が膨大になるので避ける。  \n",
    "Nullを含まない、かつユニーク件数が多いカラムを軸にどんどんユニーク件数が多くなるように複合キーを作成していくイメージ。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主キーの候補になり得るのは基本的に文字列型のため、文字列型のカラムのみを対象とする。\n",
    "string_cols = [\n",
    "    field.name for field in df.schema.fields\n",
    "    if isinstance(field.dataType, (StringType))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'Class',\n",
       " 'student_number',\n",
       " 'favorite_fruits',\n",
       " 'col_A',\n",
       " 'col_B',\n",
       " 'col_C']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nullを含まない列の抽出\n",
    "cols_not_contain_nulls = []\n",
    "\n",
    "for col in string_cols:\n",
    "    if df.filter(F.col(col).isNull()).count() == 0:\n",
    "        cols_not_contain_nulls.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['col_B', 'Class', 'student_number', 'favorite_fruits', 'col_A', 'col_C']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全てのカラムのユニーク件数を算出\n",
    "unique_nums_dict = {}\n",
    "\n",
    "for col in cols_not_contain_nulls:\n",
    "    unique_nums_dict[col] = df.select(col).distinct().count()\n",
    "\n",
    "\n",
    "# 効率化のため、ユニーク件数が多い順にカラムを並べておく。\n",
    "target_cols = sorted(unique_nums_dict, key=unique_nums_dict.get, reverse=True)\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複合キーの最大サイズ（=複合キーに採用した最大カラム数）\n",
    "# max_composite_key_len = 3\n",
    "pk_candidate = []\n",
    "\n",
    "for col in target_cols:\n",
    "    if df.select(pk_candidate).distinct().count() == df.count():\n",
    "        break\n",
    "    else:\n",
    "        pk_candidate.append(col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['col_B', 'Class', 'student_number']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pk_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の処理で大雑把に主キー候補を抽出することは出来る。  \n",
    "ただし、col_Bのようなあまり意味がなさそうなカラムも複合キーに含まれ、  \n",
    "残りのClassとstudent_numberだけで複合キーとしては十分な場合も抽出されてしまう。  \n",
    "なので、実作業ではカラムの意味も踏まえつつ有用なカラムだけを残して複合キーを作成すること。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[時間帯の重複箇所を識別](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のような時間帯A～Dがあったとき、  \n",
    "重複している時間（重複開始時刻～重複終了時刻）、各重複に関わる時間帯、その時間帯総数を求めたい場合を考える。  \n",
    "* 時間帯A: 2024年12月29日 9:00-17:00  \n",
    "* 時間帯B: 2024年12月29日 12:00-23:00  \n",
    "* 時間帯C: 2024年12月29日 10:00-13:00  \n",
    "* 時間帯D: 2024年12月29日 12:30-16:00  \n",
    "\n",
    "ただし、各重複に関わる時間帯の総数が最も小さくなるように重複時間を分けることとする。  \n",
    "例えば、時間帯Aと時間帯Cは10:00-13:00で重複するが、12:00から時間帯Bとも重複し始める。  \n",
    "この場合、時間帯Aと時間帯Cは10:00-12:00で重複しているものとし、重複している時間帯の総数は2として数える。  \n",
    "12:00-12:30では時間帯A,B,Cが重複しており、重複している時間帯の総数は3となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+\n",
      "|時間帯|           開始時刻|           終了時刻|\n",
      "+------+-------------------+-------------------+\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|\n",
      "|     D|2024-12-29 12:30:00|2024-12-29 16:00:00|\n",
      "+------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 時間帯データ\n",
    "data = [\n",
    "    (\"A\", datetime.strptime(\"2024-12-29 09:00:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 17:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (\"B\", datetime.strptime(\"2024-12-29 12:00:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 23:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (\"C\", datetime.strptime(\"2024-12-29 10:00:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 13:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (\"D\", datetime.strptime(\"2024-12-29 12:30:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 16:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    # (\"A\", datetime.strptime(\"2024-12-29 09:00:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 9:15:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    # (\"B\", datetime.strptime(\"2024-12-29 09:05:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 9:20:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    # (\"C\", datetime.strptime(\"2024-12-29 09:00:00\", \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(\"2024-12-29 9:15:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "]\n",
    "\n",
    "# データスキーマの定義\n",
    "schema = StructType([\n",
    "    StructField(\"時間帯\", StringType(), True),\n",
    "    StructField(\"開始時刻\", TimestampType(), True),\n",
    "    StructField(\"終了時刻\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# データをデータフレームに変換\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================================>           (19 + 5) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           開始時刻|\n",
      "+-------------------+\n",
      "|2024-12-29 09:00:00|\n",
      "|2024-12-29 10:00:00|\n",
      "|2024-12-29 12:00:00|\n",
      "|2024-12-29 12:30:00|\n",
      "|2024-12-29 13:00:00|\n",
      "|2024-12-29 16:00:00|\n",
      "|2024-12-29 17:00:00|\n",
      "|2024-12-29 23:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 境界時刻の抽出\n",
    "boundaries = df.select(col(\"開始時刻\")).union(df.select(col(\"終了時刻\"))).distinct().orderBy(\"開始時刻\")\n",
    "boundaries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重複している時刻は各時間帯の区切り（境界）となる時刻から特定できる。  \n",
    "ゆえに、開始時刻と終了時刻を一つにまとめてユニーク＆昇順にしたDataframeを作成しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|       重複開始時刻|       重複終了時刻|\n",
      "+-------------------+-------------------+\n",
      "|2024-12-29 09:00:00|2024-12-29 10:00:00|\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|\n",
      "|2024-12-29 17:00:00|2024-12-29 23:00:00|\n",
      "|2024-12-29 23:00:00|               NULL|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 各セグメントの作成。F.leadにより、重複開始時刻の一つあと(9:00開始なら10:00)の境界時刻を取得している\n",
    "segments = boundaries.withColumnRenamed(\"開始時刻\", \"重複開始時刻\") \\\n",
    "    .withColumn(\"重複終了時刻\", F.lead(\"重複開始時刻\").over(Window.orderBy(\"重複開始時刻\")))\n",
    "segments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の重複開始時刻・重複終了時刻１レコードが重複時間を特定できる最小の時刻範囲（重複候補時間帯）となる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+----------------+\n",
      "|重複開始時刻       |重複終了時刻       |重複時間帯  |重複時間帯の総数|\n",
      "+-------------------+-------------------+------------+----------------+\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|[A, C]      |2               |\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|[A, B, C, D]|4               |\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|[A, B, C]   |3               |\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|[A, B, D]   |3               |\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|[A, B]      |2               |\n",
      "+-------------------+-------------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 時間帯との重複を判定\n",
    "overlap_conditions = (\n",
    "    (col(\"開始時刻\") < col(\"重複終了時刻\")) & (col(\"終了時刻\") > col(\"重複開始時刻\"))\n",
    ")\n",
    "# whereはfilterの別名\n",
    "overlap_segments = df.crossJoin(segments).where(overlap_conditions) \\\n",
    "    .groupBy(\"重複開始時刻\", \"重複終了時刻\") \\\n",
    "    .agg(\n",
    "        F.collect_list(\"時間帯\").alias(\"重複時間帯\"),\n",
    "        F.size(F.collect_list(\"時間帯\")).alias(\"重複時間帯の総数\")\n",
    "    ).filter(\n",
    "        # segmentを含む時間帯が一つのものは除外\n",
    "        F.col(\"重複時間帯の総数\") > 1\n",
    "    )\n",
    "\n",
    "# 結果の表示\n",
    "overlap_segments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+----------------+--------+\n",
      "|       重複開始時刻|       重複終了時刻|  重複時間帯|重複時間帯の総数|重複時間|\n",
      "+-------------------+-------------------+------------+----------------+--------+\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|      [A, C]|               2|     2.0|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|[A, B, C, D]|               4|     0.5|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|   [A, B, C]|               3|     0.5|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|   [A, B, D]|               3|     3.0|\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|      [A, B]|               2|     1.0|\n",
      "+-------------------+-------------------+------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 重複開始・終了時刻の差分から重複時間を求めるには下記のようにする。\n",
    "# PySparkにおいて、Timestamp型同時の差分結果はDayTimeIntervalとなるが、longにキャストすれば秒を取得できる\n",
    "overlap_segments.withColumn(\n",
    "    \"重複時間\",\n",
    "    (F.col(\"重複終了時刻\") - F.col(\"重複開始時刻\")).cast(\"long\") / 3600\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+----------------+\n",
      "|       重複開始時刻|       重複終了時刻|重複時間帯|重複時間帯の総数|\n",
      "+-------------------+-------------------+----------+----------------+\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|         A|               2|\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|         C|               2|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         A|               4|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         B|               4|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         C|               4|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         D|               4|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|         A|               3|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|         B|               3|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|         C|               3|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|         A|               3|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|         B|               3|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|         D|               3|\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|         A|               2|\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|         B|               2|\n",
      "+-------------------+-------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 配列が扱いずらいのであればexplodeすればよい\n",
    "overlap_segments.withColumn(\n",
    "    \"重複時間帯\",\n",
    "    F.explode(F.col(\"重複時間帯\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "肝となるのはsegment（重複候補時間帯）を元のdfにcrossjoinさせるところ。  \n",
    "こうすることでfor文を使わなくても各セグメントを含むか否かについて条件判定ができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+-------------------+-------------------+\n",
      "|時間帯|           開始時刻|           終了時刻|       重複開始時刻|       重複終了時刻|\n",
      "+------+-------------------+-------------------+-------------------+-------------------+\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 09:00:00|2024-12-29 10:00:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 10:00:00|2024-12-29 12:00:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 12:00:00|2024-12-29 12:30:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 12:30:00|2024-12-29 13:00:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 13:00:00|2024-12-29 16:00:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 16:00:00|2024-12-29 17:00:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 17:00:00|2024-12-29 23:00:00|\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|2024-12-29 23:00:00|               NULL|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 09:00:00|2024-12-29 10:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 10:00:00|2024-12-29 12:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 12:00:00|2024-12-29 12:30:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 12:30:00|2024-12-29 13:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 13:00:00|2024-12-29 16:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 16:00:00|2024-12-29 17:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 17:00:00|2024-12-29 23:00:00|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|2024-12-29 23:00:00|               NULL|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|2024-12-29 09:00:00|2024-12-29 10:00:00|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|2024-12-29 10:00:00|2024-12-29 12:00:00|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|2024-12-29 12:00:00|2024-12-29 12:30:00|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|2024-12-29 12:30:00|2024-12-29 13:00:00|\n",
      "+------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crossJoin(segments).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重複時間を考慮した時間の計算\n",
    "重複が発生している場合、各時間帯の開始～終了時間（所要時間）を合計すると、  \n",
    "クリティカルパスの所要時間以上になってしまうので、  \n",
    "重複していても合計すればクリティカルパスの所要時間と等しくなるように重複時間を補正する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.withColumn(\n",
    "    \"開始～終了までの時間\",\n",
    "    (F.col(\"終了時刻\") - F.col(\"開始時刻\")).cast(\"long\") / 3600\n",
    ")\n",
    "\n",
    "# 重複開始・終了時刻の差分から重複時間を求めるには下記のようにする。\n",
    "# PySparkにおいて、Timestamp型同時の差分結果はDayTimeIntervalとなるが、longにキャストすれば秒を取得できる\n",
    "overlap_segments_mod = overlap_segments.withColumn(\n",
    "    \"重複時間\",\n",
    "    (F.col(\"重複終了時刻\") - F.col(\"重複開始時刻\")).cast(\"long\") / 3600\n",
    ").withColumns({\n",
    "    \"重複時間（補正）\":\n",
    "    F.col(\"重複時間\") / F.col(\"重複時間帯の総数\"),\n",
    "    \"重複時間帯\":\n",
    "    F.explode(F.col(\"重複時間帯\"))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+--------------------+\n",
      "|時間帯|           開始時刻|           終了時刻|開始～終了までの時間|\n",
      "+------+-------------------+-------------------+--------------------+\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|                 8.0|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|                11.0|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|                 3.0|\n",
      "|     D|2024-12-29 12:30:00|2024-12-29 16:00:00|                 3.5|\n",
      "+------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+----------------+--------+-------------------+\n",
      "|       重複開始時刻|       重複終了時刻|重複時間帯|重複時間帯の総数|重複時間|   重複時間（補正）|\n",
      "+-------------------+-------------------+----------+----------------+--------+-------------------+\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|         A|               2|     2.0|                1.0|\n",
      "|2024-12-29 10:00:00|2024-12-29 12:00:00|         C|               2|     2.0|                1.0|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         A|               4|     0.5|              0.125|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         B|               4|     0.5|              0.125|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         C|               4|     0.5|              0.125|\n",
      "|2024-12-29 12:30:00|2024-12-29 13:00:00|         D|               4|     0.5|              0.125|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|         A|               3|     0.5|0.16666666666666666|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|         B|               3|     0.5|0.16666666666666666|\n",
      "|2024-12-29 12:00:00|2024-12-29 12:30:00|         C|               3|     0.5|0.16666666666666666|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|         A|               3|     3.0|                1.0|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|         B|               3|     3.0|                1.0|\n",
      "|2024-12-29 13:00:00|2024-12-29 16:00:00|         D|               3|     3.0|                1.0|\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|         A|               2|     1.0|                0.5|\n",
      "|2024-12-29 16:00:00|2024-12-29 17:00:00|         B|               2|     1.0|                0.5|\n",
      "+-------------------+-------------------+----------+----------------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overlap_segments_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|重複時間帯|重複時間合計|重複時間合計（補正）|\n",
      "+----------+------------+--------------------+\n",
      "|         B|         5.0|  1.7916666666666665|\n",
      "|         D|         3.5|               1.125|\n",
      "|         C|         3.0|  1.2916666666666667|\n",
      "|         A|         7.0|   2.791666666666667|\n",
      "+----------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overlap_segments_total = overlap_segments_mod.groupBy(\"重複時間帯\").agg(\n",
    "    F.sum(\"重複時間\").alias(\"重複時間合計\"),\n",
    "    F.sum(\"重複時間（補正）\").alias(\"重複時間合計（補正）\")\n",
    ")\n",
    "overlap_segments_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+--------------------+----------------------------+------------+--------------------+\n",
      "|時間帯|           開始時刻|           終了時刻|開始～終了までの時間|開始～終了までの時間（補正）|重複時間合計|重複時間合計（補正）|\n",
      "+------+-------------------+-------------------+--------------------+----------------------------+------------+--------------------+\n",
      "|     A|2024-12-29 09:00:00|2024-12-29 17:00:00|                 8.0|                         3.8|         7.0|   2.791666666666667|\n",
      "|     B|2024-12-29 12:00:00|2024-12-29 23:00:00|                11.0|                         7.8|         5.0|  1.7916666666666665|\n",
      "|     C|2024-12-29 10:00:00|2024-12-29 13:00:00|                 3.0|                         1.3|         3.0|  1.2916666666666667|\n",
      "|     D|2024-12-29 12:30:00|2024-12-29 16:00:00|                 3.5|                         1.1|         3.5|               1.125|\n",
      "+------+-------------------+-------------------+--------------------+----------------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.join(\n",
    "    overlap_segments_total,\n",
    "    result.時間帯==overlap_segments_total.重複時間帯,\n",
    "    \"left\"\n",
    ").select(\n",
    "    \"時間帯\", \n",
    "    \"開始時刻\", \n",
    "    \"終了時刻\", \n",
    "    \"開始～終了までの時間\", \n",
    "    F.round(\n",
    "        (F.col(\"開始～終了までの時間\") - F.col(\"重複時間合計\") + F.col(\"重複時間合計（補正）\")), 1\n",
    "    ).alias(\"開始～終了までの時間（補正）\"),\n",
    "    \"重複時間合計\", \n",
    "    \"重複時間合計（補正）\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------+\n",
      "|開始～終了までの時間合計|開始～終了までの時間合計（補正）|\n",
      "+------------------------+--------------------------------+\n",
      "|                    25.5|                            14.0|\n",
      "+------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\n",
    "    F.sum(\"開始～終了までの時間\").alias(\"開始～終了までの時間合計\"),\n",
    "    F.sum(\n",
    "        \"開始～終了までの時間（補正）\"\n",
    "    ).alias(\"開始～終了までの時間合計（補正）\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クリティカルパスは9:00:00~23:00:00の計14時間なのであっている。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
