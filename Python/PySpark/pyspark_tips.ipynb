{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkのTips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "細かいTips、テクニックをまとめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Create a SparkSession。pythonからsparkを使う場合、セッションの作成が必要。\n",
    "spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n",
    "\n",
    "# デフォルトのログレベルだと大量にログが出力されるので限定する。\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 各データ読み込み\n",
    "df_receipt = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/receipt.parquet\")\n",
    "\n",
    "# 店舗データ\n",
    "df_store = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/store.parquet\")\n",
    "\n",
    "# 顧客データ\n",
    "df_customer = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/customer.parquet\")\n",
    "\n",
    "# 製品データ\n",
    "df_product = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/product.parquet\")\n",
    "\n",
    "# 製品データ\n",
    "df_category = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/category.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特定カラムのユニークな値をリストとしてすべて取得\n",
    "下記のような方法がある。どちらにせよめんどくさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '9', '1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collectの結果の各値は対象カラムをキーとする辞書のような形で取得できる\n",
    "[v[\"gender_cd\"] for v in df_customer.select(\"gender_cd\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '9', '1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.select(\"gender_cd\").dropDuplicates().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|gender_cd|\n",
      "+---------+\n",
      "|        0|\n",
      "|        9|\n",
      "|        1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframeとして取得したいなら.distinctでOK\n",
    "df_customer.select(\"gender_cd\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joinの結合条件にcontainsを用いる\n",
    "結構便利。left joinで左のカラムに右のカラムの値が含まれている行に対して結合したい場合などに使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの作成\n",
    "data_a = [\n",
    "    (1, \"This is a sample message\"),\n",
    "    (2, \"Another example message\"),\n",
    "    (3, \"Message with a keyword\"),\n",
    "    (4, \"No match here\"),\n",
    "    (5, \"samplesample\"),\n",
    "    (6, \"sample keyword\"),\n",
    "]\n",
    "columns_a = [\"id\", \"message\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "data_b = [\n",
    "    (1, \"sample\", \"MSG001\"),\n",
    "    (2, \"example\", \"MSG002\"),\n",
    "    (3, \"keyword\", \"MSG003\")\n",
    "]\n",
    "columns_b = [\"id\" ,\"message_key\", \"MSG_No\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|             message|\n",
      "+---+--------------------+\n",
      "|  1|This is a sample ...|\n",
      "|  2|Another example m...|\n",
      "|  3|Message with a ke...|\n",
      "|  4|       No match here|\n",
      "|  5|        samplesample|\n",
      "|  6|      sample keyword|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+\n",
      "| id|message_key|MSG_No|\n",
      "+---+-----------+------+\n",
      "|  1|     sample|MSG001|\n",
      "|  2|    example|MSG002|\n",
      "|  3|    keyword|MSG003|\n",
      "+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----+-----------+------+\n",
      "| id|             message|  id|message_key|MSG_No|\n",
      "+---+--------------------+----+-----------+------+\n",
      "|  1|This is a sample ...|   1|     sample|MSG001|\n",
      "|  2|Another example m...|   2|    example|MSG002|\n",
      "|  3|Message with a ke...|   3|    keyword|MSG003|\n",
      "|  4|       No match here|NULL|       NULL|  NULL|\n",
      "|  5|        samplesample|   1|     sample|MSG001|\n",
      "|  6|      sample keyword|   1|     sample|MSG001|\n",
      "|  6|      sample keyword|   3|    keyword|MSG003|\n",
      "+---+--------------------+----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.join(\n",
    "    df_b,\n",
    "    # 結合条件にcontainsを使用。messageにmessage_keyが含まれていれば紐づける\n",
    "    F.contains(df_a.message, df_b.message_key),\n",
    "    \"left\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"sample keyword\"のように２つの単語に紐づく場合は2行紐づく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframeの任意の行番号範囲を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+\n",
      "|sales_month|total_amount|row_num|\n",
      "+-----------+------------+-------+\n",
      "|     201701|      902056|      0|\n",
      "|     201702|      764413|      1|\n",
      "|     201703|      962945|      2|\n",
      "|     201704|      847566|      3|\n",
      "|     201705|      884010|      4|\n",
      "|     201706|      894242|      5|\n",
      "|     201707|      959205|      6|\n",
      "|     201708|      954836|      7|\n",
      "|     201709|      902037|      8|\n",
      "|     201710|      905739|      9|\n",
      "|     201711|      932157|     10|\n",
      "|     201712|      939654|     11|\n",
      "|     201801|      944509|     12|\n",
      "|     201802|      864128|     13|\n",
      "|     201803|      946588|     14|\n",
      "|     201804|      937099|     15|\n",
      "|     201805|     1004438|     16|\n",
      "|     201806|     1012329|     17|\n",
      "+-----------+------------+-------+\n",
      "only showing top 18 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = df_receipt.withColumn(\n",
    "    \"sales_month\",\n",
    "    F.col(\"sales_ymd\").substr(0, 6) # yyyyMMdd -> yyyyMM形式にする\n",
    ").groupBy(\"sales_month\").agg(   # 月次ごとに集計\n",
    "    F.sum(\"amount\").alias(\"total_amount\")\n",
    ").withColumn(\n",
    "    \"row_num\",\n",
    "    F.row_number().over(Window.orderBy(\"sales_month\")) - 1\n",
    ")\n",
    "\n",
    "dataset.show(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※PySparkのDataframeの部分行だけ取り出すには上記のようWindow関数を使って行番号を振るしかなさそう。  \n",
    "  メモリに読み込まずに先頭から指定したレコード数だけ取り出す場合はlimitが使えるが、どこからどこまで取り出すという指定はできない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 12\n",
    "val_size = 6\n",
    "offset = 6 # 次のtrainをどこから始めるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# 12か月ごとに学習データ、6か月ごとに検証データを定義する\n",
    "for i in range(3):\n",
    "    train_start = offset * i\n",
    "    train_end = train_start + train_size - 1\n",
    "    val_start = train_end + 1\n",
    "    val_end = val_start + offset - 1\n",
    "    train_data.append(dataset.filter(F.col(\"row_num\").between(train_start, train_end)))\n",
    "    val_data.append(dataset.filter(F.col(\"row_num\").between(val_start, val_end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark.sql.functions.coalesceの挙動\n",
    "名前から何をするのかわかりづらいので、挙動をおさらいしておく。  \n",
    "coalesceは”合体する”といった意味の英単語で、引数を左端から調べて最初に現れた非NULL値を返す関数である。  \n",
    "PySparkだけでなくSQLの文法にもある。  \n",
    "\n",
    "[pyspark.sql.functions.coalesceのドキュメント](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html?highlight=coale#pyspark.sql.functions.coalesce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|NULL|NULL|NULL|\n",
      "|   1|NULL|NULL|\n",
      "|NULL|   2|NULL|\n",
      "|NULL|NULL|   3|\n",
      "|NULL|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (None, None, None),\n",
    "    (1, None, None),\n",
    "    (None, 2, None),\n",
    "    (None, None, 3),\n",
    "    (None, 4, 5),\n",
    "], schema='a long, b long, c long')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          NULL|\n",
      "|             1|\n",
      "|             2|\n",
      "|          NULL|\n",
      "|             4|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2つのカラムをcoalesceする\n",
    "df.select(F.coalesce(\"a\", \"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|coalesce(a, b, c)|\n",
      "+-----------------+\n",
      "|             NULL|\n",
      "|                1|\n",
      "|                2|\n",
      "|                3|\n",
      "|                4|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3つのカラムをcoalesceする\n",
    "df.select(F.coalesce(\"a\", \"b\", \"c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように各行を左のカラムから右のカラムへ（a->b）調べていき、  \n",
    "最初にヒットしたNullでない値を採用して一つのカラムを返す。  \n",
    "全カラムNullの場合は単にNullになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nullの置換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.sql.DataFrame.fillnaによる置換\n",
    "王道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|NULL|NULL|NULL|\n",
      "|   1|NULL|NULL|\n",
      "|NULL|   2|NULL|\n",
      "|NULL|NULL|   3|\n",
      "|NULL|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|   c|\n",
      "+---+---+----+\n",
      "|  0|999|NULL|\n",
      "|  1|999|NULL|\n",
      "|  0|  2|NULL|\n",
      "|  0|999|   3|\n",
      "|  0|  4|   5|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数列のNullを置換\n",
    "df.fillna({\"a\": 0, \"b\": 999}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coalesceによる置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "|NULL|NULL|NULL|\n",
      "|   1|NULL|NULL|\n",
      "|NULL|   2|NULL|\n",
      "|NULL|NULL|   3|\n",
      "|NULL|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = {}\n",
    "for col in df.columns:\n",
    "    expr[col] = F.coalesce(col, F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  0|  0|  0|\n",
      "|  1|  0|  0|\n",
      "|  0|  2|  0|\n",
      "|  0|  0|  3|\n",
      "|  0|  4|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumns(\n",
    "    expr\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0の定数カラムとcoalesceするところがポイント。  \n",
    "複数行にやろうとすると面倒。  \n",
    "他にもF.whenを使用する方法があるが、当然なので割愛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL OUTER JOIN(外部結合の注意点)\n",
    "PySparkに限った話ではないが、JOIN後に残すカラムは慎重に選ぶべき。  \n",
    "例えば、テーブルAの主キーカラムAとテーブルBの主キーカラムBについて、FULL JOINで  \n",
    "カラムA==カラムBを条件としてJoinする場合、  \n",
    "Join後に残す主キーをカラムAだけとした場合、カラムBにしかなかった値はNULLになってしまう。  \n",
    "カラムBにあった値もJOIN後の主キーに残すためには追加の処理が必要。  \n",
    "JOINの仕組みを考えれば当たり前だが、注意すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルAのサンプルデータの作成\n",
    "data_a = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\")\n",
    "]\n",
    "columns_a = [\"id_a\", \"name_a\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "# テーブルBのサンプルデータの作成\n",
    "data_b = [\n",
    "    (3, \"David\"),\n",
    "    (4, \"Eve\"),\n",
    "    (5, \"Frank\")\n",
    "]\n",
    "columns_b = [\"id_b\", \"name_b\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================>                   (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+\n",
      "|id_a| name_a|id_b|name_b|\n",
      "+----+-------+----+------+\n",
      "|   1|  Alice|NULL|  NULL|\n",
      "|   2|    Bob|NULL|  NULL|\n",
      "|   3|Charlie|   3| David|\n",
      "|NULL|   NULL|   4|   Eve|\n",
      "|NULL|   NULL|   5| Frank|\n",
      "+----+-------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 残すカラムを指定しない場合\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|id_a| name_a|name_b|\n",
      "+----+-------+------+\n",
      "|   1|  Alice|  NULL|\n",
      "|   2|    Bob|  NULL|\n",
      "|   3|Charlie| David|\n",
      "|NULL|   NULL|   Eve|\n",
      "|NULL|   NULL| Frank|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 主キーとしてid_aだけ残す\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").select(\n",
    "    \"id_a\", \"name_a\", \"name_b\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思いとして、id_bに相当するname_bはあるので、id_aにid_bの値も残っててくれたらありがたいが、  \n",
    "当然そうはならない。対処としては下記のようにid_a, id_bをcoalesceに指定して、NULLを埋めて結合する方法と、  \n",
    "whenを使用する方法がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id| name_a|name_b|\n",
      "+---+-------+------+\n",
      "|  1|  Alice|  NULL|\n",
      "|  2|    Bob|  NULL|\n",
      "|  3|Charlie| David|\n",
      "|  4|   NULL|   Eve|\n",
      "|  5|   NULL| Frank|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# coalesceを使う方法\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").withColumn(\n",
    "    \"id\",\n",
    "    F.coalesce(\"id_a\", \"id_b\")\n",
    ").select(\n",
    "    \"id\", \"name_a\", \"name_b\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===================>                                     (4 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id| name_a|name_b|\n",
      "+---+-------+------+\n",
      "|  1|  Alice|  NULL|\n",
      "|  2|    Bob|  NULL|\n",
      "|  3|Charlie| David|\n",
      "|  4|   NULL|   Eve|\n",
      "|  5|   NULL| Frank|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# whenを使う方法\n",
    "df_a.join(\n",
    "    df_b, \n",
    "    df_a.id_a == df_b.id_b, \n",
    "    \"full_outer\"\n",
    ").withColumn(\n",
    "    \"id\",\n",
    "    F.when(F.col(\"id_a\").isNull(), F.col(\"id_b\")).otherwise(F.col(\"id_a\"))\n",
    ").select(\n",
    "    \"id\", \"name_a\", \"name_b\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主キー候補を機械的に抽出する\n",
    "数百のような大量のカラムを持つデータがあり、  \n",
    "かつ主キー（全行について一意かつNullの列）が不明といったダーティデータの主キー候補を機械的に抽出する方法を考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの作成\n",
    "data = [\n",
    "    (1, \"Alice\", \"C\", \"001\", 170, \"apple\", \"AAA\"),\n",
    "    (2, \"Bob\", \"A\", \"001\", 170, \"orange\", \"BBB\"),\n",
    "    (3, None, \"B\", \"001\", 156, \"orange\", \"CCC\"),\n",
    "    (None, \"Mike\", \"B\", \"002\", 160, \"orange\", \"AAA\"),\n",
    "    (5, \"Alice\", \"A\", \"002\", 156, \"apple\", \"AAA\"),\n",
    "\n",
    "]\n",
    "cols = [\"id\", \"name\", \"Class\", \"student_number\", \"height\", \"favorite_fruits\", \"col_A\"]\n",
    "df = spark.createDataFrame(data, cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考え方\n",
    "1. カラムを無作為に主キー候補に選び、ユニーク値のカウントとdfの行数が一致するか判定\n",
    "2. 一致しない場合、別のカラムを一つ選び複合キーを作成し、新たな主キー候補とする\n",
    "3. 一致するまで1.~2.を繰り返す\n",
    "\n",
    "なお、計算量の制限および、全ての列を結合させると1．の条件は必ず満たされてしまうことを防ぐために、  \n",
    "結合させるカラムの最大数は指定するものとする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主キーの候補になり得るのは基本的に文字列型のため、文字列型のカラムのみを対象とする。\n",
    "string_cols = [\n",
    "    field.name for field in df.schema.fields\n",
    "    if isinstance(field.dataType, (StringType))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'Class', 'student_number', 'favorite_fruits', 'col_A']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nullを含まない列の抽出\n",
    "cols_not_contain_nulls = []\n",
    "\n",
    "for col in string_cols:\n",
    "    if df.filter(F.col(col).isNull()).count() == 0:\n",
    "        cols_not_contain_nulls.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nums_dict = {}\n",
    "\n",
    "for col in cols_not_contain_nulls:\n",
    "    unique_nums_dict[col] = df.select(col).distinct().count()\n",
    "\n",
    "\n",
    "# 効率化のため、ユニーク件数が多い順にカラムを並べておく。\n",
    "target_cols = sorted(unique_nums_dict, key=unique_nums_dict.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Class', 'col_A', 'student_number', 'favorite_fruits']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複合キーの最大サイズ（=複合キーに採用した最大カラム数）\n",
    "max_composite_key_len = 3\n",
    "pk_candidate = []\n",
    "\n",
    "for col in target_cols:\n",
    "    if df.select(pk_candidate).distinct().count() == df.count():\n",
    "        break\n",
    "    else:\n",
    "        pk_candidate.append(col)\n",
    "    \n",
    "    if len(pk_candidate) == max_composite_key_len :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Class', 'col_A']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pk_candidate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
