{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkのTips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "細かいTips、テクニックをまとめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Create a SparkSession。pythonからsparkを使う場合、セッションの作成が必要。\n",
    "spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n",
    "\n",
    "# デフォルトのログレベルだと大量にログが出力されるので限定する。\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 各データ読み込み\n",
    "df_receipt = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/receipt.parquet\")\n",
    "\n",
    "# 店舗データ\n",
    "df_store = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/store.parquet\")\n",
    "\n",
    "# 顧客データ\n",
    "df_customer = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/customer.parquet\")\n",
    "\n",
    "# 製品データ\n",
    "df_product = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/product.parquet\")\n",
    "\n",
    "# 製品データ\n",
    "df_category = spark.read.parquet(\"../../../100knocks-preprocess/docker/work/data/category.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特定カラムのユニークな値をリストとしてすべて取得\n",
    "下記のような方法がある。どちらにせよめんどくさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '9', '1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collectの結果の各値は対象カラムをキーとする辞書のような形で取得できる\n",
    "[v[\"gender_cd\"] for v in df_customer.select(\"gender_cd\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '9', '1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.select(\"gender_cd\").dropDuplicates().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|gender_cd|\n",
      "+---------+\n",
      "|        0|\n",
      "|        9|\n",
      "|        1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframeとして取得したいなら.distinctでOK\n",
    "df_customer.select(\"gender_cd\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joinの結合条件にcontainsを用いる\n",
    "結構便利。left joinで左のカラムに右のカラムの値が含まれている行に対して結合したい場合などに使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの作成\n",
    "data_a = [\n",
    "    (1, \"This is a sample message\"),\n",
    "    (2, \"Another example message\"),\n",
    "    (3, \"Message with a keyword\"),\n",
    "    (4, \"No match here\"),\n",
    "    (5, \"samplesample\"),\n",
    "    (6, \"sample keyword\"),\n",
    "]\n",
    "columns_a = [\"id\", \"message\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "data_b = [\n",
    "    (1, \"sample\", \"MSG001\"),\n",
    "    (2, \"example\", \"MSG002\"),\n",
    "    (3, \"keyword\", \"MSG003\")\n",
    "]\n",
    "columns_b = [\"id\" ,\"message_key\", \"MSG_No\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|             message|\n",
      "+---+--------------------+\n",
      "|  1|This is a sample ...|\n",
      "|  2|Another example m...|\n",
      "|  3|Message with a ke...|\n",
      "|  4|       No match here|\n",
      "|  5|        samplesample|\n",
      "|  6|      sample keyword|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+\n",
      "| id|message_key|MSG_No|\n",
      "+---+-----------+------+\n",
      "|  1|     sample|MSG001|\n",
      "|  2|    example|MSG002|\n",
      "|  3|    keyword|MSG003|\n",
      "+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----+-----------+------+\n",
      "| id|             message|  id|message_key|MSG_No|\n",
      "+---+--------------------+----+-----------+------+\n",
      "|  1|This is a sample ...|   1|     sample|MSG001|\n",
      "|  2|Another example m...|   2|    example|MSG002|\n",
      "|  3|Message with a ke...|   3|    keyword|MSG003|\n",
      "|  4|       No match here|NULL|       NULL|  NULL|\n",
      "|  5|        samplesample|   1|     sample|MSG001|\n",
      "|  6|      sample keyword|   1|     sample|MSG001|\n",
      "|  6|      sample keyword|   3|    keyword|MSG003|\n",
      "+---+--------------------+----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.join(\n",
    "    df_b,\n",
    "    # 結合条件にcontainsを使用。messageにmessage_keyが含まれていれば紐づける\n",
    "    F.contains(df_a.message, df_b.message_key),\n",
    "    \"left\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"sample keyword\"のように２つの単語に紐づく場合は2行紐づく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframeの任意の行番号範囲を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------+\n",
      "|sales_month|total_amount|row_num|\n",
      "+-----------+------------+-------+\n",
      "|     201701|      902056|      0|\n",
      "|     201702|      764413|      1|\n",
      "|     201703|      962945|      2|\n",
      "|     201704|      847566|      3|\n",
      "|     201705|      884010|      4|\n",
      "|     201706|      894242|      5|\n",
      "|     201707|      959205|      6|\n",
      "|     201708|      954836|      7|\n",
      "|     201709|      902037|      8|\n",
      "|     201710|      905739|      9|\n",
      "|     201711|      932157|     10|\n",
      "|     201712|      939654|     11|\n",
      "|     201801|      944509|     12|\n",
      "|     201802|      864128|     13|\n",
      "|     201803|      946588|     14|\n",
      "|     201804|      937099|     15|\n",
      "|     201805|     1004438|     16|\n",
      "|     201806|     1012329|     17|\n",
      "+-----------+------------+-------+\n",
      "only showing top 18 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = df_receipt.withColumn(\n",
    "    \"sales_month\",\n",
    "    F.col(\"sales_ymd\").substr(0, 6) # yyyyMMdd -> yyyyMM形式にする\n",
    ").groupBy(\"sales_month\").agg(   # 月次ごとに集計\n",
    "    F.sum(\"amount\").alias(\"total_amount\")\n",
    ").withColumn(\n",
    "    \"row_num\",\n",
    "    F.row_number().over(Window.orderBy(\"sales_month\")) - 1\n",
    ")\n",
    "\n",
    "dataset.show(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※PySparkのDataframeの部分行だけ、取り出すには上記のようWindow関数を使って行番号を振るしかなさそう。  \n",
    "  メモリに読み込まずに先頭から指定したレコード数だけ取り出す場合はlimitが使えるが、どこからどこまで取り出すという指定はできない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 12\n",
    "val_size = 6\n",
    "offset = 6 # 次のtrainをどこから始めるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "for i in range(3):\n",
    "    train_start = offset * i\n",
    "    train_end = train_start + train_size - 1\n",
    "    val_start = train_end + 1\n",
    "    val_end = val_start + offset - 1\n",
    "    train_data.append(dataset.filter(F.col(\"row_num\").between(train_start, train_end)))\n",
    "    val_data.append(dataset.filter(F.col(\"row_num\").between(val_start, val_end)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
