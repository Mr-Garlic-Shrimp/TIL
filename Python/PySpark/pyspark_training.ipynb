{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkに慣れる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール\n",
    "基本は[Quickstart](https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html)に従って、  \n",
    "pip install pysparkすればインストールできる。  \n",
    "ただし、jdkが入っていない場合はjdkもインストールしておかないとJAVA_HOMEが設定されていないというエラーが起きる。  \n",
    "インストール後は特に明示的にJAVA‗HOMEを設定しなくても大丈夫だったが、場合によっては環境変数にパスを設定しておく必要があるかも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作テスト\n",
    "文法はPolarsに似ている。（というかPolarsがPySparkに似ている？）  \n",
    "ノートPCでの動作はPolarsよりも遅く感じる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession。pythonからsparkを使う場合、セッションの作成が必要。\n",
    "spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()\n",
    "\n",
    "# デフォルトのログレベルだと大量にログが出力されるので限定する。\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [{\"name\": \"John    D.\", \"age\": 30},\n",
    "  {\"name\": \"Alice   G.\", \"age\": 25},\n",
    "  {\"name\": \"Bob  T.\", \"age\": 35},\n",
    "  {\"name\": \"Eve   A.\", \"age\": 28}]\n",
    "\n",
    "df = spark.createDataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 30| John D.|\n",
      "| 25|Alice G.|\n",
      "| 35|  Bob T.|\n",
      "| 28|  Eve A.|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove additional spaces in name\n",
    "def remove_extra_spaces(df, column_name):\n",
    "    # Remove extra spaces from the specified column\n",
    "    df_transformed = df.withColumn(column_name, F.regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "transformed_df = remove_extra_spaces(df, \"name\")\n",
    "\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 色々お試し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下記のような読み込み方はダメ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m sample_data_2 \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn    D.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice   G.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob  T.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEve   A.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m28\u001b[39m]\n",
      "\u001b[1;32m      4\u001b[0m }\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# [{\"name\": \"John    D.\", \"age\": 30},\u001b[39;00m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#   {\"name\": \"Alice   G.\", \"age\": 25},\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#   {\"name\": \"Bob  T.\", \"age\": 35},\u001b[39;00m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#   {\"name\": \"Eve   A.\", \"age\": 28}]\u001b[39;00m\n",
      "\u001b[0;32m---> 10\u001b[0m aaa \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
      "\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n",
      "\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n",
      "\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n",
      "\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n",
      "\u001b[1;32m   1442\u001b[0m     )\n",
      "\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
      "\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
      "\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n",
      "\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n",
      "\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
      "\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n",
      "\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:955\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n",
      "\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n",
      "\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n",
      "\u001b[0;32m--> 955\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n",
      "\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n",
      "\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n",
      "\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n",
      "\u001b[1;32m    972\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:958\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n",
      "\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n",
      "\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n",
      "\u001b[1;32m    956\u001b[0m     _merge_type,\n",
      "\u001b[1;32m    957\u001b[0m     (\n",
      "\u001b[0;32m--> 958\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n",
      "\u001b[1;32m    966\u001b[0m     ),\n",
      "\u001b[1;32m    967\u001b[0m )\n",
      "\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n",
      "\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n",
      "\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n",
      "\u001b[1;32m    972\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n",
      "\u001b[1;32m   1667\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n",
      "\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
      "\u001b[1;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
      "\u001b[1;32m   1673\u001b[0m     )\n",
      "\u001b[1;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`."
     ]
    }
   ],
   "source": [
    "sample_data_2 = {\n",
    "    \"name\" : [\"John    D.\", \"Alice   G.\", \"Bob  T.\", \"Eve   A.\"],\n",
    "    \"age\" : [30, 25, 35, 28]\n",
    "}\n",
    "# [{\"name\": \"John    D.\", \"age\": 30},\n",
    "#   {\"name\": \"Alice   G.\", \"age\": 25},\n",
    "#   {\"name\": \"Bob  T.\", \"age\": 35},\n",
    "#   {\"name\": \"Eve   A.\", \"age\": 28}]\n",
    "\n",
    "aaa = spark.createDataFrame(sample_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較的楽なサンプルデータの作成方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---+----+\n",
      "|  a|  b|      c|  d|   e|\n",
      "+---+---+-------+---+----+\n",
      "|  1|2.0|string1|AAA|1000|\n",
      "|  2|3.0|string2|BBB|2000|\n",
      "|  3|4.0|string3|AAA| 300|\n",
      "|  4|5.0|string4|BBB| 100|\n",
      "|  5|6.0|string5|CCC| 999|\n",
      "+---+---+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample = spark.createDataFrame([\n",
    "    (1, 2., 'string1', 'AAA', 1000),\n",
    "    (2, 3., 'string2', 'BBB', 2000),\n",
    "    (3, 4., 'string3', 'AAA', 300),\n",
    "    (4, 5., 'string4', 'BBB', 100),\n",
    "    (5, 6., 'string5', 'CCC', 999),\n",
    "], schema='a long, b double, c string, d string, e long')\n",
    "df_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列を足す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------+\n",
      "|age|    name|new_column_1|\n",
      "+---+--------+------------+\n",
      "| 30| John D.|          33|\n",
      "| 25|Alice G.|          28|\n",
      "| 35|  Bob T.|          38|\n",
      "| 28|  Eve A.|          31|\n",
      "+---+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列を足す。\n",
    "transformed_df.withColumn(\n",
    "    \"new_column_1\", F.col(\"age\") + 3\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|John D.|\n",
      "| 35| Bob T.|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.filter(\n",
    "    F.col(\"age\") >= 30\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規表現で置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|name_mod|\n",
      "+---+--------+\n",
      "| 30| John_D.|\n",
      "| 25|Alice_G.|\n",
      "| 35|  Bob_T.|\n",
      "| 28|  Eve_A.|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.select(\n",
    "    \"age\",\n",
    "    F.regexp_replace(F.col(\"name\"), \"\\s\", \"_\").alias(\"name_mod\")  # 半角スペースをアンダースコアに変換する\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字列のスライシング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "|age|Full_name|Last_name|\n",
      "+---+---------+---------+\n",
      "| 30|  John D.|       D.|\n",
      "| 25| Alice G.|       G.|\n",
      "| 35|   Bob T.|       T.|\n",
      "| 28|   Eve A.|       A.|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.select(\n",
    "    \"age\",\n",
    "    F.col(\"name\").alias(\"Full_name\"),\n",
    "    F.substring(F.col(\"name\"), -2, 2).alias(\"Last_name\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字列の分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "|age|First_name|Last_name|\n",
      "+---+----------+---------+\n",
      "| 30|      John|       D.|\n",
      "| 25|     Alice|       G.|\n",
      "| 35|       Bob|       T.|\n",
      "| 28|       Eve|       A.|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.select(\n",
    "    \"age\",\n",
    "    F.split(\"name\", \" \")[0].alias(\"First_name\"),  # 半角スペースでnameを分割してそれぞれのカラムとする\n",
    "    F.split(\"name\", \" \")[1].alias(\"Last_name\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集計\n",
    "これもほぼPolarsと変わらない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---+----+\n",
      "|  a|  b|      c|  d|   e|\n",
      "+---+---+-------+---+----+\n",
      "|  1|2.0|string1|AAA|1000|\n",
      "|  2|3.0|string2|BBB|2000|\n",
      "|  3|4.0|string3|AAA| 300|\n",
      "|  4|5.0|string4|BBB| 100|\n",
      "|  5|6.0|string5|CCC| 999|\n",
      "+---+---+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  d|sum(e)|\n",
      "+---+------+\n",
      "|AAA|  1300|\n",
      "|BBB|  2100|\n",
      "|CCC|   999|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sample.groupBy(\"d\").agg(\n",
    "    F.sum(\"e\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較演算子を用いた2つのDataframeの結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+------+\n",
      "|  a1|  a2|a2_max|a2_min|\n",
      "+----+----+------+------+\n",
      "|ID_1|1000|  1200|   800|\n",
      "|ID_2|2000|  2200|  1800|\n",
      "|ID_3| 300|   500|   100|\n",
      "|ID_4| 100|   300|  -100|\n",
      "|ID_5| 200|   400|     0|\n",
      "+----+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame([\n",
    "    ('ID_1', 1000),\n",
    "    ('ID_2', 2000),\n",
    "    ('ID_3', 300),\n",
    "    ('ID_4', 100),\n",
    "    ('ID_5', 200),\n",
    "], schema='a1 string, a2 long').withColumns({\n",
    "    \"a2_max\": F.col(\"a2\") + 200,\n",
    "    \"a2_min\": F.col(\"a2\") - 200,\n",
    "})\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|    b1|   b2|   b3|\n",
      "+------+-----+-----+\n",
      "| apple| ID_1|  900|\n",
      "|  meat| ID_9| 2000|\n",
      "|  fish| ID_3|10000|\n",
      "| lemon|ID_10|  800|\n",
      "|potato|ID_13|    0|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.createDataFrame([\n",
    "    ('apple', 'ID_1', 900),\n",
    "    ('meat', 'ID_9', 2000),\n",
    "    ('fish', 'ID_3', 10000),\n",
    "    ('lemon', 'ID_10', 800),\n",
    "    ('potato', 'ID_13', 0),\n",
    "], schema='b1 string, b2 string, b3 long')\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:===================================>                    (12 + 7) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+----+----+------+------+\n",
      "|    b1|   b2|  b3|  a1|  a2|a2_max|a2_min|\n",
      "+------+-----+----+----+----+------+------+\n",
      "| apple| ID_1| 900|ID_1|1000|  1200|   800|\n",
      "|  meat| ID_9|2000|ID_2|2000|  2200|  1800|\n",
      "|potato|ID_13|   0|ID_4| 100|   300|  -100|\n",
      "+------+-----+----+----+----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2.join(\n",
    "    df_1,\n",
    "    [\n",
    "        df_2.b3 < df_1.a2_max,\n",
    "        df_2.b3 > df_1.a2_min,\n",
    "    ],\n",
    "    \"inner\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のようにjoinでカラムを指定する箇所に比較演算子の形で書くことで、  \n",
    "ある程度数値の幅があっても結合することが出来る。  \n",
    "上記の場合、df_2のb3の値がa2の最小値より大きく、最大値よりも小さい範囲にある行を結合する形となる。  \n",
    "\n",
    "これは例えば、ある人が申告した事象発生時刻と実際にシステムでその事象が記録された時刻にぶれがある場合でも両者を紐づけたいときに使える。  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
